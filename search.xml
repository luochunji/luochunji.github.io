<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ELK日志系统简易搭建指南]]></title>
    <url>%2F2019%2F05%2F24%2FELK%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E7%AE%80%E6%98%93%E6%90%AD%E5%BB%BA%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[​ 前段时间协助业务平台搭建了一个ELK日志系统，帮助测试人员查看、跟踪业务日志，将搭建过程简易整理。 软件准备ElasticSearch：elasticsearch-5.0.0.tar.gz Logstash：logstash-5.0.0.tar.gz Kibana：kibana-5.0.0-linux-x86_64.tar.gz 以上软件都可以通过ES官网下载，三个软件版本需要一致 安装ElasticSearch 创建elk用户（es必须非root用户启动，es的配置/启动都以elk用户完成） 123groupadd elkuseradd -g elk elkpasswd elk 解压elasticsearch-5.0.0.tar.gz到/usr/local/elk/elasticsearch-5.0.0 修改配置文件/config/elasticsearch.yml，设置集群名称、节点名称、ip和端口（自行变更，这里改为9207） 修改jvm配置/config/jvm.options，把jvm的大小调整为2G 修改启动文件/bin/elasticsearch，首行增加以下配置（JDK的路径自行替换） 12JAVA_HOME="/usr/java/jdk1.8.0_201/"JAVA_OPTS="" 启动es 1$ /usr/local/elk/elasticsearch-5.0.0/bin/elasticsearch &amp; 如果出现以下异常： 123456789101112131415java.lang.UnsupportedOperationException: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed at org.elasticsearch.bootstrap.SystemCallFilter.linuxImpl(SystemCallFilter.java:342) ~[elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.SystemCallFilter.init(SystemCallFilter.java:617) ~[elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.JNANatives.tryInstallSystemCallFilter(JNANatives.java:258) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Natives.tryInstallSystemCallFilter(Natives.java:113) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:111) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:195) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:342) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:132) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:123) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:70) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.cli.Command.main(Command.java:90) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.0.0.jar:5.0.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.0.0.jar:5.0.0] 在配置文件增加以下配置： 12bootstrap.memory_lock: falsebootstrap.system_call_filter: false 如果出现以下提示： 需要修改系统的连接数，在/etc/sysctl.conf 文件中直接修改，增加配置项 1vm.max_map_count=262144 安装Logstash 解压到/usr/local/elk/logstash-5.0.0/ 修改config/jvm.options，增加以下配置： 1-Dcom.sun.management.snmp.port=8046 修改bin/logstash.lib.sh，首行增加以下配置（JDK路径自行替换）： 12export JAVA_CMD="/usr/java/jdk1.8.0_201/bin"export JAVA_HOME="/usr/java/jdk1.8.0_201/" 修改/config/logstash.conf，配置es和grok表达式（根据实际业务日志配置） 12345678910111213141516171819202122232425input &#123; file &#123; type =&gt; "tomcat-logs-1" path =&gt; "/usr/local/tomcat/logs/catalina.out" #日志文件路径 codec =&gt; multiline &#123; pattern =&gt; "^\s" what =&gt; "previous" &#125; start_position =&gt; "beginning" &#125;&#125;filter &#123; grok &#123; &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["ip:port"] #es的ip:port action =&gt; "index" codec =&gt; rubydebug index =&gt; "%&#123;type&#125;-%&#123;+YYYY.MM.dd&#125;" #生成的索引名称 template_name =&gt; "%&#123;type&#125;" &#125;&#125; 启动logstash 1nohup /usr/local/elk/logstash-5.0.0/bin/logstash -f /usr/local/elk/logstash-5.0.0/config/logstash.conf &amp; 安装Kibana 解压到/usr/local/elk/kibana-5.0.0-linux-x86_64 修改/config/kibana.yml 启动kibana 1/usr/local/elk/kibana-5.0.0-linux-x86_64/bin/kibana &amp; 启动后，打开Kibana面板，配置index 点击create即可生成实例 最后 其他机器要采集日志，只需安装logstash并修改logstash.conf文件将日志推送到es即可 防止日志文件过大，es按日期每天生成一个实例，请定期清理es的日志实例（可用脚本实现）]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows下安装docker环境]]></title>
    <url>%2F2019%2F05%2F17%2FWindows%E4%B8%8B%E5%AE%89%E8%A3%85Vagrant%2F</url>
    <content type="text"><![CDATA[​ 在windows下安装Vagrant并搭建docker环境 Vagrant 安装vagrant 安装virtualBox 准备CentOs7， 在工作区打开命令行，输入以下命令（Vagrant-CentOS-7.box在工作区中） vagrant box add centos7.2 Vagrant-CentOS-7.box 出现以下提示表示安装成功 如果没有准备镜像，可以输入以下命令下载： vagrant box add centos/6 # for CentOS Linux 6 vagrant box add centos/7 # for CentOS Linux 7 (也可以到官方镜像库下载：http://www.vagrantbox.es/ ) 初始化 vagrant init 生成Vagrantfile文件后，修改box名称 config.vm.box = “centos7.2” 启动box vagrant up 经过漫长的等待… … 进入VirtualBox，就可以看到运行中的centos 7.ssh命令链接 vagrant ssh Docker启动vagrant之后，开始搭建docker环境 切换root用户 安装dockeryum install -y docker Complete，安装完成，启动docker systemctl start docker 设置启动虚拟机时自动启动 systemctl enable docker 查看docker版本和信息 docker version docker info 创建一个新的容器 docker run -ti centos bash 从centos中创建 容器创建并启动之后，会分配唯一的id。 容器创建后，安装wget工具 yum install -y wget 在容器中创建镜像文件 先了解一下docker commit命令 [root@localhost vagrant]# docker commit --help Usage: docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]] options选项如下： -a：提交的镜像作者； -c：使用Dockerfile指令来创建镜像； -m：提交备注说明； -p：提交时，容器暂停。 docker commit -a:cjluo 55ffa21d66c1 image:1.0 此时启动镜像，已经包含了之前安装的wget 用Dockerfile创建镜像 Dockerfile是什么？ Dockerfile是Docker的RPM Spec文件 Dockerfile是一个包含用户创建的镜像所有指令的文本文件 Dockerfile中的指令指定在创建Docker镜像时做什么 Docker读取Dockerfile的指令创建Docker镜像 每个指令都将创建新的Docker镜像层 创建一个简单的镜像 12FROM centosRUN yum install -y wget 第一行表示基础镜像是centos 第二行是镜像执行的指令 新建指令：docker build . 创建过程中会生成临时容器9709e0b44c40，创建成功后会删除临时容器，最终的容器是d81f2d989145 查看生成的镜像，名称和tag都是none，给它加上。 docker tag d81f2d989145 myimage:1.0 也可以在生成镜像的命令上直接指定 docker build -t myimage:1.0，这样生成的镜像直接就带上了名称和tag 从docker仓库中查找镜像并拉取 例如需要一个包含nginx的镜像 docker search nginx 通过search从docker仓库查找，熟悉git的话，STARS不用多说就知道是什么意思，OFFICIAL = OK表示该镜像是一个官方镜像。 docker pull nginx 从官方镜像拉取，命令不带tag表示拉取最新版本的镜像]]></content>
      <categories>
        <category>docker</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>vagrant</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC框架原理和基本实现]]></title>
    <url>%2F2019%2F04%2F13%2FRPC%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86%E5%92%8C%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[RPC，全称Remote Procedure Call，远程过程调用，即调用远程PC上的服务，就像调用本地服务一样。目前流行的RPC框架有阿里的dubbo、facebook的thrif、谷歌的grpc、新浪微博的motan等。本篇只分析RPC的基本原理，并不涉及复杂的设计，最后以一个简单的demo实现RPC框架。 RPC框架的存在背景系统在演进的过程中，不断的拆分、模块化、服务化，逐渐成为分布式系统，各模块之间的通信通过服务调用完成。 分析以上场景，远程服务调用的细节，包括了请求数据序列化、网络通信、请求反序列化、响应序列化、网络通信、响应反序列化这一过程。 假如没有RPC框架，应用开发人员每次都要按照上述流程进行服务调用，不仅要关心业务，还要实现序列化/反序列化和网络通信，重复造轮子。衍生出的另一个问题是，随着服务的不断增多，复杂度不断提高，简单的远程服务调用已经无法满足实际业务需求。因此，伴随着系统的演进，RPC框架的出现也一步步解决了以上问题。 RPC框架的基本实现最简单的RPC框架，由两个组件组成：Client和Server，分别表示服务提供方和服务消费方。模拟一个场景，以电商订单支付为例，用户下单进行支付，由订单模块发起服务调用，支付模块提供支付服务并返回最终的支付结果，为了解耦，需要设计几个模块分别负责各自的功能。 建立两个project，rpcServer和rpcClient Server端 rpcServer包含api和provider模块，api定义对外暴露的服务和入参/出参对象 支付服务定义 123public interface IPayService &#123; String doPay(User user);&#125; 用户对象 1234567891011public class User implements Serializable &#123; private static final long serialVersionUID = 9169708774572711804L; private String name; public User(String name) &#123; this.name = name; &#125; //省略set/get方法&#125; 实现支付服务 12345678public class PayServiceImpl implements IPayService &#123; @Override public String doPay(User user) &#123; String result = String.format("%s 支付成功", user.getName()); System.out.println(result); return result; &#125;&#125; 服务定义和实现都有了，服务提供方接受请求时，需要找到对应的实例和方法，通过反射调用并返回结果，将请求类名、方法名和参数封装成Request对象。 123456789101112131415/** * 服务请求对象，保存请求的类名、方法名和请求入参对象 */public class Request implements Serializable &#123; private static final long serialVersionUID = 4512515646445414036L; private String className; private String methodName; private Object[] params; //...省略set/get方法&#125; 为了让客户端调用，服务端必须将服务发布，通过ServerSocket以BIO的方式接收客户端的请求，同时利用多线程提高请求处理的能力 12345678910111213141516171819202122public class PublishService &#123; /** * 通过线程池，提高请求的处理能力 */ private ExecutorService executorService = Executors.newCachedThreadPool(); /** * @param service 服务实例 * @param port 监听端口 */ public void publish(Object service, int port) &#123; try (ServerSocket serverSocket = new ServerSocket(port)) &#123; while (true) &#123; Socket socket = serverSocket.accept(); executorService.execute(new ProcessHandler(socket, service)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 每个请求由线程池调度，通过ProcessHandler处理，ProcessHandler的职责是将接受到的请求反序列化，解析请求，调用对应的服务，将结果序列化后返回给客户端。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class ProcessHandler implements Runnable &#123; private Socket socket; private Object service; public ProcessHandler(Socket socket, Object service) &#123; this.socket = socket; this.service = service; &#125; @Override public void run() &#123; ObjectInputStream ois = null; ObjectOutputStream oos = null; try &#123; //读取客户端的请求流 ois = new ObjectInputStream(socket.getInputStream()); Request request = (Request) ois.readObject(); //服务端处理请求 Object result = parseAndInvoke(request); //返回请求结果 oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(result); oos.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; finally &#123; if (null != ois) &#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (null != oos) &#123; try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 请求解析、服务方法调用 * * @param request * @return */ private Object parseAndInvoke(Request request) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; Object[] params = request.getParams(); Class&lt;?&gt;[] types = null; if(null != params)&#123; types = new Class&lt;?&gt;[params.length]; for (int i = 0; i &lt; params.length; i++) &#123; types[i] = params[i].getClass(); &#125; &#125; String className = request.getClassName(); String methodName = request.getMethodName(); Class clazz = Class.forName(className); Method method = clazz.getMethod(methodName, types); return method.invoke(service, params); &#125;&#125; Client端客户端发起远程调用，客户端无法获得远程服务的实例，只能通过生成的代理对象调用，代理对象将请求封装成RPC请求对象，通过网络传输调用远程服务。 123456789101112public class ProxyClient &#123; /** * 生成服务接口的代理对象 * @param clazz server暴露的服务接口 * @param &lt;T&gt; * @return */ public static &lt;T&gt; T newProxyInstance(Class clazz) &#123; return (T) Proxy.newProxyInstance(clazz.getClassLoader(), new Class[]&#123;clazz&#125;, new RemoteInvocationHandler()); &#125;&#125; RemoteInvocationHandler负责接口代理的具体实现，它将调用信息封装成Request对象，通过网络传输组件发起PRC请求 12345678910111213141516171819202122/** * 接口代理，将接口调用信息封装成Request * 通过网路传输组件发起RPC请求 */public class RemoteInvocationHandler implements InvocationHandler &#123; /** * @param proxy * @param method * @param args * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Request request = new Request(); request.setClassName(method.getDeclaringClass().getName()); request.setMethodName(method.getName()); request.setParams(args); return Transmission.send(request); &#125;&#125; Transmission是负责网络通信的组件，在本Demo中，就是一个简单的socket客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Transmission &#123; /** * 默认端口 */ private static final int PORT = 8080; /** * 默认服务地址 */ private static final String HOST = "localhost"; public static Object send(Request request) &#123; ObjectOutputStream oos = null; ObjectInputStream ois = null; Object result = null; try (Socket socket = new Socket(HOST, PORT)) &#123; oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(request); oos.flush(); ois = new ObjectInputStream(socket.getInputStream()); result = ois.readObject(); &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (null != oos) &#123; try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (null != ois) &#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return result; &#125;&#125; server启动 12345678public class ServerApplication &#123; public static void main(String[] args) &#123; IPayService payService = new PayServiceImpl(); new PublishService().publish(payService, 8080); &#125;&#125; client测试代码 123456789public class App &#123; public static void main(String[] args) &#123; IPayService payService = ProxyClient.newProxyInstance(IPayService.class); User user = new User("cj.luo"); String result = payService.doPay(user); System.out.println(result); &#125;&#125; 结果输出 1cj.luo 支付成功 Server调用过程的时序图如下： Client调用过程的时序图如下： 从Server时序图看，服务都需要通过PublishService发布，Server的服务如果越来越多，这种方式不仅代码冗余，难以维护，扩展性也很差，那么如何改进？可以引入Spring对要发布的服务进行管理，通过注解的方式，将服务给Spring托管。 委托Spring管理服务发布Spring可以将包路径下的所有标记为托管对象的Bean注册到Spring容器中，可以利用这个特性管理我们Server端的RPC服务。（demo通过Springboot-starter引入Spring依赖） 定义注解，用来标记哪些服务需要对外发布 1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Componentpublic @interface RemoteService &#123; /** * 发布的服务类 * @return */ Class&lt;?&gt; value();&#125; 标记要发布的服务 123456789@RemoteService(IPayService.class)public class PayServiceImpl implements IPayService &#123; @Override public String doPay(User user) &#123; String result = String.format("%s 支付成功", user.getName()); System.out.println(result); return result; &#125;&#125; Spring的InitializingBean接口为托管的Bean提供了初始化方法的方式，它有一个afterPropertiesSet方法，凡是继承该接口的类，在初始化bean的时候会执行该方法。服务实例化后，不需要通过调用PublishService进行服务发布，而是把PublishService交给Spring管理，它在初始化完成之后，扫描带有RemoteService注解的服务，将这些服务封装成一个map，通过afterPropertiesSet发布出去。 将PublishService交给Spring容器管理 1234567891011121314@Configuration@ComponentScan("com.cjluo.rpc.remote")public class SpringConfig &#123; /** * 默认端口8080 */ private static final int PORT = 8080; @Bean public PublishService getPublishService() &#123; return new PublishService(PORT); &#125;&#125; 修改后的PublishService 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class PublishService implements ApplicationContextAware, InitializingBean &#123; /** * 对外发布的服务Map */ private Map&lt;String, Object&gt; serviceHandlerMap = new HashMap&lt;&gt;(); private final int port; public PublishService(int port) &#123; this.port = port; &#125; /** * 通过线程池，提高请求的处理能力 */ private ExecutorService executorService = Executors.newCachedThreadPool(); @Override public void afterPropertiesSet() throws Exception &#123; try (ServerSocket serverSocket = new ServerSocket(this.port)) &#123; System.out.println("服务监听启动，端口号：" + this.port); while (true) &#123; Socket socket = serverSocket.accept(); executorService.execute(new ProcessHandler(socket, serviceHandlerMap)); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; //解析注解，将注解RemoteService的服务保存 Map&lt;String, Object&gt; serviceBeanMap = applicationContext.getBeansWithAnnotation(RemoteService.class); if (!serviceBeanMap.isEmpty()) &#123; for (Object service : serviceBeanMap.values()) &#123; RemoteService remoteService = service.getClass().getAnnotation(RemoteService.class); String className = remoteService.value().getName(); serviceHandlerMap.put(className, service); &#125; &#125; &#125;&#125; ProcessHandler中，通过解析请求，服务从serviceHandlerMap获取 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class ProcessHandler implements Runnable &#123; private Socket socket; private Map&lt;String, Object&gt; serviceHandlerMap; public ProcessHandler(Socket socket, Map&lt;String, Object&gt; serviceHandlerMap) &#123; this.socket = socket; this.serviceHandlerMap = serviceHandlerMap; &#125; @Override public void run() &#123; ObjectInputStream ois = null; ObjectOutputStream oos = null; try &#123; //读取客户端的请求流 ois = new ObjectInputStream(socket.getInputStream()); Request request = (Request) ois.readObject(); //服务端处理请求 Object result = parseAndInvoke(request); //返回请求结果 oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(result); oos.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; finally &#123; if (null != ois) &#123; try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (null != oos) &#123; try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 请求解析、服务方法调用 * * @param request * @return */ private Object parseAndInvoke(Request request) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; String className = request.getClassName(); Object service = serviceHandlerMap.get(className); if (null == service) &#123; throw new RuntimeException("service not found:" + className); &#125; Object[] params = request.getParams(); Class&lt;?&gt;[] types = null; if (null != params) &#123; types = new Class&lt;?&gt;[params.length]; for (int i = 0; i &lt; params.length; i++) &#123; types[i] = params[i].getClass(); &#125; &#125; String methodName = request.getMethodName(); Class clazz = Class.forName(className); Method method = clazz.getMethod(methodName, types); return method.invoke(service, params); &#125;&#125; 通过Springboot启动Server 1234567@SpringBootApplicationpublic class ServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServerApplication.class, args); &#125;&#125; 未修改Client直接调用远程服务 BIO优化为NIO前面以BIO的方式实现网络通信，在大量并发请求下容易出现性能瓶颈，改用NIO，通过Netty进行重构。 重构Server 修改PublishService类的afterPropertiesSet()方法 1234567891011121314151617181920212223242526272829public void afterPropertiesSet() throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workderGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workderGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel channel) throws Exception &#123; ChannelPipeline pipeline = channel.pipeline(); pipeline.addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, 4, 0, 4)); pipeline.addLast(new LengthFieldPrepender(4)); pipeline.addLast("encoder", new ObjectEncoder()); pipeline.addLast("decoder", new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast(new ProcessHandler(serviceHandlerMap)); &#125; &#125;).option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); ChannelFuture future = bootstrap.bind(port).sync(); System.out.println("服务监听启动，端口号：" + this.port); future.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workderGroup.shutdownGracefully(); &#125; &#125; ProcessHandler调整，继承ChannelInboundHandlerAdapter，重写channelRead 123456789101112131415161718192021222324252627public class ProcessHandler extends ChannelInboundHandlerAdapter &#123; private Map&lt;String, Object&gt; serviceHandlerMap; public ProcessHandler(Map&lt;String, Object&gt; serviceHandlerMap) &#123; this.serviceHandlerMap = serviceHandlerMap; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Object result = parseAndInvoke((Request) msg); ctx.write(result); ctx.flush(); ctx.close(); &#125; /** * 请求解析、服务方法调用 * * @param request * @return */ private Object parseAndInvoke(Request request) throws ClassNotFoundException, NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; //省略... &#125;&#125; 重构ClientClient只需要将网络传输的逻辑重构 123456789101112131415161718192021222324252627282930313233343536373839404142public class NettyTransmission &#123; /** * 默认端口 */ private static final int PORT = 8080; /** * 默认服务地址 */ private static final String HOST = "localhost"; public static Object send(Request request) &#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); final ConsumerHandler consumerHandler = new ConsumerHandler(); try &#123; bootstrap.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast("frameDecoder", new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 0, 4, 0, 4)); pipeline.addLast("frameEncoder", new LengthFieldPrepender(4)); pipeline.addLast("encoder", new ObjectEncoder()); pipeline.addLast("decoder", new ObjectDecoder(Integer.MAX_VALUE, ClassResolvers.cacheDisabled(null))); pipeline.addLast("handler", consumerHandler); &#125; &#125;); ChannelFuture future = bootstrap.connect(HOST, PORT).sync(); future.channel().writeAndFlush(request).sync(); future.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; group.shutdownGracefully(); &#125; return consumerHandler.getResponse(); &#125;&#125; ConsumerHandler用来读取Server返回的响应消息 123456789101112131415161718public class ConsumerHandler extends ChannelInboundHandlerAdapter &#123; private Object response; public Object getResponse() &#123; return response; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; response = msg; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(cause.getMessage()); &#125;&#125; 动态代理类最后改为Netty调用 123456789101112131415161718public class RemoteInvocationHandler implements InvocationHandler &#123; /** * @param proxy * @param method * @param args * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Request request = new Request(); request.setClassName(method.getDeclaringClass().getName()); request.setMethodName(method.getName()); request.setParams(args); return NettyTransmission.send(request); &#125;&#125; 总结基本的RPC框架远远不止以上代码，client如何获取server的地址、端口，如何进一步解耦。服务上线、下线如何通知client等等……这些可以通过引入注册中心来优化，这里就不一一实现，更多的是简述RPC的设计思想和原理。 代码参考 Server: https://github.com/luochunji/rpcServer Client: https://github.com/luochunji/rpcClient]]></content>
      <categories>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>RPC</tag>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种序列化方式的性能比较]]></title>
    <url>%2F2019%2F02%2F02%2F%E5%87%A0%E7%A7%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%B7%A5%E5%85%B7%E7%9A%84%E6%A8%AA%E5%90%91%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[序列化是（Serialization）是将对象的状态信息转换为可以存储或传输的形式的过程。 在序列化期间，对象将其当前状态写入到临时或持久性存储区。 以后，可以通过从存储区中读取或反序列化对象的状态，重新创建该对象。在分布式系统中，序列化是很重要的，直接关系数据的传输大小和效率，这里比较几种常见的序列化工具（框架）之间的性能差异。 JAVA原生序列化 JDK自带的序列化，对象必须实现Serializable接口，通过ObjectOutputStream输出二进制数据，反序列化通过ObjectInputStream生成目标对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class JavaSerializable extends AbstractSerializableByte &#123; public &lt;T&gt; JavaSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; ByteArrayOutputStream bos = null; ObjectOutputStream oos = null; try &#123; bos = new ByteArrayOutputStream(); oos = new ObjectOutputStream(bos); oos.writeObject(object); oos.flush(); return bos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (bos != null) &#123; bos.close(); &#125; if (oos != null) &#123; oos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; ByteArrayInputStream bis = null; ObjectInputStream ois = null; try &#123; bis = new ByteArrayInputStream(byteIn); ois = new ObjectInputStream(bis); return (T) ois.readObject(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != bis) &#123; bis.close(); &#125; if (null != ois) &#123; ois.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125;&#125; JSON工具JSON是一种轻量级的数据交换语言，该语言以易于让人阅读的文字为基础，用来传输由属性值或者序列性的值组成的数据对象，类似xml，Json比xml更小、更快更容易解析，JSON序列化框架有很多，主流的有下面三种：Gson、fastJson和Jackson fastJson fastJson是阿里巴巴的开源JSON解析库（github传送门），在企业中应用广泛，首先要导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122public class FastJsonSerializable extends AbstractSerializableString &#123; public &lt;T&gt; FastJsonSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; void doSerializable(T object) &#123; String str = serializableStr(object); deSerializable((Class&lt;T&gt;) object.getClass(), str); &#125; @Override public &lt;T&gt; String serializableStr(T object) &#123; return JSONObject.toJSONString(object); &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, String strIn) &#123; return JSONObject.parseObject(strIn, clazz); &#125;&#125; Gson Gson是Google公司发布的一个开源的Java库（github传送门），也是一个高效的JAVA对象序列化、反序列化框架。 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt;&lt;/dependency&gt; 实现上和fastJson没有太大差异 123456789101112131415161718192021222324public class GsonSerializable extends AbstractSerializableString &#123; private Gson gson; public &lt;T&gt; GsonSerializable(T object) &#123; super(object); &#125; @Override protected &lt;T&gt; void init(T object) &#123; //初始化 gson = new Gson(); &#125; @Override public &lt;T&gt; String serializableStr(T object) &#123; return gson.toJson(object); &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, String strIn) &#123; return gson.fromJson(strIn, clazz); &#125;&#125; Jackson Jackson（github传送门）也是java语言实现的开源工具，它是Spring中Json的默认实现，虽然多年未维护了，但依旧使用广泛。 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt; &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt; &lt;version&gt;1.9.13&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132333435public class JacksonSerializable extends AbstractSerializableByte &#123; private ObjectMapper mapper; public &lt;T&gt; JacksonSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; void init(T object) &#123; //初始化 mapper = new ObjectMapper(); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; try &#123; return mapper.writeValueAsBytes(object); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; try &#123; return mapper.readValue(byteIn,clazz); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; HessianHessian（官网传送门）是一个基于二进制的协议，Hessian支持很多种语言，例如Java、python、c++,、net/c#、D、Erlang、PHP、Ruby、object-c等，它的序列化和反序列化也是非常高效，与Java原生序列化一样，被序列化/反序列化的对象也必须实现Serializable接口，实现代码的写法也很像Java原生序列化。 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.caucho&lt;/groupId&gt; &lt;artifactId&gt;hessian&lt;/artifactId&gt; &lt;version&gt;4.0.51&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class HessianSerializable extends AbstractSerializableByte &#123; public &lt;T&gt; HessianSerializable(T object)&#123; super(object); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; ByteArrayOutputStream bos = null; Hessian2Output ho = null; try &#123; bos = new ByteArrayOutputStream(); ho = new Hessian2Output(bos); ho.writeObject(object); ho.flush(); return bos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != bos) &#123; bos.close(); &#125; if (null != ho) &#123; ho.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; return deSerializable(byteIn); &#125; private &lt;T&gt; T deSerializable(byte[] byteIn) &#123; ByteArrayInputStream bis = new ByteArrayInputStream(byteIn); Hessian2Input hInput = new Hessian2Input(bis); try &#123; return (T) hInput.readObject(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bis.close(); hInput.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125;&#125; Kryo Kryo是一个快速高效的Java序列化框架（github传送门），旨在提供快速、高效和易用的API。无论文件、数据库或网络数据Kryo都可以随时完成序列化。Kryo还可以执行自动深拷贝（克隆）、浅拷贝（克隆）。这是对象到对象的直接拷贝，而不是对象-&gt;字节-&gt;对象的拷贝。 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;4.0.2&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class KryoSerializable extends AbstractSerializableByte &#123; private Kryo kryo; public &lt;T&gt; KryoSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; void init(T object) &#123; kryo = new Kryo(); kryo.setReferences(false); //无需强制注册 kryo.setRegistrationRequired(false); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; ByteArrayOutputStream bos = null; Output output = null; try &#123; bos = new ByteArrayOutputStream(); output = new Output(bos); kryo.writeObject(output, object); &#125; finally &#123; try &#123; bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; output.close(); &#125; return bos.toByteArray(); &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; ByteArrayInputStream bis = new ByteArrayInputStream(byteIn); Input input = new Input(bis); try &#123; return kryo.readObject(input, clazz); &#125; finally &#123; try &#123; bis.close(); input.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; MessagePack It’s like JSON.but fast and small. —— messagePack官网（传送门）的定义，一句话足以说明它的特点 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.msgpack&lt;/groupId&gt; &lt;artifactId&gt;msgpack&lt;/artifactId&gt; &lt;version&gt;0.6.12&lt;/version&gt;&lt;/dependency&gt; messagepack要求序列化的对象上需要加@Message注解 1234@Messagepublic class User implements Serializable &#123; //...省略&#125; 实现 12345678910111213141516171819202122232425262728293031323334public class MessagePackSerializable extends AbstractSerializableByte &#123; private MessagePack msgpack; public &lt;T&gt; MessagePackSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; void init(T object) &#123; msgpack = new MessagePack(); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; try &#123; return msgpack.write(object); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; try &#123; return msgpack.read(byteIn, clazz); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; Protobuf protobuf（Protocol Buffers，官网传送门）是由Google公司发布的数据交换格式，提供跨语言、跨平台的序列化和反序列化实现，底层由C++实现，其他平台使用时必须使用protocol compiler进行预编译生成protoc二进制文件，原生的protobuf演示略显麻烦，这里只是为了做性能测试，因此使用百度公司对protobuf二次封装的API:jprotobuf（github传送门） jprotobuf是针对Java程序开发一套简易类库，目的是简化java语言对protobuf类库的使用，无需再去了解proto文件操作与语法，直接使用java注解定义字段类型即可。protobuf性能主要消耗在文件的预编译上，因此jprotobuf提供的maven预编译插件，在工程编译/打包时生成protobuf二进制文件。 引入依赖 123456789101112&lt;!--jprotobuf-precompile-plugin 支持maven编译时同时进行jprotobuf对象的预编译操作.--&gt;&lt;dependency&gt; &lt;groupId&gt;com.baidu&lt;/groupId&gt; &lt;artifactId&gt;jprotobuf-precompile-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.3&lt;/version&gt;&lt;/dependency&gt;&lt;!--百度封装的protobuf API--&gt;&lt;dependency&gt; &lt;groupId&gt;com.baidu&lt;/groupId&gt; &lt;artifactId&gt;jprotobuf&lt;/artifactId&gt; &lt;version&gt;2.1.11&lt;/version&gt;&lt;/dependency&gt; 引入插件 1234567891011121314151617&lt;plugin&gt; &lt;groupId&gt;com.baidu&lt;/groupId&gt; &lt;artifactId&gt;jprotobuf-precompile-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.3&lt;/version&gt; &lt;configuration&gt; &lt;!--&lt;skipErrorNoDescriptorsFound&gt;true&lt;/skipErrorNoDescriptorsFound&gt;--&gt; &lt;filterClassPackage&gt;com.baidu&lt;/filterClassPackage&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;precompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 实现 123456789101112131415161718192021222324252627282930313233343536373839public class ProtobufSerializable extends AbstractSerializableByte &#123; private Codec userCodec; public &lt;T&gt; ProtobufSerializable(T object) &#123; super(object); &#125; @Override public &lt;T&gt; void init(T object) &#123; this.userCodec = ProtobufProxy.create(object.getClass()); &#125; @Override public &lt;T&gt; byte[] serializableByte(T object) &#123; try &#123; return userCodec.encode(object); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public &lt;T&gt; T deSerializable(Class&lt;T&gt; clazz, byte[] byteIn) &#123; return deSerializable(byteIn); &#125; private &lt;T&gt; T deSerializable(byte[] byteIn) &#123; try &#123; return (T) userCodec.decode(byteIn); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 性能测试123456测试平台：Windows10硬件信息：CPU Core-i5-2430M 2.4GHz 内存8GJDK版本: 1.8.0_65测试方法：1、每组测试先对JVM进行预热 2、每种实现分三组进行，分别序列化一千、一万、十万次，每组执行100次，取平均值 首先，各执行一次，比较每种序列化后的字节长度，结果如下图 从结果看，java原生的序列化生成的字节大小非常大，hessian其次，三种json序列化大小一样，然后是protobuf、messagepack和kryo，接下来比较性能，测试方法如下： 123456789101112131415161718192021222324public class Test &#123; public static void main(String[] args) throws Exception &#123; User user = new User(); user.setName("小明"); user.setAge(18); long total = 0L; //根据实际的实现替换 ISerializable serializable = new ProtobufSerializable(user); //测试次数 int loop = 100; for (int i = 0; i &lt; loop; i++) &#123; long start = System.currentTimeMillis(); for (int j = 0; j &lt; Constant.total; j++) &#123; serializable.doSerializable(user); &#125; long time = System.currentTimeMillis() - start; System.out.printf("第%d次耗时:%dms \n", i + 1, time); total += time; &#125; System.out.printf("平均耗时:%dms \n", (total / loop)); &#125;&#125; 测试过程省略，直接看测试结果 从测试结果得出以下结论： Java原生的序列化/反序列化性能最差，生成的字节数也最大，无法跨语言； 三种Json序列化实现里，fastJson最好，jackson其次，gson最差，生成的字节数一样。 Hessian略好于java原生实现（dubbo对hessian做了优化，性能比原生的hessian更好），跨语言支持； protobuf &gt;&gt; kryo &gt; Messagepack，这三种序列化后生成的字节数相当，但是protobuf与另两种相比，性能有几倍甚至十几倍的提升，十万次的测试耗时仅相当于其他两种一万次的耗时 fastjson的性能最接近protobuf，阿里号称其自从发布以来，性能从未被其他Java实现的JSON库超越，果然牛逼！ 最后，测试的完整代码参见：源码 如有不对的地方，欢迎拍砖！]]></content>
      <categories>
        <category>性能</category>
        <category>序列化</category>
      </categories>
      <tags>
        <tag>序列化</tag>
        <tag>json</tag>
        <tag>hessian</tag>
        <tag>kryo</tag>
        <tag>messagepack</tag>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-定义规范和流程的模板模式]]></title>
    <url>%2F2019%2F01%2F09%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%AE%9A%E4%B9%89%E8%A7%84%E8%8C%83%E5%92%8C%E6%B5%81%E7%A8%8B%E7%9A%84%E6%A8%A1%E6%9D%BF%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[模板（Template）模式，经常用来作为一个业务逻辑的骨架，在父类定义执行流程，具体到每个方法，将实现细节延申到之类中。模板模式不允许子类改变执行流程（step1、step2、step3由父类定义），流程中的每一步骤可以由子类重写。 类图以演示的烹饪类及其子类为例 实例模板模式涉及到两个角色： 抽象模板：定义一个或多个抽象操作以及由这些操作组合而成的流程方法； 具体模板：继承抽象模板并实现它定义的抽象方法，抽象模板可以有多个具体模板，每个具体模板对抽象方法有不同的实现。 以烹饪为例，抽象模板定义烹饪的流程 1234567891011121314151617181920212223//烹饪抽象类public abstract class Cook &#123; //声明为final，烹饪的流程不允许子类重写 public final void cook()&#123; //1、放油 putOil(); //2、放食物 putFood(); //3、放调味料 putSeason(); //4、烹饪 cooking(); //5、起锅（通用的方法，由父类实现） finish(); &#125; abstract void putOil(); abstract void putFood(); abstract void putSeason(); abstract void cooking(); final void finish()&#123; System.out.println("起锅"); &#125;&#125; 12345678910111213141516171819202122//红烧肉public class CookPork extends Cook &#123; @Override void putOil() &#123; System.out.println("放入猪油"); &#125; @Override void putFood() &#123; System.out.println("放入五花肉"); &#125; @Override void putSeason() &#123; System.out.println("撒盐、三勺醋、十三香"); &#125; @Override void cooking() &#123; System.out.println("红烧"); &#125;&#125; 1234567891011121314151617181920212223//爆炒青菜public class CookVegetable extends Cook &#123; @Override void putOil() &#123; System.out.println("放入花生油"); &#125; @Override void putFood() &#123; System.out.println("放入青菜"); &#125; @Override void putSeason() &#123; System.out.println("撒盐"); &#125; @Override void cooking() &#123; System.out.println("爆炒"); &#125;&#125; 测试类 12345678910public class TemplateTest &#123; public static void main(String[] args) &#123; Cook cookPork = new CookPork(); cookPork.cook(); System.out.println("-------------------"); Cook cookVegetable = new CookVegetable(); cookVegetable.cook(); &#125;&#125; 输出结果 1234567891011放入猪油放入五花肉撒盐、三勺醋、十三香红烧起锅-------------------放入花生油放入青菜撒盐爆炒起锅 使用场景父类定义规范和流程，子类实现每个步骤的具体流程，如JDBC的规范：开启事务、获取Session、提交事务、关闭Session，回滚等，这些步骤由不同ORM框架厂商实现并遵守，如Hibernate、Mybatis，开发人员不需要重复写这些代码，只要关注具体的CURD操作。再比如J2EE的servlet规范，定义了一套init/doGet/doPost/service……方法，这些方法不是抽象的，在servlet有默认的实现，但是它允许开发者重写这些方法，整体的调用流程还是由servlet控制，大大提高可扩展性，SpringMVC就是基于servlet的模板方法扩展； 总结模板模式的优点： 封装性好，在父类定义规范和不可变的流程，子类实现细节； 扩展性好，由子类实现方法扩展和修改，符合开闭原则； 提高代码复用； 缺点： 通过继承方式，导致每个不同的逻辑都必须要一个子类来实现，假如只有step3不同，其他步骤完全一致，也要两个类来分别实现，随着实现逻辑的增多，类会越来越庞大； 子类的逻辑会影响流程的执行结果，在代码阅读上增加了难度；]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>模板模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-过程不同结果相似的策略模式]]></title>
    <url>%2F2019%2F01%2F07%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%87%E7%A8%8B%E4%B8%8D%E5%90%8C%E7%BB%93%E6%9E%9C%E7%9B%B8%E4%BC%BC%E7%9A%84%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[从一个地点去另一个地点，你可以选择步行、出租车、公交车等各种交通工具，最终目的是到达目的地。每一种交通工具，还可以选择多种路线方案，避开拥堵、走捷径等等，每一种方案都是一种策略，这就是策略模式。 定义 简单来说，策略模式（Strategy）是一种过程不同、结果相同或者相似的设计模式。在程序设计中，为某个行为（例如查找、排序）定义一系列的算法并封装起来提供给调用者，最终由调用者决定使用哪一种，这里每一个算法都是一种策略，为了保证策略的一致性，通常会用一个抽象的策略类来定义算法，具体的细节由每一个策略类实现。 实例策略模式由三个角色组成 上下文 策略抽象 策略实现 以排序为例，传入一个未排序数字数组，最终返回有序数组，先定义策略抽象 1234public interface SortStrategy &#123; Integer[] sort(Integer[] unSortArray);&#125; 写三个排序算法：冒泡法、选择法、插入法 1234567891011121314151617181920212223//冒泡排序public class BubbleSort implements SortStrategy &#123; @Override public Integer[] sort(Integer[] arr) &#123; int i, temp, len = arr.length; boolean changed; do &#123; changed = false; len -= 1; for (i = 0; i &lt; len; i++) &#123; if (arr[i] &gt; arr[i + 1]) &#123; temp = arr[i]; arr[i] = arr[i + 1]; arr[i + 1] = temp; changed = true; &#125; &#125; &#125; while (changed); System.out.println("冒泡排序结果:"+ Arrays.toString(arr)); return arr; &#125;&#125; 123456789101112131415161718192021//选择排序public class SelectionSort implements SortStrategy &#123; @Override public Integer[] sort(Integer[] arr) &#123; int min, temp; for (int i = 0; i &lt; arr.length; i++) &#123; min = i; for (int j = i+1; j &lt; arr.length; j++) &#123; if (arr[j] &lt; arr[min]) &#123; min = j; &#125; &#125; temp = arr[min]; arr[min] = arr[i]; arr[i] = temp; &#125; System.out.println("选择排序结果:"+ Arrays.toString(arr)); return arr; &#125;&#125; 1234567891011121314151617//插入排序public class InsertionSort implements SortStrategy &#123; @Override public Integer[] sort(Integer[] arr) &#123; for (int i = 1; i &lt; arr.length; i++ ) &#123; int temp = arr[i]; int j = i - 1; for (; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j-- ) &#123; arr[j + 1] = arr[j]; &#125; arr[j + 1] = temp; &#125; System.out.println("插入排序结果:"+ Arrays.toString(arr)); return arr; &#125;&#125; 定义上下文 123456789101112public class Context &#123; private SortStrategy strategy; //必须调用者指定策略 public Context(SortStrategy strategy) &#123; this.strategy = strategy; &#125; public Integer[] doSort(Integer[] array)&#123; return strategy.sort(array); &#125;&#125; 测试类 1234567891011121314151617public class StrategyTest &#123; public static void main(String[] args) &#123; Integer[] array = new Integer[]&#123;8,7,4,9,5,3,1,2,6&#125;; Context context; //策略：冒泡排序 context = new Context(new BubbleSort()); context.doSort(array); //策略：插入排序 context = new Context(new InsertionSort()); context.doSort(array); //策略：选择排序 context = new Context(new SelectionSort()); context.doSort(array); &#125;&#125; 输出结果： 123冒泡排序结果:[1, 2, 3, 4, 5, 6, 7, 8, 9]插入排序结果:[1, 2, 3, 4, 5, 6, 7, 8, 9]选择排序结果:[1, 2, 3, 4, 5, 6, 7, 8, 9] 适用场景策略模式是对算法的封装，它把算法和结果分开，调用者需要知道都有哪些策略，而不必关心具体的实现细节，从而达到调用不同策略产生不同行为的结果。策略模式把选择权交给调用者，并且允许调用者自由替换，大大提高了灵活性，策略模式通常应用在以下场景： Java的比较器Comparator； 各种排序、查找、加密算法的封装； 调用者只关心结果，不关心过程； 存在大量if…else判断的复杂逻辑场景 总结策略模式的优点： 算法封装，完全符合“开闭原则”，不修改代码的基础上也可以更换实现策略，非常灵活； 避免过多的if…else造成可读性差的烂代码 缺点： 调用者必须知道所有的策略且策略要明确告知调用者运行结果； 一定程度上增加了类的数量；]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>策略模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例的破坏和防御]]></title>
    <url>%2F2019%2F01%2F06%2F%E5%8D%95%E4%BE%8B%E7%9A%84%E7%A0%B4%E5%9D%8F%E5%92%8C%E9%98%B2%E5%BE%A1%2F</url>
    <content type="text"><![CDATA[上篇介绍了几种单例的写法，这一片介绍如何破坏单例以及防御 反射破坏思考一个问题，单例真的安全吗？以懒汉式单例为例，将构造器声明为private，是不是就万无一失，无法用构造器对其实例化？在Java的特性——反射面前，私有构造器也不起作用了。 12345678910111213141516171819202122public static void main(String[] args) throws InvocationTargetException, NoSuchMethodException, InstantiationException, IllegalAccessException &#123; LazySingleton singleton1 = LazySingleton.getInstance(); LazySingleton singleton2 = getInstanceByConstruct(); System.out.println("singleton1 == singleton2 :" + (singleton1 == singleton2));&#125;/** * 通过反射实例化对象 * @return * @throws NoSuchMethodException * @throws IllegalAccessException * @throws InvocationTargetException * @throws InstantiationException */public static LazySingleton getInstanceByConstruct() throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Class clazz = LazySingleton.class; Constructor constructor = clazz.getDeclaredConstructor(); //访问权限 constructor.setAccessible(true); return (LazySingleton) constructor.newInstance();&#125; 运行结果 1singleton1 == singleton2 :false 通过反射成功实例化对象，单例破坏，那么要如何防御？ 防御反射破坏既然是通过构造器实例化，自然而然可以想到，可以在构造器里防止单例被破坏。 123456private LazySingleton() &#123; //加上判断，防止通过构造器被再次实例化 if(null != instance)&#123; throw new RuntimeException("单例已存在"); &#125;&#125; 再运行一次main方法 12345678910Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:422) at com.cjluo.chapter1.singleton.test.BreakSingleton.getInstanceByConstruct(BreakSingleton.java:35) at com.cjluo.chapter1.singleton.test.BreakSingleton.main(BreakSingleton.java:17)Caused by: java.lang.RuntimeException: 单例已存在 at com.cjluo.chapter1.singleton.LazySingleton.&lt;init&gt;(LazySingleton.java:18) ... 6 more 抛出异常，实例化失败，提示单例已存在，成功防御通过反射对单例的破坏。但是，单例安全了吗？还可以通过其他方式破坏吗？当然可以，接下来介绍如何通过拷贝对象破坏单例。 对象拷贝破坏前面介绍了原型模式和深拷贝，这里就通过深拷贝破坏单例 1234567891011121314151617181920212223242526272829303132333435363738394041public static void main(String[] args) throws InvocationTargetException, NoSuchMethodException, InstantiationException, IllegalAccessException &#123; LazySingleton singleton1 = LazySingleton.getInstance(); LazySingleton singleton3 = deepClone(singleton1); System.out.println("singleton1 == singleton2 :" + (singleton1 == singleton3)); &#125;/** * 深拷贝 * @param singleton * @return */public static LazySingleton deepClone(LazySingleton singleton) &#123; ByteArrayOutputStream bos = null; ObjectOutputStream oos = null; ByteArrayInputStream bis; ObjectInputStream ois; try &#123; bos = new ByteArrayOutputStream(); oos = new ObjectOutputStream(bos); oos.writeObject(singleton); bis = new ByteArrayInputStream(bos.toByteArray()); ois = new ObjectInputStream(bis); return (LazySingleton) ois.readObject(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bos.close(); oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null;&#125; 运行结果 1singleton1 == singleton2 :false 对象被序列化、反序列化后生成的实例，已经破坏了单例。这种方式的破坏如何防御呢？先从readObject()源码分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120/** * ObjectInputStream.readObject * */public final Object readObject() throws IOException, ClassNotFoundException&#123; if (enableOverride) &#123; return readObjectOverride(); &#125; // if nested read, passHandle contains handle of enclosing object int outerHandle = passHandle; try &#123; //进入readObject0方法 Object obj = readObject0(false); handles.markDependency(outerHandle, passHandle); ClassNotFoundException ex = handles.lookupException(passHandle); if (ex != null) &#123; throw ex; &#125; if (depth == 0) &#123; vlist.doCallbacks(); &#125; return obj; &#125; finally &#123; passHandle = outerHandle; if (closed &amp;&amp; depth == 0) &#123; clear(); &#125; &#125;&#125;private Object readObject0(boolean unshared) throws IOException &#123; boolean oldMode = bin.getBlockDataMode(); if (oldMode) &#123; int remain = bin.currentBlockRemaining(); if (remain &gt; 0) &#123; throw new OptionalDataException(remain); &#125; else if (defaultDataEnd) &#123; /* * Fix for 4360508: stream is currently at the end of a field * value block written via default serialization; since there * is no terminating TC_ENDBLOCKDATA tag, simulate * end-of-custom-data behavior explicitly. */ throw new OptionalDataException(true); &#125; bin.setBlockDataMode(false); &#125; byte tc; while ((tc = bin.peekByte()) == TC_RESET) &#123; bin.readByte(); handleReset(); &#125; depth++; try &#123; //判断对象类型 switch (tc) &#123; case TC_NULL: return readNull(); case TC_REFERENCE: return readHandle(unshared); case TC_CLASS: return readClass(unshared); case TC_CLASSDESC: case TC_PROXYCLASSDESC: return readClassDesc(unshared); //字符串 case TC_STRING: case TC_LONGSTRING: return checkResolve(readString(unshared)); //数组 case TC_ARRAY: return checkResolve(readArray(unshared)); //枚举 case TC_ENUM: return checkResolve(readEnum(unshared)); //对象 case TC_OBJECT: return checkResolve(readOrdinaryObject(unshared)); case TC_EXCEPTION: IOException ex = readFatalException(); throw new WriteAbortedException("writing aborted", ex); case TC_BLOCKDATA: case TC_BLOCKDATALONG: if (oldMode) &#123; bin.setBlockDataMode(true); bin.peek(); // force header read throw new OptionalDataException( bin.currentBlockRemaining()); &#125; else &#123; throw new StreamCorruptedException( "unexpected block data"); &#125; case TC_ENDBLOCKDATA: if (oldMode) &#123; throw new OptionalDataException(true); &#125; else &#123; throw new StreamCorruptedException( "unexpected end of block data"); &#125; default: throw new StreamCorruptedException( String.format("invalid type code: %02X", tc)); &#125; &#125; finally &#123; depth--; bin.setBlockDataMode(oldMode); &#125; &#125; 反序列化时，会先判断对象的类型，String、Array、Object等，如果是Object，进入readOrdinaryObject方法读取对象。 123456if (bin.readByte() != TC_OBJECT) &#123; throw new InternalError();&#125;//读取对象的属性、方法，后面会用到ObjectStreamClass desc = readClassDesc(false);desc.checkDeserialize(); 首先，readClassDesc会读取对象的属性和几个特殊方法（writeObjectMethod、readObjectMethod、readObjectNoDataMethod、writeReplaceMethod、readResolveMethod），紧接着再次判断对象类型是否为普通的对象，然后进行实例化。 123456789101112131415Class&lt;?&gt; cl = desc.forClass(); if (cl == String.class || cl == Class.class || cl == ObjectStreamClass.class) &#123; throw new InvalidClassException("invalid class descriptor"); &#125; Object obj; try &#123; //对象实例化 obj = desc.isInstantiable() ? desc.newInstance() : null; &#125; catch (Exception ex) &#123; throw (IOException) new InvalidClassException( desc.forClass().getName(), "unable to create instance").initCause(ex); &#125; 接下来的几行代码是各种校验，重点的代码在这里 1234567891011121314//重点在这行，hasReadResolveMethod判断是否有readResolve方法 if (obj != null &amp;&amp; handles.lookupException(passHandle) == null &amp;&amp; desc.hasReadResolveMethod()) &#123; //调用ReadResolve方法 Object rep = desc.invokeReadResolve(obj); if (unshared &amp;&amp; rep.getClass().isArray()) &#123; rep = cloneArray(rep); &#125; if (rep != obj) &#123; handles.setObject(passHandle, obj = rep); &#125; &#125; 读取readResolve方法 1234567891011121314151617181920212223Object invokeReadResolve(Object obj) throws IOException, UnsupportedOperationException&#123; requireInitialized(); if (readResolveMethod != null) &#123; try &#123; return readResolveMethod.invoke(obj, (Object[]) null); &#125; catch (InvocationTargetException ex) &#123; Throwable th = ex.getTargetException(); if (th instanceof ObjectStreamException) &#123; throw (ObjectStreamException) th; &#125; else &#123; throwMiscException(th); throw new InternalError(th); // never reached &#125; &#125; catch (IllegalAccessException ex) &#123; // should not occur, as access checks have been suppressed throw new InternalError(ex); &#125; &#125; else &#123; throw new UnsupportedOperationException(); &#125;&#125; 防御拷贝破坏如果我们要防止单例被深拷贝破坏，关键点就在于对象是否有readResolve方法。在LazySingleton加上该方法。 123private Object readResolve()&#123; return instance;&#125; 再次运行 1singleton1 == singleton2 :true 无法破坏的单例前文有提到一种单例写法，是很完美的单例——枚举单例，它既保证线程安全，又不需要任何同步加锁操作，除此之外，它可以防御以上两种单例的破坏，因为JDK在底层实现上就做了防御处理。 1234567891011121314public enum EnumSingleton &#123; /** * 枚举实例 */ INSTANCE; EnumSingleton() &#123; &#125; public void method()&#123; &#125;&#125; 通过反射获取构造器实例化单例 1234567891011121314 public static void main(String[] args) throws InvocationTargetException, NoSuchMethodException, InstantiationException, IllegalAccessException &#123; EnumSingleton enumSingleton = EnumSingleton.INSTANCE; EnumSingleton enumSingleton1 = getInstance(); System.out.println("enumSingleton == enumSingleton1 :" + (enumSingleton == enumSingleton1)); &#125; public static EnumSingleton getInstance() throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Class clazz = EnumSingleton.class; Constructor constructor = clazz.getDeclaredConstructor(String.class,int.class); //访问权限 constructor.setAccessible(true); return (EnumSingleton) constructor.newInstance(); &#125; 运行结果，无法通过反射实例化枚举对象 1234Exception in thread "main" java.lang.IllegalArgumentException: Cannot reflectively create enum objects at java.lang.reflect.Constructor.newInstance(Constructor.java:416) at com.cjluo.chapter1.singleton.test.BreakSingleton.getInstance(BreakSingleton.java:49) at com.cjluo.chapter1.singleton.test.BreakSingleton.main(BreakSingleton.java:22) 对应的源码 12345678910111213141516171819202122@CallerSensitivepublic T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException&#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); &#125; &#125; //对class类型进行校验，如果是枚举，直接抛出异常 if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException("Cannot reflectively create enum objects"); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) &#123; ca = acquireConstructorAccessor(); &#125; @SuppressWarnings("unchecked") T inst = (T) ca.newInstance(initargs); return inst;&#125; JDK底层将反射实例化的类型做了限制，排除了枚举。因此这种方式无法破坏枚举单例，那么通过深拷贝的方式呢？（深拷贝代码与上述相似，不再列举） 123456public static void main(String[] args) throws InvocationTargetException, NoSuchMethodException, InstantiationException, IllegalAccessException &#123; EnumSingleton enumSingleton = EnumSingleton.INSTANCE; EnumSingleton enumSingleton1 = deepClone(enumSingleton); System.out.println("enumSingleton == enumSingleton1 :" + (enumSingleton == enumSingleton1)); &#125; 运行结果： 1enumSingleton == enumSingleton1 :true 通过对象拷贝，不需要额外处理，枚举的单例也不会被破坏。分析源码readObject0方法 12345678910111213141516171819202122private Object readObject0(boolean unshared) throws IOException &#123; //省略... depth++; try &#123; //判断对象类型 switch (tc) &#123; //省略... //枚举 case TC_ENUM: return checkResolve(readEnum(unshared)); //对象 case TC_OBJECT: return checkResolve(readOrdinaryObject(unshared)); default: throw new StreamCorruptedException( String.format("invalid type code: %02X", tc)); &#125; &#125; finally &#123; depth--; bin.setBlockDataMode(oldMode); &#125; &#125; readEnum，返回枚举实例 123456789101112131415161718192021222324252627282930313233343536373839private Enum&lt;?&gt; readEnum(boolean unshared) throws IOException &#123; if (bin.readByte() != TC_ENUM) &#123; throw new InternalError(); &#125; ObjectStreamClass desc = readClassDesc(false); if (!desc.isEnum()) &#123; throw new InvalidClassException("non-enum class: " + desc); &#125; int enumHandle = handles.assign(unshared ? unsharedMarker : null); ClassNotFoundException resolveEx = desc.getResolveException(); if (resolveEx != null) &#123; handles.markException(enumHandle, resolveEx); &#125; String name = readString(false); Enum&lt;?&gt; result = null; Class&lt;?&gt; cl = desc.forClass(); if (cl != null) &#123; try &#123; @SuppressWarnings("unchecked") //获取枚举实例 Enum&lt;?&gt; en = Enum.valueOf((Class)cl, name); result = en; &#125; catch (IllegalArgumentException ex) &#123; throw (IOException) new InvalidObjectException( "enum constant " + name + " does not exist in " + cl).initCause(ex); &#125; if (!unshared) &#123; handles.setObject(enumHandle, result); &#125; &#125; handles.finish(enumHandle); passHandle = enumHandle; return result;&#125; 与readOrdinaryObject不同的是，Object对象通过newInstance实例化新的对象，而枚举对象通过Enum.valueOf获取的还是枚举对象自身，readEnum不会继续后面的resolveMethod判断，直接返回当前枚举实例。因此，枚举对象的实例，即使通过深拷贝，也始终是它自身。 总结1、普通的单例可以通过反射、深拷贝破坏 2、普通的单例防御单例破坏，需在构造器中加校验，在对象中加以下方法，返回当前实例 123private Object readResolve()&#123; return instance;&#125; 3、枚举单例无法被反射和深拷贝破坏]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-委派模式]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A7%94%E6%B4%BE%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[委派（Delegate）模式，有两个类参与处理同一个请求，接受请求的类将请求委托给另一个类（被委派对象）来处理，具有保护被委派对象，隐藏实现细节的功能。委派模式至少必须两个类参与，以聚合取代继承。 实例项目外包：A公司从甲方承接了一个工程项目，然后外包给B公司，三个角色： 甲方（调用者） A公司（委派对象） B公司（被委派对象） 123456public class CompanyB &#123; public void doSomething()&#123; System.out.println("CompanyB doSomething"); &#125;&#125; 12345678public class CompanyA &#123; //实例化CompanyB private CompanyB delegate = new CompanyB(); //真正实施这个工程的是B公司 public void doSomething()&#123; delegate.doSomething(); &#125;&#125; 测试类 1234public static void main(String[] args) &#123; CompanyA companyA = new CompanyA(); companyA.doSomething();&#125; 结果 1CompanyB doSomething 特点 A类（委派类）和B类（被委派类）之间没有继承关系，A类中有一个属性为类B； A类B类有相同的方法和属性 与代理模式的区别代理模式满足两个角色：代理对象和被代理对象，与委派模式很像，但他们区别在于： 代理模式的代理类和被代理类可能有继承关系（cglib代理），委派模式类与类之间不需要继承； 代理类在调用代理类前/后做前置/后置处理，委派模式只是做单纯的调用； 委派模式可以层层委派，比如上例中B公司可以把工程再外包给C公司、C公司再外包给D公司……]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>委派模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-原型模式]]></title>
    <url>%2F2019%2F01%2F04%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[原型模式（Prototype）是用于拷贝一个已经存在的实例，返回新的实例，被复制的实例就是所谓的“原型”，尤其是拷贝复杂对象，创建新的实例非常耗时，原型模式就可以解决性能的问题。 克隆何为克隆？就是复制一个一模一样的实例，Java中是通过clone方法来实现，以西游记中真假美猴王为例，假猴王有着和真猴王一模一样的长相、武器装备甚至各种能力，但是他们是两个不同的实例，可以说，假猴王是真猴王克隆的，Java中，可被克隆的对象一定要实现Cloneable接口，并且一定要覆盖父类的clone方法。 美猴王： 12345678910111213141516//真猴王，实现了Cloneable接口并覆盖父类clone方法public class MonkeyKing implements Cloneable &#123; //身高 private int height; //金箍棒 private GoldenStick goldenStick = new GoldenStick(); @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; //。。。省略set/get方法&#125; 金箍棒： 123456public class GoldenStick &#123; //金箍棒长度 private int length = 2; //。。。省略set/get方法&#125; 克隆一个假猴王（真猴王悟空，假猴王六耳猕猴） 1234567891011121314151617public class CloneTest &#123; public static void main(String[] args) &#123; try &#123; MonkeyKing wukong = new MonkeyKing(); wukong.setHeight(150); MonkeyKing mihou = (MonkeyKing) wukong.clone(); System.out.println("悟空 == 六耳猕猴: " + (wukong == mihou)); System.out.println("悟空王身高：" + wukong.getHeight()); System.out.println("六耳猕猴身高：" + mihou.getHeight()); System.out.println("悟空的金箍棒 == 六耳猕猴的金箍棒：" + (wukong.getGoldenStick() == mihou.getGoldenStick())); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果： 1234悟空 == 六耳猕猴： false悟空身高：150六耳猕猴身高：150悟空的金箍棒 == 六耳猕猴的金箍棒：true 真猴王的金箍棒 == 假猴王的金箍棒？显然是不对的，在这个例子中，假猴王的金箍棒复制的是真猴王金箍棒的引用，实际上他们仍然指向同一个金箍棒对象，这里引申出另一个概念：深拷贝和浅拷贝 浅拷贝Java中，除了基本数据类型和String以外，其他对象使用赋值操作，赋值的是该对象的引用，不是值。只是将引用传递过去，实际上还指向原来的对象，这就是浅拷贝。 深拷贝浅拷贝clone方法只能克隆当前基本数据类型成员变量的值，如果成员变量包含其他对象，那么必须在clone方法里逐一克隆成员变量。 GoldenStick实现Cloneable接口 1234567891011public class GoldenStick implements Cloneable&#123; private int length = 2; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; //。。。省略get/set方法&#125; 在MonkeyKing的clone方法中重写clone实现 123456789101112131415161718public class MonkeyKing implements Cloneable&#123; //身高 private int height; //金箍棒 private GoldenStick goldenStick = new GoldenStick(); @Override protected Object clone() throws CloneNotSupportedException &#123; MonkeyKing mihou = (MonkeyKing) super.clone(); //克隆金箍棒 mihou.goldenStick = (GoldenStick) goldenStick.clone(); return mihou; &#125; //。。。省略get/set方法&#125; 运行CloneTest 1234悟空 == 六耳猕猴： false悟空身高：150六耳猕猴身高：150悟空的金箍棒 == 六耳猕猴的金箍棒：false 结果可见，真假美猴王的金箍棒不再是同一个了。 但是，假如类有上百个成员变量，美猴王除了金箍棒还有紧箍咒、虎皮裙。。。按照这种方式是不是每个成员变量都要实现克隆接口，重写clone方法，然后在MonkeyKing中逐一克隆？显然这是一种效率低下且繁琐的做法，有什么办法能改进？没错，字节码拷贝！所有的类最终都是生成字节码，可以通过拷贝字节码来创建新的对象。 字节码拷贝字节码拷贝的前提条件是类必须是可序列化的，因此待拷贝的对象和其成员变量都要实现序列化接口 123public class MonkeyKing implements Serializable&#123; //省略&#125; 123public class GoldenStick implements Serializable&#123; //省略&#125; 拷贝方法和测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ClassDeepCopy &#123; //字节码深拷贝 public static MonkeyKing deepClone(MonkeyKing monkeyKing) &#123; ByteArrayOutputStream bos = null; ObjectOutputStream oos = null; ByteArrayInputStream bis = null; ObjectInputStream ois = null; try &#123; bos = new ByteArrayOutputStream(); oos = new ObjectOutputStream(bos); oos.writeObject(monkeyKing); bis = new ByteArrayInputStream(bos.toByteArray()); ois = new ObjectInputStream(bis); return (MonkeyKing) ois.readObject(); &#125; catch (Throwable e) &#123; &#125; finally &#123; try &#123; bos.close(); oos.close(); bis.close(); ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null; &#125; public static void main(String[] args) &#123; MonkeyKing wukong = new MonkeyKing(); wukong.setHeight(150); MonkeyKing mihou = ClassDeepCopy.deepClone(wukong); System.out.println("悟空 == 六耳猕猴： " + (wukong == mihou)); System.out.println("悟空身高：" + wukong.getHeight()); System.out.println("六耳猕猴身高：" + mihou.getHeight()); System.out.println("悟空的金箍棒 == 六耳猕猴的金箍棒：" + (wukong.getGoldenStick() == mihou.getGoldenStick())); &#125;&#125; 测试结果： 1234悟空 == 六耳猕猴： false悟空身高：150六耳猕猴身高：150悟空的金箍棒 == 六耳猕猴的金箍棒：false 使用场景 类的成员变量不多，又不想通过new方法来创建实例； 难以根据类生成实例； 类实例生成与框架解耦； 总结原型模式优点： 通过原型模式创建实例比new对象效率更高，对性能消耗明显或者对象复杂的类有显著作用； 对比New生成实例，不需要关心构造方法，简化创建实例； 缺点： 需要为每个非基本类型或String类型的成员变量重写clone方法，如果成员变量多，很繁琐； 如果修改已有的类的代码，违背了开闭原则；]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>原型模式</tag>
        <tag>浅拷贝</tag>
        <tag>深拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-单例的几种写法]]></title>
    <url>%2F2019%2F01%2F03%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[单例是Java中最简单也是最常用的设计模式之一，它属于创建型模式，在JVM的启动到停止期间，单例只允许类存在一个实例，这个类的实例是由类自己创建并提供给其他类访问，单例的三个特点： jvm中始终只有一个实例 由类自己创建 为其他类提供唯一实例 单例有多种写法，以下列举最常用的七种写法。 懒汉式懒汉式单例声明一个静态的instance = null的对象，因此类在初始化时，类并不会马上实例化，外部对象要调用该类的实例，只能通过getInstance方法，在第一次调用时进行实例化，起到延迟加载的作用。getInstance方法加了同步锁保证线程安全，但在并发时会造成性能瓶颈 12345678910111213141516171819202122public class LazySingleton &#123; /** * 类加载时并没有初始化（延迟加载） */ private static LazySingleton instance = null; private LazySingleton() &#123; //构造函数私有化 &#125; /** * 使用内置同步锁，线程安全，但在并发下会造成性能问题 * @return */ public static synchronized LazySingleton getInstance() &#123; if (null == instance) &#123; instance = new LazySingleton(); &#125; return instance; &#125;&#125; 饿汉式饿汉式单例与懒汉式的区别在于： 类初始化时就进行实例化； getInstance没有加锁; 没有延迟加载，也就意味着即使没有其他类调用，这个实例也一直存在，浪费内存资源，正因为初始化时就实例化，因此不存在线程安全问题，无需在getInstance上加同步锁； 1234567891011121314151617public class EagerSingleton &#123; /** * 类加载时已经初始化 */ private static EagerSingleton instance = new EagerSingleton(); private EagerSingleton() &#123; //私有化构造函数 &#125; /** * 直接返回已经类加载时已经实例化的对象，不存在线程安全问题 * @return */ public static EagerSingleton getInstance() &#123; return instance; &#125;&#125; 静态内部类式这种方式利用了 classloader 机制来保证初始化 instance 时只有一个线程，即使外部类被加载了，instance 还是未初始化，因为 SingletonHolder 类没有被显式调用，只有通过显式调用 getInstance 方法时，才会显式装载 SingletonHolder 类，从而实例化 instance。如果实例化 instance 很消耗资源，所以想让它延迟加载，另外一方面，又不希望在 Singleton 类加载时就实例化，因为不能确保 Singleton 类还可能在其他的地方被显示调用从而被加载，那么这个时候用这种写法既可以实现延迟加载，又能保证线程安全性。 123456789101112131415161718192021public class InnerSingleton &#123; /** * 静态内部类在单例类初始化时不会初始化 */ private static class SingletonHolder &#123; private static InnerSingleton INSTANCE = new InnerSingleton(); &#125; private InnerSingleton() &#123; &#125; /** * 调用getInstance方法时，才去加载内部类，懒加载的同时也节约空间，同时也是线程安全的 * * @return */ public static InnerSingleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 双重校验式​双重校验在没有加volatile修饰符时是线程不安全的，多个线程之间彼此看到不到最新的变量值。volatile保证线程之间内存可见性。这种写法虽然用了同步锁，但只有第一次加载时会加锁，因此不存在性能问题。 1234567891011121314151617181920212223242526public class DoubleCheckSingleton &#123; /** * volatile修饰对象，保证线程间内存可见性的问题 */ private volatile static DoubleCheckSingleton instance; private DoubleCheckSingleton() &#123; &#125; public static DoubleCheckSingleton getInstance() &#123; //没有加volatile修饰符时，线程不安全，过程如下 //1、线程AB同时到达这里，此时instance == null if (instance == null) &#123; //2、只有一个线程能进入同步方法，假设A进入，线程B等待 synchronized (DoubleCheckSingleton.class) &#123; //3、线程A先进入，new对象后退出返回实例 //4、线程B后进入，因为线程A实例化后的类，对线程B不可见，此时instance == null，因 此线程B也进行new对象后返回实例 if (instance == null) &#123; instance = new DoubleCheckSingleton(); &#125; &#125; &#125; return instance; &#125;&#125; 枚举式枚举式单例可以说是最简洁、最高效的单例模式，用最少的代码既保证实例唯一，又保证线程安全；缺点只有一个：它无法被继承。 1234567891011public enum EnumSingleton &#123; /** * 枚举变量 */ INSTANCE; public void method()&#123; &#125;&#125; ThreadLocal式很多时候，我们都在考虑线程间变量共享，而ThreadLocal刚好相反，它为每个线程提供一份独立的副本，隔离线程之间对同一变量访问的冲突，互不影响。 12345678910111213141516171819public class ThreadLocalSingleton &#123; /** * ThreadLocal为每个线程提供了副本，互不影响 */ private static final ThreadLocal&lt;ThreadLocalSingleton&gt; TL_SINGLETON = new ThreadLocal&lt;ThreadLocalSingleton&gt;() &#123; @Override protected ThreadLocalSingleton initialValue() &#123; return new ThreadLocalSingleton(); &#125; &#125;; private ThreadLocalSingleton() &#123; &#125; public static ThreadLocalSingleton getInstance() &#123; return TL_SINGLETON.get(); &#125;&#125; CAS式要获得唯一的实例，可以用利用AtomicReference的原子性，利用底层的CAS来实现 123456789101112131415161718192021222324public class CASSingleton &#123; private static AtomicReference&lt;CASSingleton&gt; instance = new AtomicReference&lt;&gt;(); private CASSingleton() &#123; &#125; public static CASSingleton getInstance() &#123; //自旋，直到唯一实例被创建 for (; ; ) &#123; CASSingleton singleton = instance.get(); if (null == singleton) &#123; singleton = new CASSingleton(); if (instance.compareAndSet(null, singleton)) &#123; return singleton; &#125; &#125; else &#123; return singleton; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写JDK动态代理]]></title>
    <url>%2F2018%2F12%2F31%2F%E6%89%8B%E5%86%99JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[JDK动态代理放寒假了，学生小明要回家，但是买不到火车票，只好找黄牛购票，用JDK的动态代理实现。 首先定义一个Student接口 123public interface Student &#123; void buy();&#125; 定义小明，实现Student接口 1234567public class XiaoMing implements Student &#123; @Override public void buy() &#123; System.out.println("我是小明，我要买票去上海的硬座"); &#125;&#125; 定义黄牛，实现InvocationHandler接口（JDK的动态代理一定要实现这个接口） 12345678910111213141516171819202122232425public class HuangNiu implements InvocationHandler &#123; private Student target; /** * 生成代理对象 * @param target * @return */ public Object getInstance(Student target) &#123; this.target = target; Class clazz = target.getClass(); return Proxy.newProxyInstance(clazz.getClassLoader(), clazz.getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("我是黄牛，你要什么票?"); System.out.println("--------------"); method.invoke(target, args); System.out.println("--------------"); return null; &#125;&#125; 测试类： 1234567public class ProxyTest &#123; public static void main(String[] args) &#123; Student huangniu = (Student) new HuangNiu().getInstance(new XiaoMing()); huangniu.buy(); &#125;&#125; 输出结果： 1234我是黄牛，你要什么票?--------------------我是小明，我要买票去上海的硬座-------------------- 小明要购票，小明不需要自己去售票处，而是找了黄牛，让黄牛代劳，真正购票的人是黄牛 小明 ==&gt;被代理对象 黄牛 ==&gt;代理对象 这时候，运行的是黄牛这个代理类，试着将Proxy.newProxyInstance方法生成的代理类输出 1234567891011121314151617181920212223242526/** * 输出代理对象class * * @param proxy */ private void output(Object proxy) &#123; String className = proxy.getClass().getSimpleName(); String baseDir = getClass().getResource("").getPath(); byte[] data = ProxyGenerator.generateProxyClass(className, new Class[]&#123;Student.class&#125;); FileOutputStream os = null; try &#123; os = new FileOutputStream(baseDir + "/" + className + ".class"); os.write(data); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 得到$Proxy0.class文件（生成的代理类名都是以“$”为前缀，数字为后缀），反编译后的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public final class $Proxy0 extends Proxy implements Student &#123; private static Method m1; private static Method m2; private static Method m3; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return ((Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;)).booleanValue(); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final void buy() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return ((Integer)super.h.invoke(this, m0, (Object[])null)).intValue(); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName("java.lang.Object").getMethod("equals", Class.forName("java.lang.Object")); m2 = Class.forName("java.lang.Object").getMethod("toString"); m3 = Class.forName("com.cjluo.chapter1.proxy.jdk.Student").getMethod("buy"); m0 = Class.forName("java.lang.Object").getMethod("hashCode"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 代理类对被代理对象的所有方法都做了代理，通过代理类的InvocationHandler.invoke调用，因此，自己实现动态代理的关键步骤就是： 生成动态代理类； 编译并代理类并加载代理类class文件； 返回代理类给调用者； 以上涉及的JDK动态代理相关类有 123InvocationHandlerProxyClassLoader 自己实现动态代理首先定义CustomerInvocationHandler接口 1234public interface CustomerInvocationHandler &#123; Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 理一下JDK动态代理的逻辑，方法的调用者调用的是小明的代理对象 ==&gt;黄牛，因此要定义CustomerProxy并在该类中生成黄牛这个代理类： 123456789101112131415161718192021222324252627282930313233public class CustomerProxy &#123; //硬编码代理对象名称 private static final String proxyClassName = "$Proxy0"; public static Object newProxyInstance(CustomerClassLoader loader, Class&lt;?&gt;[] interfaces, CustomerInvocationHandler h) &#123; try &#123; //1、生成代理类java文件 File f = generatorProxy(interfaces); //2、编译代理类java为class compilerJava(f); //3、自定义加载器加载class Class clazz = loader.findClass(proxyClassName); //4、生成代理对象并返回 Constructor c = clazz.getConstructor(CustomerInvocationHandler.class); return c.newInstance(h); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 生成代理类java文件代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 生成代理类java文件 * * @param interfaces * @return */private static File generatorProxy(Class&lt;?&gt;[] interfaces) &#123; //接口名 Class clazz = interfaces[0]; String interfaceName = clazz.getName(); //换行 String newLine = "\r\n"; StringBuffer sb = new StringBuffer(); sb.append("package " + clazz.getPackage().getName() + ";").append(newLine); sb.append("import java.lang.reflect.Method;").append(newLine); sb.append("public class " + proxyClassName + " implements " + interfaceName + "&#123;").append(newLine); sb.append("protected CustomerInvocationHandler h;").append(newLine); sb.append("public " + proxyClassName + "(CustomerInvocationHandler h)&#123;").append(newLine); sb.append(" this.h = h; ").append(newLine); sb.append("&#125;").append(newLine); for (Method m : clazz.getMethods()) &#123; sb.append("public " + m.getReturnType().getName() + " " + m.getName() + "()&#123;").append(newLine); sb.append("try&#123;").append(newLine); sb.append("Method m = " + interfaceName + ".class.getMethod(\"" + m.getName() + "\",new Class[]&#123;&#125;);").append(newLine); sb.append("this.h.invoke(this,m,null);").append(newLine); sb.append("&#125;catch(Throwable e)&#123;").append(newLine); sb.append("&#125;").append(newLine); sb.append("&#125;").append(newLine); &#125; sb.append("&#125;").append(newLine); String src = sb.toString(); File file = new File(CustomerProxy.class.getResource("").getPath() + "/" + proxyClassName + ".java"); try &#123; FileWriter fw = new FileWriter(file); fw.write(src); fw.flush(); fw.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return file;&#125; 编译代理类 123456789101112131415161718192021/** * 编译代理类 * * @param f */private static void compilerJava(File f) &#123; //java编译器 JavaCompiler compiler = ToolProvider.getSystemJavaCompiler(); StandardJavaFileManager manager = compiler.getStandardFileManager(null, null, null); Iterable iterable = manager.getJavaFileObjects(f); JavaCompiler.CompilationTask task = compiler.getTask(null, manager, null, null, null, iterable); task.call(); try &#123; manager.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; //删除Java文件 f.delete(); &#125;&#125; 自定义加载器加载class 123456789101112131415161718192021222324252627282930313233343536373839public class CustomerClassLoader extends ClassLoader &#123; private File baseDir; public CustomerClassLoader() &#123; String basePath = CustomerClassLoader.class.getResource("").getPath(); this.baseDir = new File(basePath); &#125; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; String className = getClass().getPackage().getName() + "." + name; if (baseDir != null) &#123; File classFile = new File(baseDir, name.replaceAll("\\.", "/") + ".class"); if (classFile.exists()) &#123; FileInputStream in = null; ByteArrayOutputStream out = null; try &#123; in = new FileInputStream(classFile); out = new ByteArrayOutputStream(); byte[] buff = new byte[1024]; int len; while ((len = in.read(buff)) != -1) &#123; out.write(buff, 0, len); &#125; return defineClass(className, out.toByteArray(), 0, out.size()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //删除class文件 classFile.delete(); &#125; &#125; &#125; return null; &#125;&#125; 测试类： 1234567public class CustomerProxyTest &#123; public static void main(String[] args) &#123; Student huangniu = (Student) new HuangNiu().getInstance(new XiaoMing()); huangniu.buy(); &#125;&#125; 运行结果 1234我是黄牛，你要什么票?--------------------我是小明，我要买票去上海的硬座--------------------]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式 - 代理模式]]></title>
    <url>%2F2018%2F12%2F30%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[代理模式是JAVA常用的设计模式之一，代理模式不直调用代理对象，而是通过对象的代理类来处理，好比是中介、黄牛、经纪人，代理模式又分为静态代理和动态代理。 代理对象：面向调用者，在调用者和被代理对象之间作为隔离层加以控制，可以增强被代理对象的方法。 被代理对象：真正的执行者，需要对代理对象暴露 静态代理何为静态？其实就是在编码阶段，写好代理类，然后编译运行，在程序运行前，代理类已经存在。 以学生小明找黄牛购票为例： 123public interface Student &#123; void buy();&#125; 小明： 123456public class XiaoMing implements Student &#123; @Override public void buy() &#123; System.out.println("我是学生小明，我要一张去上海的票"); &#125;&#125; 黄牛： 123456789101112131415161718public class HuangNiu implements Student &#123; private Student target; //代理类实例化时需要传入被代理类实例的引用 public HuangNiu(Student student) &#123; this.target = student; &#125; @Override public void buy() &#123; System.out.println("我是黄牛，你要什么票？"); System.out.println("------------"); //调用被代理类的方法 this.target.buy(); System.out.println("------------"); &#125;&#125; 测试类： 12345678public class StaticProxyTest &#123; public static void main(String[] args) &#123; Student xiaoMing = new XiaoMing(); HuangNiu huangNiu = new HuangNiu(xiaoMing); huangNiu.buy(); &#125;&#125; 运行结果： 1234我是黄牛，你要什么票？------------我是学生小明，我要一张去上海的票------------ 动态代理相对于静态代理，动态代理的代理类是在运行时生成，动态代理在实现上又有两种，分别是jdk动态代理和cglib动态代理。 JDK动态代理JDK动态代理要求被代理对象必须实现接口，其原理是代理对象实现该接口的方法，同时调用被代理对象的方法。将生成后的代理对象，强制转换为接口被调用者调用。还是以小明和黄牛为例： 小明，实现学生接口的buy方法 1234567public class XiaoMing implements Student &#123; @Override public void buy() &#123; System.out.println("我是学生，我要买票去上海的硬座"); &#125;&#125; 黄牛，作为小明的代理实现InvocationHandler 12345678910111213141516171819202122232425public class HuangNiu implements InvocationHandler &#123; private Student target; /** * 生成代理对象 * * @param target * @return */ public Object getInstance(Student target) &#123; this.target = target; Class clazz = target.getClass(); return Proxy.newProxyInstance(clazz.getClassLoader(), clazz.getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("我是黄牛，你要什么票?"); System.out.println("--------------"); //代理对象持有被代理对象实例的引用，通过反射调用目标方法 method.invoke(target, args); System.out.println("--------------"); return null; &#125; 测试类： 1234567891011public class ProxyTest &#123; public static void main(String[] args) &#123; /** * 实例化代理类时调用getInstance方法传入了小明的实例，生成代理类==&gt;黄牛，强制转换为Student * 接口，因此实际上是由黄牛去执行买票操作； */ Student huangniu = (Student) new HuangNiu().getInstance(new XiaoMing()); huangniu.buy(); &#125;&#125; 运行结果： 1234我是黄牛，你要什么票?--------------我是学生，我要买票去上海的硬座-------------- HuangNiu类没有直接实现Person接口，而是通过反射生成字节码文件的方式，动态生成真正的代理类。 Cglib动态代理与JDK动态代理不同的是，cglib代理不强制被代理类实现接口，它是通过生成代理类的子类，并重写代理类方法来实现的，所以代理类不能用final修饰，还是以小明和黄牛为例： 小明： 123456public class XiaoMing &#123; public void buy() &#123; System.out.println("我是学生，我要买票去上海的硬座"); &#125;&#125; 黄牛： 12345678910111213141516171819202122232425//MethodInterceptor是cglibpublic class HuangNiu implements MethodInterceptor &#123; public Object getInstance(Class clazz)&#123; //cglib封装的高性能的代码生成库 Enhancer enhancer = new Enhancer(); //设置父类 enhancer.setSuperclass(clazz); //回调当前实例 enhancer.setCallback(this); return enhancer.create(); &#125; @Override public Object intercept(Object target, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("我是黄牛，你要什么票?"); System.out.println("--------------"); //注意，这里如果用invoke会造成死循环，因为代理类是被代理类的子类，如果调用invokeSuper方法， //会调用父类的方法，而invoke会重复进入当前代理类的方法，造成死循环 methodProxy.invokeSuper(target,objects); System.out.println("--------------"); return null; &#125;&#125; cglib动态代理并没有像JDK动态代理那样，直接持有代理类实例的引用，那么intercept方法的第一个参数Object是怎么生成？根据类加载的顺序，实例化子类前，必先实例化其父类 实例化 ChildClass ==&gt; new FatherClass ==&gt; new ChildClass； 测试类： 1234567public class CglibProxyTest &#123; public static void main(String[] args) &#123; XiaoMing p = (XiaoMing) new HuangNiu().getInstance(XiaoMing.class); p.buy(); &#125;&#125; 运行结果： 1234我是黄牛，你要什么票?--------------我是学生，我要买票去上海的硬座-------------- 应用场景代理模式随处可见，最常见的是spring的AOP，比如spring的事务代理，在对数据进行操作时会涉及事务开启、事务提交、事务回滚等操作，简化的数据操作模型如下图： 业务场景中存在大量的事务操作（红色），而这些往往不是开发者的关注点，开发者更注重于业务逻辑（蓝色）。因此代理模式可以增强业务逻辑代码，在事务执行前开启事务，在事务执行后进行回滚/提交/关闭。 总结 静态代理需要编写代理类，动态代理不需要； 静态代理在无需修改被代理类的前提下，对代理类进行增强和扩展，但是静态代理只能对一个被代理类服务，如果被代理类过多，就会产生相应数量的代理类实现与被代理类一致的接口，产生冗余，不易维护； JDK动态代理只要求代理类实现InvocationHandler接口，被代理类实现业务接口即可。 静态代理编译时生成的class的性能高于JDK动态代理通过反射生成class； cglib不要求被代理类实现接口，而是通过继承的方式实现，因此类和方法都不能用final修饰。cglib底层用高性能的字节码生成器，性能高于反射。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>静态代理</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式-隐藏细节的工厂模式]]></title>
    <url>%2F2018%2F12%2F28%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%9A%90%E8%97%8F%E7%BB%86%E8%8A%82%E7%9A%84%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[如果你要一台宝马，带上钱，去宝马4S店，就可以买到宝马； 如果你要一台奔驰，带上钱，去奔驰4S点，就可以买到奔驰； 如果你要一台奥迪…… 你需要什么，就给你什么，而不需要关心它的细节，这就是工厂模式，工厂模式负责生产调用者需要的“产品”（实例）。通常所说的工厂模式，有这三种： 简单工厂（simple factory） 工厂方法（factory method） 抽象工厂（abstract factory） 逐一分析，先定义Car接口 1234public interface Car &#123; String getName();&#125; 再分别定义宝马、奔驰、奥迪三个“产品” 1234567//宝马public class Bmw implements Car &#123; @Override public String getName() &#123; return "Bmw"; &#125;&#125; 1234567//奥迪public class Audi implements Car &#123; @Override public String getName() &#123; return "Audi"; &#125;&#125; 1234567//奔驰public class Benz implements Car &#123; @Override public String getName() &#123; return "Benz"; &#125;&#125; 简单工厂定义SimpleFactory： 123456789101112131415public class SimpleFactory &#123; public static Car getCar(String name) &#123; if ("bmw".equalsIgnoreCase(name)) &#123; return new Bmw(); &#125; else if ("benz".equalsIgnoreCase(name)) &#123; return new Benz(); &#125; else if ("audi".equalsIgnoreCase(name)) &#123; return new Audi(); &#125;else&#123; System.out.println("没有你要的车！"); return null; &#125; &#125;&#125; 测试类： 12345678910111213public class SimpleFactoryTest &#123; public static void main(String[] args) &#123; Car car = SimpleFactory.getCar("benz"); System.out.println(car.getName()); car = SimpleFactory.getCar("bmw"); System.out.println(car.getName()); car = SimpleFactory.getCar("audi"); System.out.println(car.getName()); &#125;&#125; 测试结果： 123BenzBmwAudi 简单工厂简单粗暴，就像一个强大的汽车制造商，生产BBA甚至更多品牌的汽车，客户想要什么车，工厂就生产什么车，如果生产不了，返回null。 我们思考一下，现实中有没有这种工厂，既可以生产奔驰，又可以生产宝马和奥迪？不存在的。一般是奥迪工厂生产奥迪、奔驰工厂生产奔驰，宝马工厂生产宝马。。。这时候就要对简单工厂进行改造，产生了工厂方法模式。 工厂方法首先要有一个汽车工厂接口，这个接口定义了一套汽车生产规范和流程，所有的汽车制造商都要按照这个规范和流程生产汽车。 1234//汽车工厂public interface CarFactory &#123; Car getCar();&#125; 然后各制造商按照这个规范生产汽车，具体怎么造，各厂商自己去实现细节。 宝马工厂 1234567//宝马工厂public class BmwFactory implements CarFactory &#123; @Override public Car getCar() &#123; return new Bmw(); &#125;&#125; 奥迪工厂 1234567//奥迪工厂public class AudiFactory implements CarFactory &#123; @Override public Car getCar() &#123; return new Audi(); &#125;&#125; 奔驰工厂 1234567//奔驰工厂public class BenzFactory implements CarFactory &#123; @Override public Car getCar() &#123; return new Benz(); &#125;&#125; 客户需要哪个品牌的汽车，就去找哪个工厂生产，测试类： 12345678910111213public class FunFactoryTest &#123; public static void main(String[] args) &#123; Car car = new BmwFactory().getCar(); System.out.println(car.getName()); car = new BenzFactory().getCar(); System.out.println(car.getName()); car = new AudiFactory().getCar(); System.out.println(car.getName()); &#125;&#125; （运行结果同简单工厂模式，就不列举了） 方法工厂不同于简单工厂之处在于，它做了细分，把大而全的工厂拆解成多个工厂生产不同的“产品”，职责清晰，目标明确，但是方法工厂缺点在于，客户要自己找到这个工厂才能得到想要的车，实际上这是不合理的，因此结合简单工厂和方法工厂，又衍生出了抽象工厂。 抽象工厂试想一下，客户要某个品牌的车，还得自己去找对应品牌的工厂，这种需要客户介入的行为是很不友好的，假如有一个经销商，它负责对客户收单，然后让对应的工厂去生产客户想要的车，这就完美解决了以上问题。 抽象工厂类： 1234567891011121314151617181920//结合了简单工厂和方法工厂的抽象工厂类public abstract class AbstractCarFactory &#123; //定制一个造车流程和规范，让各造车厂商去实现 protected abstract Car getCar(); //从客户获取订单，从对应的工厂获取客户想要的车 public Car getCar(String name) &#123; if ("bmw".equalsIgnoreCase(name)) &#123; return new BmwFactory().getCar(); &#125; else if ("benz".equalsIgnoreCase(name)) &#123; return new BenzFactory().getCar(); &#125; else if ("audi".equalsIgnoreCase(name)) &#123; return new AudiFactory().getCar(); &#125; else &#123; System.out.println("没有你要的车"); return null; &#125; &#125;&#125; 各造车工厂 1234567//奥迪工厂public class AudiFactory extends AbstractCarFactory &#123; @Override protected Car getCar() &#123; return new Audi(); &#125;&#125; 1234567//奔驰工厂public class BenzFactory extends AbstractCarFactory &#123; @Override protected Car getCar() &#123; return new Benz(); &#125;&#125; 1234567//宝马工厂public class BmwFactory extends AbstractCarFactory &#123; @Override protected Car getCar() &#123; return new Bmw(); &#125;&#125; 因为抽象类无法实例化，通常都有一个默认实现，定义一个默认的工厂 12345678910//默认工厂public class DefaultCarFactory extends AbstractCarFactory &#123; //指定默认的工厂为奥迪工厂 private AudiFactory factory = new AudiFactory(); @Override protected Car getCar() &#123; return factory.getCar(); &#125;&#125; 测试类： 12345678910public class AbstractCarFactoryTest &#123; public static void main(String[] args) &#123; AbstractCarFactory factory = new DefaultCarFactory(); Car car = factory.getCar("bmw"); if(null != car)&#123; System.out.println(car.getName()); &#125; &#125;&#125; 应用场景spring中bean的创建就使用了工厂模式，各种bean工厂都实现了BeanFactory接口，调用者只需要输入beanName或者id，工厂就可以生成所需要的Bean。 网络传输协议的选择，调用者只需要描述需要什么协议，比如HTTP/TCP/POP3，协议工厂便可以生产对应的实现。 总结 工厂模式将类的实例化过程完全隐藏，调用者只需要传入类名，就可以获得所需的实例，如果在代码中存在大量的new实例化对象，万一改了类的构造方法，对代码的破坏简直是灾难，必须将所有相关的代码一一修改，如果你用工厂模式，就可以避免这种事发生。 简单工厂对类实例化的简单封装，它的缺点是不利于拓展，工厂并非真的大而全，如果要增加一个工厂，哪怕只是简单的修改if/else代码块，也是违背面向对象设计的开闭原则; 工厂方法对简单工厂进行了抽象，每个产品都由单独的工厂来创建，要新增产品，只要新增这个产品的工厂，完全遵循对修改关闭，对扩展开放的原则；但是工厂方法需要调用者明确哪个工厂生产什么并且显式的调用，并没有完全解耦； 抽象工厂又是对工厂方法的进一步封装，同类产品由同一工厂生产，它的缺点是抽象工厂如果存在多个产品，子类必须实现所有的抽象方法，相当于开了整套的流水线。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-持久化]]></title>
    <url>%2F2018%2F08%2F17%2FRedis-%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[有时候服务器会遇到一些状况，不得不重启机器，Redis的数据都是基于内存，重启后内存中的数据就会丢失，这时候有可能出现几种场景： Redis作为数据库时，数据无法返回； Redis作为缓存时，缓存失效导致雪崩，所有的请求直接打到DB上造成服务无法响应 …… 此时我们就希望Redis重启后能够保存重启前的数据，避免以上的情况发生，从内存中将数据同步到硬盘上，重启后可以从硬盘恢复到内存中，就是持久化过程。Redis支持两种持久化方式：RDB（快照）和AOF。 RDB（快照）所谓快照，就是当符合一定条件时，Redis会将内存中的所有数据生成一份副本并存储在硬盘上，以下几种情况会触发快照： 根据配置规则自动快照； 用户执行SAVE或BGSAVE命令； 执行FLUSHALL命令； 执行复制 根据配置规则自动快照Redis提供了配置文件，允许用户自己配置快照条件，满足条件时，触发快照，条件为时间T和影响Key的个数N，即当时间T内被写入（更新）的Key个数大于N时，满足条件触发快照，例如： 12save 600 1save 300 10 每个条件占用一行，以save开头，多个条件之间是“或”的关系，以上的例子表示，600秒内至少有1个key被写入（更新）或300秒内至少有10个key被写入（更新）时，进行快照。 用户执行SAVE或BGSAVE命令除了满足条件自动触发快照外，当服务需要重启前或者迁移数据时，需要手工执行快照操作，Redis提供两个命令来执行： （1）SAVE命令 当执行SAVE命令时，Redis同步执行快照，此时会阻塞所有客户端请求（类似JVM的stop the world），如果数据较多，可能导致Redis长时间不响应，所以在生产环境下尽量避免使用这个命令，以免造成服务不可用。 （2）BGSAVE命令 与SAVE命令不同的是，BGSAVE命令执行时，是以异步的方式进行快照，此时还会接收客户端的请求并作出响应，命令开始执行时会立即返回OK表示开始执行，如果要知道快照是否完成，可以通过LASTSAVE命令来获取最后一次成功快照的时间，返回的时间是Unix时间戳，例如： 12redis-&gt;LASTSAVE(integer)1423537869 执行FLUSHALL命令执行FLUSHALL命令时，Redis会清除所有数据，此时只要定义了自动快照的条件，无论是否满足这个条件，都会执行快照。 执行复制时在集群环境下，当Redis将数据从master复制到slave时，即使没有定义自动快照条件，且没有手工执行快照，也会执行快照生成快照文件。 快照过程快照的文件目录和文件名可以通过dir和dbfilename两个参数来配置，默认会保存在当前进程的工作目录中的dump.rdb文件。 12dbfilename dump.rdbdir ./ 下面是快照的过程： 执行fork函数复制一份进程（父进程）的副本（子进城）； 父进程继续接收并处理客户端请求，子进程开始将内存中的数据写入硬盘中的临时文件； 子进程将写入完成的临时文件替换旧的RDB文件，快照完成。 以上过程可以发现Redis在执行快照过程中不会修改RDB文件，只会写入临时文件中，只有快照结束后才会用临时文件覆盖RDB文件，这样可以保证RDB文件在任意时刻的完整性，以便于通过定时备份RDB文件来实现Redis的备份。RDB文件是经过压缩，以二进制格式存储，所占空间会小于内存中的数据大小，更利于传输和存储。 Redis启动后会先读取RDB文件，将数据从硬盘载入内存中，根据数据量大小和服务器性能的差异，这个时间也会有所不同。通过快照方式实现持久化，一旦Redis异常退出（例如断电、强杀进程等），就会丢失最后一次快照之后更改的所有数据。此时就要根据实际的应用场景，通过设置自动快照条件参数的方式将可能造成的数据丢失控制在能接收的范围内，比如Redis作为缓存，丢失最近几秒的数据或者几十个Key的数据在可接受范围内。但是如果Redis作为数据库，丢失的数据相对重要，这种情况一旦发生，损失要降到最小，则可以使用下面介绍的AOF方式进行持久化。 AOF当使用Redis存储重要的数据时，可以打开AOF持久化配置，使Redis异常退出造成的数据损失降到最低，AOF的全称是“Append-Only File”，从字面上看就是追加到文件中，也就是Redis每执行一条写命令都会追加到硬盘文件中（这个过程会影响到Redis的性能，但大多数情况下可以忽略，AOF的性能也取决于硬盘的响应速度）。Redis默认不开启AOF，可以通过配置appendonly参数来启用 1appendonly yes 开启AOF持久化后每执行一条会影响Redis数据的命令时，会将该命令追加写入AOF文件，默认情况下，AOF文件的保存位置和快照文件相同，都由dir参数配置，默认文件名为appendonly.aof，可以通过appendfilename参数配置 12appendfilename "appendonly.aof"dir ./ AOF的实现AOF是以文本字符串的方式追加命令，如执行以下命令： 123set key 0set key 1get keydel key 因为Redis只会将影响数据的命令写入，因此第3条get命令会被忽略，此时appendonly.aof的文件内容如下： 123456789101112131415161718192021*2$6SELECT$1*3$3set$3key$1*3$3set$3key$1*2$3del$3key AOF文件的内容保存的是客户端与Redis通信的原始协议内容，这里还有个问题，第二条set命令覆盖了第一条set命令设置的key值，第四条del命令删除了key值，那么这四条命令相对来说是冗余的，即使aof文件不保存也不影响最终的结果，随着执行命令的增多，AOF文件的大小也会不断增大，即使内存中实际的数据可能也没有多少。我们希望减小AOF文件的大小，上面已经看到，文件中可能出现冗余的命令，此时只要设置条件，让Redis把冗余的命令删除，重写AOF文件。这个条件可以在redis.conf中配置 12auto-aof-rewrite-percentage 100 //当前AOF文件大小超过上次重写时文件大小的百分比auto-aof-rewrite-min-size 64mb //允许重写的最小AOF文件大小，AOF文件小于这个值，不重写 除了以上配置外，也可以通过BGREWRITEAOF命令手工执行AOF的重写，以上AOF文件重写后，内容是空的，因为是插入key -&gt; 删除key，对于AOF的重写过程而言，只与内存中的数据有关，因此以上命令相当于什么也没有发生。在Redis启动时，会逐个执行AOF中的命令将硬盘中的数据载入到内存中，相对于快照的方式，速度稍慢。 虽然每次执行更改数据的操作时，AOF理论上都会将命令记录在AOF文件中，但事实上由于操作系统缓存的存在，数据并没有立即写入硬盘，而是进入了系统的硬盘缓存，每隔一段时间缓存刷新时将数据写入硬盘。如果在这间隔时间内系统异常退出也会导致硬盘缓存中的数据丢失。既然用AOF就是为了数据损失降低到最小，自然无法容忍这样的情况，因此要在AOF文件写入时主动刷新系统缓存把数据写入硬盘。可以通过appendfsync参数设置同步时机： 123# appendfsync always //每次写入AOF都执行刷新appendfsync everysec //每秒执行一次刷新# appendfsync no //由操作系统决定何时刷新 默认情况下是everysec（每秒），既保证性能，又兼顾了数据安全，Redis允许同时开启AOF和快照两种持久化策略。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>持久化</tag>
        <tag>rdb</tag>
        <tag>aof</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-性能优化]]></title>
    <url>%2F2018%2F08%2F13%2FRedis-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[实际开发的场景中，尤其是涉及到高并发的应用，关系型数据库常常无法满足业务需求，这时候从问题出发，尝试用redis这个基于内存的数据库，通常会因为它带来的几十倍甚至上百倍的性能使得问题迎刃而解。虽然满足了需求，但是redis性能还有没有提升的空间？在有限的资源内是不是已经把它的性能“榨干”？在这些问题之前，首先要明白redis在所使用的机器上到底能跑多快。 性能redis的性能测试是通过benchmark命令执行的，格式为：redis-benchmark [option &lt;value&gt;]，例如：redis-benchmark -c 1 -q，常用命令参数如下表 option选项 描述 默认值 -h &lt;hostname&gt; 服务端地址 127.0.01 -p &lt;port&gt; 服务端口 6379 -s &lt;socket&gt; 服务端socket -a &lt;password&gt; 密码 -c &lt;clients&gt; 并发连接 50 -n &lt;requests&gt; 请求数 100000 -d &lt;size&gt; 以字节形式指定set/get值的大小 2 --dbnum &lt;db&gt; 选择数据库 0 -k &lt;boolean&gt; 1=keep alive 0=reconnect 1 -r &lt;keyspacelen&gt; set/get/INCR使用随机 key, SADD 使用随机值 -p &lt;numreq&gt; 使用管道方式发送请求 -q 退出redis，仅显示query/sec值 --csv 以csv格式输出 -1 循环，不中断测试 -t &lt;tests&gt; 仅运行\&lt;tests>中以逗号分隔的命令 -I Idle 模式。仅打开 N 个 idle 连接并等待 单机，CPU:i5-2430M@2.40GHz，内存8GB，cygwin虚拟环境，单客户端，请求数10000，测试结果如下： 因为笔记本配置不高，又是在虚拟环境下，所以非管道方式下的测试数据显得性能一般，改为管道模式再测试一次 以上两组结果展示了一些常用redis命令在1秒内可以执行的次数，如果redis-benchmark不带任何参数，将默认使用50个客户端来进行性能测试。从以上运行结果可见在管道模式下，redis性能是非管道模式下的十几倍甚至几十倍以上，但是这些结果并不代表实际性能，因为这个测试程序只负责测试命令本身的执行，并不处理返回的结果。当发现客户端的实际性能和测试结果差距较大时，可能的原因有： 没有使用管道模式 redis的每个/每组命令都创建了新的连接 第一个问题好解决，使用pipeline即可。第二个问题要如何解决呢？首先了解一下redis客户端与服务端的交互方式。客户端向redis服务端发送命令前，首先要建立连接，redis的服务端和客户端一般都不在一台物理机上，因此需要通过底层的网络通讯建立连接，假设一次交互从请求到响应结束共耗时10ms，redis的性能很高，只用了1ms将数据处理完毕，网络传输和建立连接耗时9ms，在高并发场景下，大量的时间都消耗在网络传输和建立连接上，之前提到的管道模型可以解决频繁网络传输的问题，那么如何解决频繁连接的问题？redis的连接池就是为了这个场景而设计的。redis的连接池在客户端建立多个连接而不释放，当需要连接服务端的时候，从池中取出连接，处理完毕后将连接归还池中重用，就避免了反复建立连接的时间消耗，也保证在多线程环境下的安全。常用开发语言的客户端都有连接池的实现。 内存优化计算机发展到今天，内存依然是很宝贵的资源，价格也居高不下（屯内存发家致富。。。），redis是基于内存的数据库，所有的数据都存储在内存中，所以如何优化内存，减少空间占用是一个非常重要的话题。 精简、规范的key名和value这是最直接的减少内存占用的方式，如very.important.person:20可以改为vip:20，但是不建议写成v:20，精简的原则是保证可读性和易维护性以及尽量避免冲突。比如存储性别的key的值是male和female，可以优化为0和1来表示。 内部编码优化如果靠精简key和value不足以满足减少空间的需求，这时候就要根据redis的内部编码方式来节省更多的空间。redis为每种数据类型都提供了两种编码方式（字符串类型的embstr编码是3.0版加入的，列表类型的quicklist编码是3.2版加入的，在此暂不介绍了）,以散列为例，散列是通过hashTable实现的，查找、赋值的时间复杂度是O(1)，如果散列中的元素很少，O(1)的性能和O(n)比没有绝对优势，这种情况下redis会采用一种更紧凑，更节约空间的编码方式zipList实现。当散列的数据增多，编码方式会自动转为hashTable（这个转换过程是透明的，由redis内部自动完成），查看一个key的编码方式，可以使用object encoding命令 redis的底层存储模型都是一个redisObject，它的结构如下： 12345678typedef struct redisObject &#123; unsigned type:4; unsigned notused:2; unsigned encoding:5; unsigned lru:22; int refcount; void *ptr; &#125; robj; 其中type字段表示的是key的数据类型，取值如下： 12345#define REDIS_STRING 0#define REDIS_LIST 1#define REDIS_SET 2#define REDIS_ZSET 3#define REDIS_HASH 4 encoding字段表示的就是编码方式，取值如下： 123456789#define REDIS_ENCODING_RAW 0#define REDIS_ENCODING_INT 1#define REDIS_ENCODING_HT 2#define REDIS_ENCODING_ZIPMAP 3#define REDIS_ENCODING_LINKEDLIST 4#define REDIS_ENCODING_ZIPLIST 5#define REDIS_ENCODING_INTSET 6#define REDIS_ENCODING_SKIPLIST 7#define REDIS_ENCODING_EMBSTR 8 各数据类型的编码方式、满足条件和执行object encoding命令后的结果 字符串redis使用sdshdr类型变量来存储字符串，上文提到的redisObject的ptr属性指向该变量的地址。sdshdr的结构如下： 12345struct sdshdr &#123; int len; //字符串长度 int free; //buf可用空间 char buf[]; //字符串内容&#125; 当value时字符串且长度大于39时，例如：set key &quot;hello world,hello world,hello world,hello world&quot;，redis将以raw编码方式存储，如图 当value是非整型字符串时且长度小于39时，例如：set key redis，redis将以embstr编码方式存储，如图 与raw的差异是，embstr编码方式的sdshdr是与redisObject在连续的内存空间中。 当value可以用64位的有符号整数表示时，redis会转换成long类型并以int编码方式存储。例如：set key 123 redisObject结构中的refcount字段存储的是value被引用的数量，在这种结构中，value可以被多个key引用。redis会预先存储0到9999这些数字的redisObject对象，如果set的key在这10000个数字内，可以直接引用已经创建的对象，而不是重新建一个（有点像Java的常量池），这种共享方式能够节约一定的存储空间。 散列散列的编码方式有两种：hashtable和ziplist，在配置文件中可以定义使用ziplist的条件： 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 当散列的字段个数小于hash-max-ziplist-entries且每个字段和字段值的长度都小于hash-max-ziplist-value时，使用ziplist编码存储，否则用hashtable。ziplist编码是一种紧凑型的编码格式，它以时间换空间的方式提高存储空间的利用率，适合在散列字段较少时使用。ziplist的内存结构如下： zlbytes是uint32_t类型，表示占用空间，长度为4字节，zltail也是uint32_t类型，长度为4字节，表示到最后一个元素的偏移量，记录zltail可以使程序直接定位到尾部元素而无需遍历整个结构。zllen是unit16_t类型，存储的是元素的数量，长度是2字节，zlend用来标记结构的末尾，值是255，长度1字节。 ziplist的每个元素由4个部分组成，第一部分用来存储前一个元素的大小以实现倒序查找，当前一个元素的大小&lt;254字节时，第一个部分占用1字节，否则占用5字节。第二、三部分分别表示元素的编码类型和元素大小，当元素大小&lt;=63字节，元素编码类型是ZIP_STR_06B（0&lt;&lt;6），同时第三个部分用6个二进制位来记录元素的长度，所以第二、三部门总占用1字节。当63字节&lt;元素&lt;=16383字节时，元素编码类型是ZIP_STR_14B，总占用2字节，当元素&gt;16383时，元素编码为ZIP_STR_32B占用5字节。第四部分就是元素的存储内容，reids在多种编码方式中，都会将可以转为数字类型的字符以数字来存储，此时第二、三个部分来表示数字的类型（此时的编码方式为ZIP_INT_16B、ZIP_INT_32B等）。当执行命令hset hello world时，内存结构如图： 从结构上来看，ziplist比hashtable节省了前驱指针和后驱指针的空间，数据是连续的，具有清晰的边界，在遍历ziplist时，每次查找会跳过一个元素确保只查找字段名，找到后取下一个元素就是字段值，因此在元素个数不多时，这个性能还是可以接受的。 列表列表的编码方式也有两种，分别为linkedlist和ziplist，linkedlist是一种非常常见的数据结构，每个Node有一个prev指针和next指针组成双端链表。ziplist上面已经介绍过了，就不再说明了。配置文件中可以定义使用ziplist的条件： 12list-max-ziplist-entries 512list-max-ziplist-value 64 当列表的元素个数少于512且所有元素都小于64字节时，使用ziplist编码存储。redis3.2版新增了quicklist编码方式，这个编码方式可以看作是linkedlist和ziplist的结合，其原理是将长linkedlist分成若干个以链表形式组成的ziplist，即发挥了ziplist编码减少空间的优势，又具有linkedlist编码的性能。 集合集合的编码方式也是两种，分别为hashtable和intset，配置文件中可以定义使用intset的条件： 1set-max-intset-entries 512 除了元素个数小于配置的值外，还必须满足所有元素都是整数。intset编码结构的定义为： 12345typedef struct intset&#123; uint32_t encoding; //编码方式 uint32_t length; //元素个数 int8_t contents[]; //内容数组&#125; intset; ​ intset的数据结构由三部分构成，encoding编码方式，length元素个数，contents元素内容数组。虽然contents声明为int8_t，但是实际类型还是取决于encoding的值，encoding的取值有以下三种： 123INTSET_ENC_INT16INTSET_ENC_INT32INTSET_ENC_INT64 ​ 默认的encoding是INTSET_ENC_INT16（2字节），当新插入的元素大于2字节时，集合的encoding会升级为INTSET_ENC_INT32（4字节）并调整元素的位置和长度，如果还无法满足，将升级为INTSET_ENC_INT64（8字节）。intset编码方式是有序的存储元素，因此无论是插入还是删除元素，都要调整元素后面的位置，当元素太多时，性能很差。当新增的元素不是整数或个数超过了配置的参数值，redis将集合的编码方式自动转换为hashtable（这种转换时不可逆的，即使删除了非整数元素或将个数减少到范围内，也无法改变编码方式，假如这么做了，就需要重新遍历集合的所有元素，时间复杂度为O(n)）。 有序集合有序集合的编码方式可能是skiplist和ziplist，同样可以在配置文件中定义使用ziplist的条件： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 使用条件与散列、列表的ziplist一样，这里主要介绍skiplist。先看看skiplist的定义： 123456789101112131415161718typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; // 跳表头尾指针 unsigned long length; // 跳表的长度 int level; // 跳表的高度&#125; zskiplist;//跳表节点typedef struct zskiplistNode &#123; redisObject *obj; // 节点数据 double score;// 分数 struct zskiplistNode *backward; // 后驱指针 // 前驱指针数组 struct zskiplistLevel &#123; struct zskiplistNode *forward; unsigned int span; // 到下一个数据的偏移量 &#125; level[];&#125; zskiplistNode; 跳表包含4个属性：header、tail、length和level，其中header和tail都是由zskiplistNode（后面简称Node），node中保存元素的redisObject。以header为例， 可以看出跳表的数据由多个层级组成，每个层级间隔保存了链表的节点数据，这样的优点是查找效率类似二分查找法，比如要查找score = 86这个节点，查找过程如下 整个过程只比较了三次（横向箭头比较，竖向箭头不比较），比普通链表从头开始遍历节省了时间，缺点也很明显，浪费了空间。跳表在有序集合中元素个数很大的情况下，查询效率可以媲美二分法，但是对内存的浪费也很明显。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>redis性能</tag>
        <tag>redis优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-特性（下）]]></title>
    <url>%2F2018%2F08%2F09%2FRedis-%E7%89%B9%E6%80%A7%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前篇基于redis的高级特性，了解如何使用redis的事务，设置数据的有效期和对列表、集合等的排序，本篇篇基于场景，了解redis的其他特性。 队列在系统中往往有许多需要处理的工作，有的工作对时效性要求高，需要立即返回结果，有的对时效性不高，容许一定时间的延迟，比如后台日志、邮件发送。这些时效性不高的功能，可以把他们作为一个任务放在队列里，再由专门的程序来处理。没错，这就是经典的生产者消费者模型。连接生产者和消费者的桥梁就是队列（queue），生产者负责将任务封装并放入队列中，消费者不断的从队列中获取任务并处理，熟悉的程序员应该很了解这个模型带来的好处：松耦合，易扩展。redis的列表类型可以很好的实现高性能的任务队列，结合lpush和rpop命令，很容易想到让生产者将任务lpush到某个列表key中，另一端消费者用rpop不断的从这个列表key取出任务，这个列表key就起到了队列的作用。 熟悉Java队列的开发者应该清楚，队列分为阻塞队列和非阻塞队列，redis的队列没有阻塞与非阻塞之分，但是通过命令可以实现阻塞队列和非阻塞队列的功能，上文提到的简单生产者和消费者模型在redis的实现，如果队列中没有任务，返回nil。下一次再获取任务旧需要通过定时任务或者人工触发，如果不想通过这种方式，redis的brpop命令可以解决这个问题。在队列没有任务的情况下，brpop与rpop的唯一区别在于前者会阻塞住连接，直到有新的入队元素加入。 每次执行brpop命令，redis从右侧弹出当前的key和任务，如果队列中没有元素，进入阻塞状态。下表比较了列表作为队列时，相关的命令。 命令 功能 参数说明 返回值 是否阻塞 lpush key [value...] 将元素从表头插入 value支持多个元素 列表长度 rpush key [value...] 将元素从表尾插入 value支持多个元素 列表长度 rpop key 从表尾弹出元素 弹出的元素值（如果是空列表，返回nil） X lpop key 从表头弹出元素 弹出的元素值（如果是空列表，返回nil） X blpop [key...] timeout 从多个列表的表头弹出元素 key可以是多个列表，timeout为超时时间（单位秒），0表示永不超时 返回两个值：第一个为列表的key，第二个为弹出的元素 O brpop [key...] timeout 从多个列表的表尾弹出元素 key可以是多个列表，timeout为超时时间（单位秒），0表示永不超时 返回两个值：第一个为列表的key，第二个为弹出的元素 O 以上的命令都需要一个插入命令和一个弹出命令组合才能将列表作为一个FIFO或LIFO队列使用。假如现在遇到了这样的一个场景，系统中有一些任务需要加急处理，而目前的队列前面有N个普通任务在排队处理，显然目前的设计无法满足场景需求。这时候brpop又可以派上用场了，brpop与rpop的区别除了阻塞客户端连接外，它还可以接收多个列表key，并且根据列表从左至右的顺序取key中的元素，如果所有的key都没有元素，进入阻塞状态。因此上述场景可以通过两个列表来解决，queue1是普通的任务队列，queue2是加急任务队列，brpop会优先从左侧开始弹出列表的任务。blpop与brpop类似，只是一个从表头弹出，一个从表尾弹出，不再赘述。 管道redis的客户端和服务端在生产、测试环境中一般是不会在同一台机器上，因此客户端和服务端的通讯都要经过网络传输，发送和返回都会产生耗时，网络性能不同产生的耗时也不同。前面提到的redis事务，在开始执行事务时，客户端将接收的命令放入列表中，只有在执行exec命令时，将列表的命令一次性发给服务端，服务端将执行结果一次性返回，除了保证原子性外，还节约了多次的网络开销。那么在非事务下，是否也有类似的方式减少网络开销？redis的底层通信协议对管道的支持解决了这个问题。首先看个例子，分别对两个key自增到10万，分别采用管道模型和非管道模型，比较两者的执行时间（以Java的jedis客户端为例）。 123456789101112131415161718192021222324@Testpublic void PipelineTest() &#123; int max = 100000; jedis = jPool.getResource(); for (int i = 0; i &lt; 10; i++) &#123; jedis.del(new String[]&#123;"key1", "key2"&#125;); //管道 long start = System.currentTimeMillis(); Pipeline pipeline = jedis.pipelined(); for (int j = 0; j &lt; max; j++) &#123; pipeline.incr("key1"); &#125; pipeline.sync(); long t1 = System.currentTimeMillis() - start; //非管道 start = System.currentTimeMillis(); for (int j = 0; j &lt; max; j++) &#123; jedis.incr("key2"); &#125; long t2 = System.currentTimeMillis() - start; System.out.printf("管道耗时:%s ms,非管道耗时:%s ms \n", String.valueOf(t1), String.valueOf(t2)); &#125; jedis.close();&#125; 以下是在redis服务器上执行10次的结果： 以上测试机器的配置为I5二代，8G内存，10次执行结果的平均值来看，非管道模型的耗时大约是管道模型耗时的50~60倍之间，这个结果应该很好理解，10万次自增命令，非管道模型下需要10万组的传输层通信，往返共20万次，而管道模型只需1组传输，2次往返，极大减少了网络开销。 发布与订阅redis的另一个特点是提供发布和订阅模式，它的特点是订阅者负责订阅频道，发送者负责向频道发送二进制字符串消息，频道的所有订阅者们都会收到消息，以下是发布和订阅的相关命令 命令 功能 参数说明 备注 publish channel message 将消息message发布到频道channel channel 频道，message 消息 返回收到message的订阅者数量 subscribe channel... 订阅一个或多个频道的消息 channel 频道，支持多个 执行订阅命令后，进入订阅状态，客户端只能使用发布/订阅相关的4个命令。返回值可能收到3种类型的回复 psubscribe pattern... 按给定的pattern规则订阅频道 pattern 规则通配符，支持多个 同subscribe命令 unsubscribe channel... 退订通过subscribe订阅的频道 channel 频道，支持多个 只能退订通过subscribe订阅的频道，不影响psubscribe订阅的频道，如果不带参数，则退订所有频道 punsubscribe channel... 退订通过psubscribe订阅的频道 channel 频道，支持多个 只能退订通过psubscribe订阅的频道，不影响subscribe订阅的频道，如果不带参数，则退订所有频道 subscribe命令可以订阅任何频道，在执行命令后，客户端进入订阅状态。 每订阅一个频道，将返回三条消息，订阅n个频道返回n组3条信息，每组的第一条消息是订阅类型subscribe或psubscribe，第二条消息是订阅的频道名称，第三条消息是当前客户端订阅的频道数量。 发布者在订阅的频道发布一条消息，返回的数字表示当前频道的订阅者个数。 这时频道的订阅者将收到发布者发布的信息，也是三条消息，第一条为message，表示类型为接收消息，第二条为发布消息的频道，第三条是接收的内容。 如果订阅者退订频道，也将收到三条消息，第一条是退订方式unsubscribe或punsubscribe，第二条是退订的频道名称，第三条是当前客户端订阅的频道数。 psubscribe命令订阅频道还能指定规则，规则支持glob的通配符，例如规则【channel.?*】可以匹配以【channel.】开头的所有频道，但是不包括【channel.】，此时向channel.开头的频道发布消息，该客户端都能收到信息。 此时收到4条消息，第一条表示通过psubscribe命令订阅频道，第二条表示频道的通配符，第三条表示实际发布消息的频道，第四条才是具体的消息内容。 注意: psubscribe命令可以重复订阅消息，如果客户端订阅了两次【channel.?*】，这时发布者向channel.1发布一条消息，发布者返回值是2而不是1，客户端也将收到两组消息。 订阅频道的客户端只能接收订阅后发布者发布的消息，发布者在订阅者订阅频道前发布的消息，客户端是无法收到的]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>队列</tag>
        <tag>管道</tag>
        <tag>发布</tag>
        <tag>订阅</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-特性（上）]]></title>
    <url>%2F2018%2F08%2F08%2FRedis-%E7%89%B9%E6%80%A7%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在有的业务系统中，redis用来做缓存，在有的系统中，redis用来做数据库，另一些系统可能作为队列使用（当然还有其他N中用法），以上这些功能都在基本的数据类型的基础上，由各种命令组合而成。 事务只要做过系统开发，难免跟数据库打交道，常用的数据库有mysql，sqlserver，oracle和posgresql，这些都是关系型数据库，如今越来越多的非关系型数据库已经崭露头角，redis就是其中之一，基于内存使得它有很高的性能。 数据库的一大特性就是支持事务（有的数据库还不支持），当然redis也是支持的。redis中的事务是一组命令的集合，在同一个事务中的命令，要么都执行，要么都不执行，事务的应用最经典的场景就是银行转账，转账过程大概如下图 ATM将A的钱从银行账户中转出 ATM将A的钱转入B的银行账户 操作是分两步进行的，如果执行了1，在执行2时失败了，假如没有事务的控制，那么A的银行账户扣款了，而B的银行账户却没有入账，钱消失了。因此步骤1和步骤2要么都执行成功，要么都执行失败。 redis的事务是声明在同一个事务下，将一组命令发送给redis，然后再让redis执行这组命令，以上转账场景用redis实现如下： 123456set accountA 1000 -- 初始化A账户余额set accountB 0 -- 初始化B账户余额multi -- 开启事务decrby accountA 1000 -- A账户转出1000incrby accountB 1000 -- B账户转入1000exec -- 执行 执行结果： multi、exec命令从multi命令开始，redis返回OK，进入事务，后面的两个命令decrby和incrby并没有立即返回结果，而是返回queued，表示这两条命令已经进入等待执行的事务队列中。在执行exec命令后，redis将等待执行的事务队列中的所有命令（状态为queued的命令）按照发送顺序依次执行，最终的返回值就是每条命令执行后返回值组成的列表。在发送exec命令之前，如果连接的客户端断开，redis会清空当前客户端的事务队列，待执行的命令都不执行，在发送exec命令之后，返回结果之前，连接的客户端断开，redis会继续执行已在事务队列的命令。 事务的另一个功能可以保证命令的连续性，在没有事务的前提下，如果A客户端发送命令时，B客户端也在发送命令，无法保证A发出的命令连续执行，中间有可能被B发出的命令插入执行，开启事务可以杜绝这种情况发生。 事务异常如果在事务执行时某个命令执行出错，redis将如何处理？首先要知道哪几种情况会导致命令执行出错 语法错误。如果输入的命令不存在或者命令的参数个数不对，比如： 第一条命令是正确的，成功加入事务队列，第二条命令少了参数，第三个命令不存在，执行exec后直接返回错误，连语法正确的命令都没有执行。 运行时错误。运行时错误指在命令执行时出现错误，比如使用散列类型的命令操作集合类型的key，这种错误在事务开启时，redis无法发现，会作为正常的命令放入事务队列中，比如： 执行exec后，第二条命令返回了错误，但是第一条和第三条命令依然执行了，redis没有关系数据库事务提供的回滚功能，因此必须由开发人员在执行出错之后处理。 这两种会导致事务执行失败的错误，其中语法错误完全可以在开发时找出并解决，另外如果能够对key定制合理的命名规范，也可以有效的避免运行时错误 watch、unwatch命令多客户端连接的情况下，有可能对同一个key执行操作，因此可能出现竞争，有些情况下需要先获得一条命令的执行结果，然后再根据这个结果执行下一条命令，在竞争的情况下，A客户端执行后，B客户端又修改了A执行后的结果，随后A客户端执行命令得到的结果可能是错误的。要解决这个问题，能不能使用事务来处理？答案是不行，因为事务的执行结果要么全都执行，要么全都不执行，结果也是一起返回的，无法将前一条命令的结果作为下一条命令的参数。换个思路，在上一条命令执行结束后，要保证这个key的值不被其他客户端或其他命令修改，直到所有命令执行完后才允许其他客户端修改，事务的watch命令就派上用场。 用watch命令监视key后，事务执行前修改了key的值，最后事务中的incr命令并没有执行，exec返回空结果。执行exec命令后，不论事务是否被成功执行，都会取消对key的监视。如果在执行事务前取消对key的监视，可以使用unwatch命令。 过期时间实际的开发中会遇到一些有时效的数据，比如缓存，过了一定时间就要删除这些数据，在关系数据库中需要一个字段记录到期时间，然后定期轮询删除过期数据，redis的做法更简洁，设置key的时候可以同时设置过期时间，到时redis会自动将key删除。expire命令的用法为expire key seconds，其中seconds表示key的过期时间，单位是秒。在秒杀的场景中，限流的一种简单思路就是限制用户的访问，用户的id作为key，限制用户30秒内只能进行一次秒杀（真实的场景比这个更复杂）。 过期后自动删除了key，返回nil，也可以用set key value ex second 或 set key value px millisecond 命令来设置key和过期时间，前者的时间单位是秒，后者是毫秒。这两个操作都是原子的。如果想知道key的有效时间还剩多久，可以使用ttl命令，返回key的剩余时间（秒） 如果key没有设置过期时间，即key是永久有效的，ttl将返回-1，如果key不存在,ttl将返回-2。 既然有设置过期时间的命令，那有没有取消过期时间的命令？肯定的，redis提供了persist命令清除过期时间，用过orm框架的应该很熟悉，persist就是持久的意思。 除persist命令外，使用set或 getset命令设置key也会清除过期时间，其他只对value的操作，不会影响key的过期时间。 expire的命令的seconds参数必须是整数，所以最小时间是1秒，如果要更精确到毫秒，要使用pexpire命令，除了单位是毫秒以外，其他功能与expire一致。另外还有两个命令也是可以设置过期时间：expireat和pexpireat，区别在于这两个命令的时间参数分别使用unix时间戳的秒和毫秒作为单位。 至此，设置过期时间的五个命令如下 expire key seconds 设置单位为秒的过期时间 pexpire key milliseconds 设置单位为毫秒的过期时间 set key value ex seconds 设置单位为秒的过期时间 set key value px milliseconds 设置单位为毫秒的过期时间 expireat key timestamp 设置unix时间戳过期时间，单位秒 pexpireat key milliseconds-timestamp 设置unix时间戳过期时间，单位毫秒 注意：如果使用watch监视具有过期时间的key，当key过期被删除后，watch命令不会认为该key被修改过。 过期时间的应用场景很多，最常用的是缓存，为了提高网站的负载能力，要将一些访问频率高且访问耗时的数据结果缓存起来，让一段时间内的访问请求从缓存获取结果，同时希望这些缓存在一定时间之后过期删除。如果大量的使用缓存且过期时间设置过长就会导致redis占用大量的内存，另一方面如果为了防止内存占用过大而将过期时间设置的太短，可能导致缓存命中率过低而消耗CPU或IO资源。因此要不断的尝试，设置一个合理的过期时间，内存的利用率提高的同时，也节约CPU和IO的资源。在redis的配置文件中可以修改maxmemory参数，限制最大可用内存，当超过了最大内存时，redis会根据maxmemory-policy参数指定的淘汰机制删除key保证占用内存小于maxmemory，maxmemory-policy支持的机制如下 规则 说明 volatile-lru 只对设置了过期时间的key使用lru算法，删除一个key（默认规则） allkeys-lru 对所有key使用lru算法，删除一个key volatile-random 随机删除一个即将过期的key allkeys-random 随机删除一个key volatile-ttl 删除过期时间最近的一个key noeviction 永不过期，返回错误 排序需要排序的情况，可以使用有序集合实现，常见的场景是大数据排序，例如销量排行榜，一般只要求获取Top10或Top100之内的少量数据，不会将所有数据进行排序。除了有序集合外，如果要对列表、集合进行排序，可以使用sort命令来实现。 对列表排序： 对集合的排序： 注意：如果用smembers命令查看集合元素，不排序的情况下也是有序的，这是因为对于整数，redis做了优化，后续再分析。 对有序集合的排序，忽略元素的score，按照value排序: 以上都是对数字的情况下进行排序，sort命令也可以排序非数字元素，不带参数直接排序会报错，增加 alpha参数后，sort命令尝试将非数字元素转换为double来比较，默认按照字典顺序升序排列： 可以通过增加参数 desc使结果按照降序排列，如果要分页显示，还可以通过limit offset count参数返回指定范围的结果（是不是有点像sql？） 在列表或集合中，key一般用来存储id，而单纯的对id的排序意义不大，更多时候我们需要根据具体的某个属性来排序，比如用户按照年龄排序，这时需要多个数据类型整合排序 排序的结果是按照年龄的大小升序排列，by后面的参数为参考键，参考键支持字符串key或散列中的某个field（key-&gt;field表示），如果提供了by参数，sort会根据by后的参数替换通配符*，获取值后根据该值进行排序。如果by后的参考键不包含通配符*，redis不执行排序操作。 上面的场景最终排序后获取的是用户id，实际上还不够，更直观的是能直接把用户的姓名或者其他信息展示出来，这时候就要借助get参数，它的规则和by参数一样，也支持字符串key或散列中的某个field，并使用*作为通配符。现在，要实现排序后返回用户姓名，可以这样写 与by参数不同的是，sort命令中只允许一个by参数，而get参数却允许多个，所以还可以这么用 可见有几个get参数，每个元素返回的结果就有几个，如果还要返回用户id，使用get即可，该命令会返回元素本身的值。 默认情况下，排序后的结果会直接返回，但是在实际的场景中，短时间内连续查询，每次都进行排序，而结果没有变化，过于消耗性能，这时候可以把结果缓存起来，store参数可以起到缓存结果的作用。将上述排序的结果缓存到result中 保存的结果为列表类型，如果缓存的key已经存在，将会覆盖，加上store参数后，命令的返回值为结果个数。 sort命令的时间复杂度为O(n+mlog(m))，其中n表示要排序的列表（集合或有序集合）中的元素个数，m表示要返回的元素个数（如果有limit offset count参数，根据count个数返回），n越大，性能越低，在排序的过程中会创建一个临时的容器来存储待排序的元素，因此使用sort命令要注意以下几点： 待排序的元素尽可能的少 使用limit参数控制返回的元素个数 如果访问频率高，排序耗时，尽可能使用store缓存排序的结果]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>事务</tag>
        <tag>排序</tag>
        <tag>监听</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-数据类型]]></title>
    <url>%2F2018%2F08%2F06%2FRedis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[redis的数据类型有五种，分别是 字符串 散列（哈希） 列表 集合 有序集合 redis也是一种基于内存的数据库，五种数据类型的共同之处是数据都以Key-Value键值对保存，对于习惯了Java的程序员来说，Map的Key和Value可以自定义，value还可以嵌套Map/List/Set，比如Map&lt;String,Object&gt; m，Object可以是任意的数据类型，但是这种在redis中是不允许的，key只能是一个类型，例如字符串的Key如果存入的value为散列数据，那么redis会抛出一个错误 说明这个key已经存入了其他数据类型的value。接下来逐个介绍各种数据类型。 字符串字符串是reids最基本的数据类型，可以存储图片、字符、Json格式化的对象，一个字符串类型允许存储的大小最大512MB（已经很大了，内存可是很宝贵的！） 常用命令（斜体表示形式参数，[]表示可选参数） table th:nth-of-type(3) { width: 85px; # 可使用%比例 } table th:last-of-type { width: 45%; } 命令 功能 复杂度 可选参数 说明 set key value [p] 保存key-value O(1) ex second px millisecondnxxx 设置过期时间为秒设置过期时间为毫秒key不存在时，才执行setkey存在时，才执行set get key 读取key的value O(1) key不存在，返回(nil) incr key 整数递增 O(1) key不存在，创建，value先初始化为0，然后+1key不是数字，抛出异常key为数字，+1且返回递增后的值 incrby key increment 按 increment递增 O(1) 与incr相似increment&gt;0，增加数值increment&lt;0，减少数值返回值为计算后的结果 decr key 整数递减 O(1) key不存在，创建，value先初始化为0，然后-1key不是整数，抛出异常key为整数，-1且返回递减后的值 decrby key increment 按 increment递减 O(1) 与decrby相似increment&gt;0，减少数值increment&lt;0，增加数值返回值为计算后的结果 incrbyfloat key increment 增加指定浮点值 O(1) increment如果是整数，效果等同于incrby增量可以用科学计数法表示，如3e5,4e-2increment&gt;0，减少数值increment&lt;0，增加数值返回值为计算后的结果 append key value 尾部追加 O(1) 如果key不存在，等同于set key value，否则追加value，返回追加后的长度 strlen key 计算value字符串长度 O(1) 如果key不存在，返回0对于中文或其他字符，按照实际utf-8的编码长度返回，如“你好”，返回的长度是6 mset key… 批量设置key-value O(N) 操作多个key时，是一个原子操作如果成功，则所有key都同时设置 mget key… 批量获取key-value O(N) 如果给定的key不存在，返回nil setbit key offset value 设置偏移量上的bit O(1) value只能是0或1当key不存在时，自动生成一个新的字符串，扩容时，非填充的位置以0填充offser的值在[0,2^32)之间 getbit key offset 获取偏移量上的bit O(1) 如果offset大于字符串的长度，或者key不存在，返回0 bitcount key [start] [end] 统计bit被设置为1的数量 O(N) startend 起始位置结束位置 bitop [op] destkey key… 对key进行位操作结果保存到destkey上 O(N) andorxornot 逻辑且逻辑或逻辑异或逻辑非 bitpos key bit [start] [end] 返回第一个被设置为1或0的bit位 O(N) startend 起始位置结束位置 应用实例以上字符串命令在项目中会经常使用，关于bit的相关操作，很有必要掌握，确实解决了很多业务和性能问题。最常遇到的应该是统计用户活跃度问题，举个例子，产品要求统计上线以来，每个用户的活跃度曲线，我们要做的就是两件事， 统计每个用户的活跃天数，具体的访问日期 根据维度、粒度绘制曲线图 这种统计方式与关系型数据库相比，性能更高，也不用写复杂的SQL，取得数据后就是绘制曲线图，这不是本文的重点，就不详述了。如果有其他的统计需求，比如用户A第一次使用产品或某个日期区间内第一次使用产品的日期，可以使用bitpos命令， 散列（哈希）散列也是以kv结构存储，与Java的Map很像，但是它的value只能是字符串，也就是无法像Map那样可以嵌套。散列的存储结构可以简单的用下图表示 常用命令 命令 功能 复杂度 说明 hset key field value 设置key中的field值为value O(1) 如果field不存在且value设置成功，返回1如果field已存在，新值将覆盖旧值，返回0 hget key field 获取key中指定域的值 O(1) hmset key field value [field value…] 设置一个或多个field-value对 O(N) 如果filed已存在，将会覆盖 hmget key field… 获取key中一个或多个域的值 O(N) key不存在，返回nilfield不存在，返回nil hgetall key 获取key所有域和值 O(N) hexists key field 判断field是否存在 O(1) 如果field存在，返回1如果field不存在，或key不存在，返回0 hsetnx key field value 当且仅当field不存在时，赋值为value O(1) 如果field存在，不执行赋值操作如果key不存在，相当于创建key并赋值field hincrby key field increment 让key中的field按increment递增 O(1) field的value不是数字，报错increment&gt;0，增加数值increment&lt;0，减少数值以上返回值为计算后的结果 hincrbyfloat key field increment 让key中的field按increment浮点数递增 O(1) 同hincrby hdel key field… 删除key的一个或多个field O(N) 返回成功删除field的个数 hkeys key 只获取key的所有field O(N) 包含所有field的表当key不存在时，返回一个空表 hvals key 只获取key的所有field的值 O(N) 包含所有value的表当key不存在时，返回一个空表 hlen key 返回field的数量 O(1) key不存在时，返回0 应用实例有了散列数据类型，当作为数据库时，一个用户的属性包含id，姓名，性别，联系电话，在关系型数据库的表结构可以表示为 Id name gender mobile 1 张三 男 135xxxxxx 2 李四 男 135xxxxxx 用redis的散列可以表示为 假如要增加一个属性驾驶证号码，这个属性只针对id=1的数据有效，对于其他数据是冗余的，那么只能对表结构进行修改，增加一个字段 Id name gender mobile cardId 1 张三 男 135xxxxxx xxxxxx 2 李四 男 135xxxxxx null redis的散列结构不需要这么做，它的存储结构都是独立的，可以自由的增减field，不影响其他field 列表列表类型可以存储一个有序的字符串列表，支持双端操作，底层的实现是双向链表，所以两端添加元素时性能很高，越接近两端的元素越快，但是随机访问的性能就比较差，这跟Java的链表很相似。 常用命令 命令 功能 复杂度 说明 lpush key value… 将一个或多个value插入key列表的表头 O(1) 返回列表的长度 rpush key value… 将一个或多个value插入key列表的表尾 O(1) 返回列表的长度 lpop key 移除并返回key列表的头元素 O(1) 返回列表的头元素当key不存在时，返回nil rpop key 移除并返回key列表的尾元素 O(1) 返回列表的尾元素当key不存在时，返回nil llen key 返回列表的长度 O(1) 如果key不存在，返回0如果key不是列表类型，报错 lrange key start end 返回列表key中起始和结束区间内的元素 O(S+N) start end以0为底，可以&gt;0也可以&lt;0为0时表示最左边第一个元素1表示最左边第二个元素-1表示最右边第一个元素-2表示最右边第二个元素，依此类推start end的区间为闭区间，包含start和end下标的元素如果start&gt;列表最大下标，返回空列表如果end&gt;列表的最大下标，则返回到最右边的元素 lrem key count value 移除列表中与value相等的元素 O(N) count&gt;0，从表头开始向表尾遍历删除与value相等的元素，数量为countcount&lt;0，从表尾开始向表头遍历删除与value相等的元素，数量为count的绝对值count=0，删除所有表中与value相等的元素 lindex key index 返回列表中下标为index的元素 O(N) index&gt;0，列表从表头开始遍历第index个元素index&lt;0，列表从表尾开始遍历第index个元素 lset key index value 将下标为index的元素值设置为value O(N) index&gt;0，列表从表头开始遍历第index个元素 index&lt;0，列表从表尾开始遍历第index个元素 linsert key Before/After pivot value 将value插入列表中插入pivot之前或之后 O(N) 在列表中不存在pivot值时，不执行操作key不存在时，不执行操作 rpoplpush src dest 将列表src的表尾元素弹出插入列表dest的表头 O(1) 弹出的元素会作为返回值返回如果src不存在，返回nil，不执行其他操作如果dest不存在，创建dest列表如果src和dest相同，等同于将表尾元素移到表头 集合集合与列表有一些相似之处，也很容易区分，集合的元素是不可重复的，与Java的HashSet类似，也是无序的。 常用命令 命令 功能 复杂度 可选参数 说明 sadd key member… 将一个或多个member元素加入到集合key中 O(N) 如果key不存在，自动创建已存在与集合的member元素将被忽略成功则返回加入的元素数量（忽略的元素不计算在内） srem key member… 将一个或多个member元素删除 O(N) 返回成功移除的元素数量 smembers key 返回集合key中的所有元素 O(N) key不存在，返回空集合 sismember key member 判断元素member是否在集合key中 O(1) 如果member元素在集合key中，返回1如果key不存在或member元素不在集合key中，返回0 sdiff key… 差集运算 O(N) 当key不存在，视为空集合返回差集集合 sidffstore dest key… 差集运算，将结果保存到dest中 O(N) dest如果已存在，将其覆盖dest可以是key自身返回结果集中的元素数量 sinter key… 交集运算 O(N*M) 当key不存在，视为空集合返回交集集合 sinterstore dest key… 交集运算，将结果保存到dest中 O(N*M) dest如果已存在，将其覆盖dest可以是key自身返回结果集中的元素数量 sunion key… 并集运算 O(N) 当key不存在，视为空集合返回并集集合 sunionstore dest key… 并集运算，将结果保存到dest中 O(N) dest如果已存在，将其覆盖dest可以是key自身返回结果集中的元素数量 scard key 获取集合key的元素数量 O(1) 返回集合个数，当key不存在时，返回0 srandmember key 随机获取集合key中的一个或多个元素（取决于count的大小） O(1)O(N) count 当0&lt;count&lt;集合size，返回count个不重复的元素当count&gt;=集合size，返回整个集合当count&lt;0，返回count个可能重复的元素当key不存在，未提供count参数，返回nil当key不存在，提供count参数，返回空集合 spop key 删除并返回集合中的一个随机元素 O(1) 返回被移除的随机元素当key不存在或key是空集合，返回nil smove src dest member 将member元素从src集合移动到dest集合 O(1) 当src不存在或member元素不在src中，不执行任何操作，当dest中已包含member元素，该操作仅将src的member元素删除如果member元素在src中且被成功移除，返回1如果member元素不是src集合成员，并且没有对dest执行操作返回0 有序集合有序集合与集合的区别就在于它是有序的，在集合的基础上，有序集合中的元素多了一个score属性，使得有序集合可以基于score进行排序等与数值有关的操作，有序集合和列表也有些相似，以下是列表、集合、有序集合三者的比较 列表 集合 有序集合 是否有序 ✔ x ✔ 是否唯一 x ✔ ✔ 底层实现 链表 散列表 散列表和跳表 有序集合与列表的差异还在于： 列表访问两端的元素很快，元素越多，访问中间的元素越慢，有序集合基于散列表和跳表实现，读取中间部门的数据也很快 列表无法简单的调整元素的位置，有序集合可以 常用命令 命令 功能 复杂度 可选参数 说明 zadd key score member [score member …] 将一个或多个member元素及其score加入有序集合key中 O(M*log(N)) 当member元素已存在有序集合中将更新score值和元素的位置如果key不存在，创建一个有序集合并执行插入操作返回成功添加的元素数量（不包含已存在的元素） zscore key member 获得元素的score O(1) 如果key不存在或member元素不在有序集合key中，返回nil zrange key start stop [withscores] 获取有序集合key中，指定区间内的元素 O(log(N)+M) withscores 按照score从小到大的顺序返回索引从start到stop之间的所有元素如果start或stop大于0，表示从前向后查找如果start或stop小于0，表示从后向前查找-1表示最后一个与元素可选参数withscores表示一起返回元素和score值例如元素1，score1，元素2，score2… zrevrange key start stop [withscores] 获取有序集合key中，指定区间内的元素 O(log(N)+M) withscores 与zrange不同的是，zrevrange是按照从大到小的顺序来排列 zrangebyscore key min max [withscores] [limit offset count] 获取有序集合 key 中，所有 score 值介于 min 和max 之间(包括等于 min 或 max )的元素。 O(log(N)+M) withscoreslimit offset count 有序集合元素按 score 值递增(从小到大)次序排列withscores参数会将score一起返回，limit参数指定返回结果的数量和区间，类似sql的 select limit offset countmin，max在默认情况下的取值为闭区间也可以通过加(符号来使用开区间 zincrby key increment member 为一个元素的score值加上增量increment O(log(N)) 返回元素新的score值当key不存在，或member不在有序集合key中时，等同于zadd key score member当increment&gt;0，增加score当increment&lt;0，减少score zcard key 获取有序集合key中元素的数量 O(1) 返回有序集合元素个数当key不存在时，返回0 zcount key min max 获取有序集合key中，score值在min和max之间（包括等于min或max）的元素数量 O(log(N)+M) 返回score值在min和max之间的元素数量 zrem key member … 删除有序集合key中一个或多个member元素 O(M*log(N)) 如果member不存在，忽略否则返回成功删除的元素数量（不包含忽略的元素） zremrangebyrank key start stop 删除有序集合key中指定排名(rank)区间内(包含start和stop)的所有元素 O(log(N)+M) 有序集合按照score从小到大排序删除start和stop区间内（包含start和stop）的元素当start或stop&gt;0，表示从前向后查找当start或stop&lt;0，表示从后向前查找 zremrangebyscore key min max 删除有序集合key中，指定score区间内的所有元素 O(log(N)+M) 有序集合按照score从小到大排序，删除score区间在min和max之间的元素默认情况下，删除的元素包含等于min或max的元素，可以使用(符号表示开区间 zrank key member 获取member元素在有序集合key中从小到大排序后的索引 O(log(N)) score值最小的索引为0 zrevrank key member 获取member元素在有序集合key中从大到小排序后的索引 O(log(N)) score值最大的索引为0 zinterstore dest numkeys key …[weights weight…][aggregate sum/min/max] 计算给定的一个或多个有序集合的交集其中给定 key 的数量必须以 numkeys 参数指定，并将该交集(结果集)储存到 dest O(NK)+O(Mlog(M)) weightsaggregate weights，设置每个集合的权重，参与计算时元素要乘上该集合的权重aggregate默认取值为sum，计算每个元素score的和设置为min时，计算每个元素的最小score值设置为max时，计算每个元素的最大score值 zunionstore dest numkeys key …[weights weight…][aggregate sum/min/max] 计算给定的一个或多个有序集合的并集其中给定 key 的数量必须以 numkeys 参数指定，并将该并集(结果集)储存到 dest O(N)+O(Mlog(M)) weightsaggregate 同zinterstore命令]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>redis数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis安装]]></title>
    <url>%2F2018%2F08%2F04%2FRedis%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[在学习Redis之前，首先要安装redis的运行环境，了解它的基础命令和基本知识。 下载redis 进入redis官网的下载页面https://redis.io/download 从这个页面我们可以得到的一些基本信息是，redis分为稳定版和非稳定版，其中偶数版本号为稳定版，奇数版本号为非稳定版。截至目前（2018年8月）最新的稳定版为4.0.11，Beta版为5.0-rc4，如果要下载之前的版本，页面往下拉 有oldVersion可以选择，直接提供下载的是3.2.12版本，如果要找其他版本，你可以通过这个连接http://download.redis.io/releases/下载，我安装的是3.0版本（3.0应该是redis的一个里程碑，加入集群支持后，redis完成了分布式的实现，新的embedded string编码格式，LRU算法的优化和其他性能的提升，如果之前接触过Redis，这些功能应该让你感到兴奋，从此memcache有的功能，redis几乎都有了，技术选型中使用redis将是更好的选择） 安装redis在linux下安装 我是在centos6的虚拟机上安装，输入以下命令 1234wget http://download.redis.io/releases/redis-3.0.0.tar.gztar xzf redis-3.0.0.tar.gzcd redis-3.0.0make 逐行执行，成功后再执行 1make install 将启动命令等复制到bin目录中，方便以后敲命令，linux下安装编译比较简单，没有其他依赖。 在windows下安装redis官网上没有windows的安装包，微软发了个补丁让redis可以在windows下运行，但是考虑今后的生产环境应该都会使用linux，所以在windows下也建议以虚拟机或Cygwin的方式运行redis（Cygwin是一个类似虚拟机的linux模拟环境，它将linux下的软件重编译后在linux下运行，而且他的软件包还提供了上千个工具，windows下学习linux相关应用的神器！！！） 首先，在Cygwin官网上下载http://www.cygwin.com/setup-x86_64.exe，根据提示选择下载方式、安装路径、最后出现软件包安装界面 将gcc和make的组件勾选，我这里显示”keep”是因为已经安装过了，如果没安装过，显示的是版本号，Skip表示不安装。 然后下载redis的源码，命令与linux下安装时一样，但是要注意，在make之前还要修改src目录下的redis.h文件，在头部加入 12345#ifdef CYGWIN#ifndef SA ONSTACK#define SA ONSTACK 0x08000000#endif#endif cygwin的文件路径就是安装路径，例如D:\cygwin64\，修改文件可以直接找到目录下的文件，用文本编辑器打开修改，也可以在linux下用vim修改。 修改之后在redis根目录执行make，如果报错，请参考这篇文章《redis3.0.6安装（linux和windows）》 启停Redis 首先来了解几个文件，在执行make install之后，redis的几个常用可执行文件会复制到Cygwin的bin目录下，因此可以通过直接输入命令来运行，省去路径 文件/命令 说明 redis-server redis服务端启动 redis-cli redis客户端启动 redis-benchmark redis性能测试 redis-check-aof aof文件修复工具 redis-check-dump rdb文件检查工具 redis-sentinel sentinel服务 启动redis1$ redis-server 出现以上提示，表示启动成功 redis的默认端口为 6379，如果要修改端口，可以用以下参数启动（xxxx用实际端口号替换） 1$ redis-server --port xxxx 也可以通过 1$ redis-server -h 或 $ redis-server --help 查看其他启动参数，具体的启动参数如下 参数 说明 省略 按照默认配置启动 /etc/xxx.conf 按照指定的配置文件启动 -v 或 –version 查看redis版本号 -slaveof ip port 将当前服务器转为指定服务器的slave –loglevel xxx 配置日志级别，xxx有4个可选参数，分别是debug\verbose\notice\warning 停止Redisredis支持优雅停机，在客户端发送shutdown命令，会先断开客户端连接，然后根据配置执行持久化策略，完成后退出 1$ redis-cli shutdown redis可以处理系统的kill命令，效果与shutdown一样，但是kill -9 命令将直接终止redis进程，无法优雅停机 启动命令行客户端 1$ redis-cli 和启动服务端一样，启动客户端也可以使用启动参数 参数 说明 -h hostname，服务端的ip -p port，服务端的端口 -s socket 包含了hostname和port -a password 服务端密码 -r n，n表示重复执行的次数，如-r 100 -i 配合-r命令，命令执行的间隔时间 -n 选择数据库，默认是0 -x 从磁盘/缓冲区读取 -d 设置换行符，默认是”\n” -c 启用集群 –raw 返回格式化后的输出信息 –no-row 返回原始的输出信息 –csv 以csv格式输出 –stat 查看redis服务的实时状态，基本信息包含keys数量\memory大小\client数量\blocked统计请求数\连接数等 –latency 服务延迟持续采样，一般用于监视性能，输出平均延时时间 –latency-history 服务延迟持续采样，间隔时间输出，大概每15秒左右输出一组结果 –latency-dist 服务延迟采样，以带色块的频谱（这个翻译可能不太好）输出，默认每1秒输出一行，可以用-i参数修改 –lru-test 针对某个key，测试lru算法 –slave slave服务 –rdb 转储rdb到本地文件 –pipe 将redis协议格式的数据发送的服务端执行 –pipe-timeout 功能同上，带超时参数，默认30秒，如果服务端没有返回结果，自动终止 –bigkeys 查找占用内存较大的key，列出每种数据类型key的最大size和平均size –scan 扫描redis的key –pattern 以sacn的方式，扫描指定的pattern –intrinsic-latency 测试系统延迟，需在server上执行 –eval 执行lua脚本]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>Cygwin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring事务详解(二)]]></title>
    <url>%2F2018%2F06%2F02%2FSpring%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[上一篇主要介绍了Spring事务的基本概念，包括基本流程、传播属性和隔离级别等，接下来将详细分析Spring事务源码，一步步了解Spring事务是如何实现的。 事务配置入口从事务的配置入手 12345&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt;&lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt;&lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt; 开启声明式事务，需要配置两个地方： 数据源dataSource，任何ORM的实现都要实现dataSource接口，因为必须拿到Connection对象； 事务管理器transactionManager，由Spring提供，管理所有的事务操作。 打开DataSourceTransactionManager 1234public class DataSourceTransactionManager extends AbstractPlatformTransactionManager implements ResourceTransactionManager, InitializingBean &#123; //省略 &#125; DataSourceTransactionManager是JDBC默认的事务管理器实现，它继承自AbstractPlatformTransactionManager，继续向上查找，发现其实现了PlatformTransactionManager接口 123public abstract class AbstractPlatformTransactionManager implements PlatformTransactionManager, Serializable &#123; //省略&#125; PlatformTransactionManager是啥？它是Spring事务的管理接口，除了jdbc的实现DataSourceTransactionManager外，Spring还提供了其他几种ORM的实现，如下： 顺着这个配置，可以找到定义tx:annotation-driven的xsd文件spring-tx.xsd，再通过其命名空间，从spring.handlers中找到对应的实现类TxNamespaceHandler。 1http\://www.springframework.org/schema/tx=org.springframework.transaction.config.TxNamespaceHandler TxNamespaceHandlerTxNamespaceHandler是Spring事务管理的核心功能，它允许使用xml或注解来配置声明式事务 1234567891011121314151617181920212223242526272829public class TxNamespaceHandler extends NamespaceHandlerSupport &#123; //tx标签的属性名称 static final String TRANSACTION_MANAGER_ATTRIBUTE = "transaction-manager"; //默认的属性管理器 static final String DEFAULT_TRANSACTION_MANAGER_BEAN_NAME = "transactionManager"; //获取事务管理器的beanName static String getTransactionManagerName(Element element) &#123; //如果配置了 transaction-manager，取配置的beanName，否则取默认值 //DEFAULT_TRANSACTION_MANAGER_BEAN_NAME return (element.hasAttribute(TRANSACTION_MANAGER_ATTRIBUTE) ? element.getAttribute(TRANSACTION_MANAGER_ATTRIBUTE) : DEFAULT_TRANSACTION_MANAGER_BEAN_NAME); &#125; @Override public void init() &#123; //注册&lt;tx:advice/&gt;的解析类TxAdviceBeanDefinitionParser registerBeanDefinitionParser("advice", new TxAdviceBeanDefinitionParser()); //注册&lt;tx:annotation-driven/&gt;的解析类AnnotationDrivenBeanDefinitionParser registerBeanDefinitionParser("annotation-driven", new AnnotationDrivenBeanDefinitionParser()); //注册&lt;tx:jta-transaction-manager/&gt;的解析类 //JtaTransactionManagerBeanDefinitionParser registerBeanDefinitionParser("jta-transaction-manager", new JtaTransactionManagerBeanDefinitionParser()); &#125;&#125; Spring初始化时，会先加载init方法将解析器注册到parsers中，在解析xml时，根据标签从parsers中找到对应的解析器，使用委派模式进行解析 初始化时注册解析器到parsers中 12345private final Map&lt;String, BeanDefinitionParser&gt; parsers = new HashMap&lt;&gt;();//...省略protected final void registerBeanDefinitionParser(String elementName, BeanDefinitionParser parser) &#123; this.parsers.put(elementName, parser);&#125; 解析xml配置文件时，根据标签查找解析器 12345public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //查找解析器 BeanDefinitionParser parser = findParserForElement(element, parserContext); return (parser != null ? parser.parse(element, parserContext) : null);&#125; 123456789private BeanDefinitionParser findParserForElement(Element element, ParserContext parserContext) &#123; String localName = parserContext.getDelegate().getLocalName(element); BeanDefinitionParser parser = this.parsers.get(localName); if (parser == null) &#123; parserContext.getReaderContext().fatal( "Cannot locate BeanDefinitionParser for element [" + localName + "]", element); &#125; return parser;&#125; 具体的解析实现委派给对应的解析器 xml解析上文中，TxNamespaceHandler中支持两种事务配置方式：xml和注解，xml配置的范例如下，它对应的解析器为TxAdviceBeanDefinitionParser，用来处理事务的通知规则 12345678&lt;tx:advice id="transactionAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="add*" propagation="REQUIRED" rollback-for="RuntimeException"/&gt; &lt;tx:method name="remove*" propagation="REQUIRED" rollback-for="RuntimeException"/&gt; &lt;tx:method name="update*" propagation="REQUIRED" rollback-for="RuntimeException"/&gt; &lt;tx:method name="query*" read-only="true"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133//事务通知规则解析器class TxAdviceBeanDefinitionParser extends AbstractSingleBeanDefinitionParser &#123; //method元素,切面的方法配置 private static final String METHOD_ELEMENT = "method"; //方法名称 private static final String METHOD_NAME_ATTRIBUTE = "name"; //attributes元素 private static final String ATTRIBUTES_ELEMENT = "attributes"; //超时时间，获取超时时间配置 private static final String TIMEOUT_ATTRIBUTE = "timeout"; //事务只读属性 private static final String READ_ONLY_ATTRIBUTE = "read-only"; //事务传播属性 private static final String PROPAGATION_ATTRIBUTE = "propagation"; //事务隔离级别 private static final String ISOLATION_ATTRIBUTE = "isolation"; //回滚配置 private static final String ROLLBACK_FOR_ATTRIBUTE = "rollback-for"; //不回滚配置 private static final String NO_ROLLBACK_FOR_ATTRIBUTE = "no-rollback-for"; @Override protected Class&lt;?&gt; getBeanClass(Element element) &#123; return TransactionInterceptor.class; &#125; //标签解析实现，在父类AbstractSingleBeanDefinitionParser的parseInternal中调用 @Override protected void doParse(Element element, ParserContext parserContext, BeanDefinitionBuilder builder) &#123; //获取事务管理器 builder.addPropertyReference("transactionManager", TxNamespaceHandler.getTransactionManagerName(element)); //遍历&lt;tx:attributes/&gt;下的配置 List&lt;Element&gt; txAttributes = DomUtils.getChildElementsByTagName(element, ATTRIBUTES_ELEMENT); if (txAttributes.size() &gt; 1) &#123; //如果有多个&lt;tx:attributes/&gt;标签，记录解析异常（只允许有一个&lt;tx:attributes/&gt;标签） parserContext.getReaderContext().error( "Element &lt;attributes&gt; is allowed at most once inside element &lt;advice&gt;", element); &#125; else if (txAttributes.size() == 1) &#123; //仅有一个&lt;tx:attributes/&gt;标签，继续解析 Element attributeSourceElement = txAttributes.get(0); //解析&lt;tx:attributes/&gt;标签下的子标签 RootBeanDefinition attributeSourceDefinition = parseAttributeSource(attributeSourceElement, parserContext); //解析结果存入propertyValueList中 builder.addPropertyValue("transactionAttributeSource", attributeSourceDefinition); &#125; else &#123; // 如果没有&lt;tx:attributes/&gt;标签，先假设使用的是注解配置 builder.addPropertyValue("transactionAttributeSource", new RootBeanDefinition("org.springframework.transaction.annotation.AnnotationTransactionAttributeSource")); &#125; &#125; private RootBeanDefinition parseAttributeSource(Element attrEle, ParserContext parserContext) &#123; //将&lt;tx:attributes/&gt;下的method配置解析成list集合 List&lt;Element&gt; methods = DomUtils.getChildElementsByTagName(attrEle, METHOD_ELEMENT); ManagedMap&lt;TypedStringValue, RuleBasedTransactionAttribute&gt; transactionAttributeMap = new ManagedMap&lt;&gt;(methods.size()); transactionAttributeMap.setSource(parserContext.extractSource(attrEle)); for (Element methodEle : methods) &#123; //解析方法名称 String name = methodEle.getAttribute(METHOD_NAME_ATTRIBUTE); //以方法名定义TypedStringValue对象 TypedStringValue nameHolder = new TypedStringValue(name); nameHolder.setSource(parserContext.extractSource(methodEle)); //事务规则对象，包含回滚规则、传播规则、只读属性、超时时间和隔离级别规则 RuleBasedTransactionAttribute attribute = new RuleBasedTransactionAttribute(); //分别解析传播属性、隔离级别、超时时间、只读属性、回滚和不回滚配置 String propagation = methodEle.getAttribute(PROPAGATION_ATTRIBUTE); String isolation = methodEle.getAttribute(ISOLATION_ATTRIBUTE); String timeout = methodEle.getAttribute(TIMEOUT_ATTRIBUTE); String readOnly = methodEle.getAttribute(READ_ONLY_ATTRIBUTE); if (StringUtils.hasText(propagation)) &#123; attribute.setPropagationBehaviorName(RuleBasedTransactionAttribute.PREFIX_PROPAGATION + propagation); &#125; if (StringUtils.hasText(isolation)) &#123; attribute.setIsolationLevelName(RuleBasedTransactionAttribute.PREFIX_ISOLATION + isolation); &#125; if (StringUtils.hasText(timeout)) &#123; try &#123; attribute.setTimeout(Integer.parseInt(timeout)); &#125; catch (NumberFormatException ex) &#123; parserContext.getReaderContext().error("Timeout must be an integer value: [" + timeout + "]", methodEle); &#125; &#125; if (StringUtils.hasText(readOnly)) &#123; attribute.setReadOnly(Boolean.valueOf(methodEle.getAttribute(READ_ONLY_ATTRIBUTE))); &#125; //回滚规则解析 List&lt;RollbackRuleAttribute&gt; rollbackRules = new LinkedList&lt;&gt;(); if (methodEle.hasAttribute(ROLLBACK_FOR_ATTRIBUTE)) &#123; String rollbackForValue = methodEle.getAttribute(ROLLBACK_FOR_ATTRIBUTE); //解析回滚配置 addRollbackRuleAttributesTo(rollbackRules,rollbackForValue); &#125; if (methodEle.hasAttribute(NO_ROLLBACK_FOR_ATTRIBUTE)) &#123; String noRollbackForValue = methodEle.getAttribute(NO_ROLLBACK_FOR_ATTRIBUTE); //解析不回滚配置 addNoRollbackRuleAttributesTo(rollbackRules,noRollbackForValue); &#125; attribute.setRollbackRules(rollbackRules); //将解析的结果放入方法名对象nameHolder中 transactionAttributeMap.put(nameHolder, attribute); &#125; RootBeanDefinition attributeSourceDefinition = new RootBeanDefinition(NameMatchTransactionAttributeSource.class); attributeSourceDefinition.setSource(parserContext.extractSource(attrEle)); attributeSourceDefinition.getPropertyValues().add("nameMap", transactionAttributeMap); return attributeSourceDefinition; &#125; //解析回滚配置，可以用","配置多种回滚异常，解析成list private void addRollbackRuleAttributesTo(List&lt;RollbackRuleAttribute&gt; rollbackRules, String rollbackForValue) &#123; String[] exceptionTypeNames = StringUtils.commaDelimitedListToStringArray(rollbackForValue); for (String typeName : exceptionTypeNames) &#123; rollbackRules.add(new RollbackRuleAttribute(StringUtils.trimWhitespace(typeName))); &#125; &#125; private void addNoRollbackRuleAttributesTo(List&lt;RollbackRuleAttribute&gt; rollbackRules, String noRollbackForValue) &#123; String[] exceptionTypeNames = StringUtils.commaDelimitedListToStringArray(noRollbackForValue); for (String typeName : exceptionTypeNames) &#123; rollbackRules.add(new NoRollbackRuleAttribute(StringUtils.trimWhitespace(typeName))); &#125; &#125;&#125; 以上是解析xml事务通知配置的代码，接下来看注释方式的解析代码 注解解析注解方式的配置，需要AOP的支持，AOP默认采用jdk动态代理实现，如果类没有实现接口，无法生成代理类，如果遇到此类情况，需要开启cglib代理的支持 1&lt;tx:annotation-driven transaction-manager="transactionManager" proxy-target-class="true"/&gt; 注解解析器的实现为AnnotationDrivenBeanDefinitionParser 1234567891011121314151617181920class AnnotationDrivenBeanDefinitionParser implements BeanDefinitionParser &#123; //解析器实现 @Override @Nullable public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //注册TransactionalEventListenerFactory registerTransactionalEventListenerFactory(parserContext); //解析mode属性 String mode = element.getAttribute("mode"); if ("aspectj".equals(mode)) &#123; // mode="aspectj" registerTransactionAspect(element, parserContext); &#125; else &#123; // mode="proxy" AopAutoProxyConfigurer.configureAutoProxyCreator(element, parserContext); &#125; return null; &#125; 首先注册了TransactionalEventListenerFactory，然后获取mode属性，根据mode属性做进一步解析，首先分析proxy模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//静态内部类，aop代理配置private static class AopAutoProxyConfigurer &#123; public static void configureAutoProxyCreator(Element element, ParserContext parserContext) &#123; //1、注册InfrastructureAdvisorAutoProxyCreator AopNamespaceUtils.registerAutoProxyCreatorIfNecessary(parserContext, element); String txAdvisorBeanName = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME; if (!parserContext.getRegistry().containsBeanDefinition(txAdvisorBeanName)) &#123; Object eleSource = parserContext.extractSource(element); //2、创建TransactionAttributeSource definition，之前xml的分析中，如果没有&lt;tx:attributes/&gt;标签，和这里设置的一样 RootBeanDefinition sourceDef = new RootBeanDefinition( "org.springframework.transaction.annotation.AnnotationTransactionAttributeSource"); sourceDef.setSource(eleSource); //设置role属性，标记为内部bean sourceDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); //如果@Component/@Repository/@Service/@Controller注解中配置了value属性，value值作为bean的名称，否则用默认名称 String sourceName = parserContext.getReaderContext().registerWithGeneratedName(sourceDef); //3、创建TransactionInterceptor definition. RootBeanDefinition interceptorDef = new RootBeanDefinition(TransactionInterceptor.class); interceptorDef.setSource(eleSource); //也标记为内部bean interceptorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); //注册事务管理器 registerTransactionManager(element, interceptorDef); //将sourceName对应的bean注入TransactionInterceptor.transactionAttributeSource interceptorDef.getPropertyValues().add("transactionAttributeSource", new RuntimeBeanReference(sourceName)); //获取bean的名称 String interceptorName = parserContext.getReaderContext().registerWithGeneratedName(interceptorDef); //4、创建TransactionAttributeSourceAdvisor definition. RootBeanDefinition advisorDef = new RootBeanDefinition(BeanFactoryTransactionAttributeSourceAdvisor.class); advisorDef.setSource(eleSource); //也标记为内部bean advisorDef.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); //将sourceName对应的bean注入BeanFactoryTransactionAttributeSourceAdvisor.transactionAttributeSource advisorDef.getPropertyValues().add("transactionAttributeSource", new RuntimeBeanReference(sourceName)); //将interceptorName对应的bean注入BeanFactoryTransactionAttributeSourceAdvisor.adviceBeanName advisorDef.getPropertyValues().add("adviceBeanName", interceptorName); //解析order属性 if (element.hasAttribute("order")) &#123; advisorDef.getPropertyValues().add("order", element.getAttribute("order")); &#125; //将TransactionAttributeSourceAdvisor以txAdvisorBeanName为名称注册到IOC容器中 parserContext.getRegistry().registerBeanDefinition(txAdvisorBeanName, advisorDef); //5、创建并注册CompositeComponentDefinition CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), eleSource); compositeDef.addNestedComponent(new BeanComponentDefinition(sourceDef, sourceName)); compositeDef.addNestedComponent(new BeanComponentDefinition(interceptorDef, interceptorName)); compositeDef.addNestedComponent(new BeanComponentDefinition(advisorDef, txAdvisorBeanName)); parserContext.registerComponent(compositeDef); &#125; &#125; &#125; 以上这段代码，就是Spring声明式事务的核心，整体看了个大概，接下来就分析细节。 InfrastructureAdvisorAutoProxyCreator它是做什么的？先看它的UML关系图 不难发现它间接实现了BeanPostProcessor接口，因此在bean实例化前后分别会调用postProcessBeforeInitialization和postProcessAfterInitialization方法 12345@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) &#123; //返回当前bean return bean;&#125; postProcessBeforeInitialization没有太多的操作，直接返回当前bean。 1234567891011121314//如果bean的子类标记了bean需要代理，使用配置的拦截器创建代理@Overridepublic Object postProcessAfterInitialization(@Nullable Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; //根据beanName和beanClass创建cacheKey Object cacheKey = getCacheKey(bean.getClass(), beanName); //判断当前类是否已经被代理 if (!this.earlyProxyReferences.contains(cacheKey)) &#123; //装饰器包装bean return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 123456789101112131415161718192021222324252627282930protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; //如果已经处理过，直接返回bean if (StringUtils.hasLength(beanName) &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; //如果bean无需代理，直接返回 if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; //判断是否是Spring内部bean，内部bean无需代理 if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // 获取当前bean的Advisor[] Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; //把bean标记为已代理放入缓存中 this.advisedBeans.put(cacheKey, Boolean.TRUE); //创建bean的代理类 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; postProcessAfterInitialization主要做了以下处理 判断类是否已经被代理，如果已被代理，返回bean，否则继续下一步； 获取当前bean的Advisor，如果当前bean无需代理，返回bean； 生成代理bean并返回； 以上的代码其实就是Spring中AOP对bean的代理增强，后续将分析如何提取事务注解并使事务生效的]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring事务</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>Spring</tag>
        <tag>事务注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring事务详解(一)]]></title>
    <url>%2F2018%2F05%2F31%2FSpring%E4%BA%8B%E5%8A%A1%E8%AF%A6%E8%A7%A3(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[什么是事务事务（Transaction）是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。 事务的四大特性（ACID）： 原子性（Atomicity），每个事务都是一个不可分割的工作单元，事务的操作结果要么都执行成功，要么都执行失败。 一致性（Consistency），事务必须是使数据库从一个一致性状态到另一个一致性状态。 隔离性（Isolation），事务与事务之间的执行不能互相干扰，即一个事务内部的操作及使用的数据对并发的其他事务是不可见的。 持久性（Durability），又叫做永久性（Permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久的，后续的其他操作或异常都不应该对其有任何影响。 事务操作的基本流程 事务开启（begin） 事务执行（execute） 事务提交（自动提交AutoCommit / 手动提交CustomCommit）/事务回滚（rollback）（假如出现错误） 事务关闭（close） 事务的基本原理Spring事务是基于数据库对事务的支持，数据库不支持事务，Spring是无法提供事务功能，下表列出了对事务支持/不支持的主流数据库产品 数据库产品 事务支持 MySql（MyISAM） ❌ MySql（InnoDB） ✔ PostgreSql ✔ SQL Server ✔ Oracle ✔ Sybase ✔ MongoDb ❌ Spring封装了JDBC的数据库操作，可以按照以下步骤进行 获取连接Connection con = DriverManager.getConnection() 开启事务，设置自动提交属性con.setAutoCommit（true/false） 执行CRUD 提交事务/回滚事务con.commit()/con.rollback() 关闭连接 con.close() Spring事务传播性Spring事务的传播性，就是在多个事务同时存在时，Spring如何处理这些事务的行为，在 TransactionDefinition中定义了如下属性 事务类型 作用 PROPAGATION_REQUIRED Spring默认传播属性，如果当前没有事务，新建一个事务 PROPAGATION_REQUIRES_NEW 新起事务，如果当前存在事务，则挂起当前事务，新起的事务与被挂起的事务不存在关联关系，各自独立，外层事务失败回滚，不会回滚内层事务执行的结果，内层事务失败抛出异常，外层事务可以捕获，是否回滚由业务决定 PROPAGATION_SUPPORTS 如果当前没有事务，就以非事务的方式执行 PROPAGATION_MANDATOR 如果当前没有事务，抛出异常 PROPAGATION_NOT_SUPPORTED 以非事务的方式执行操作，如果当前已存在事务，则挂起当前事务 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常 PROPAGATION_NESTED 如果存在一个活动的事务，则运行在嵌套事务中，如果没有活动事务，则按照PROPAGATION_REQUIRED类型执行，内部事务的回滚不会对外部事务造成影响，只对DataSourceTransactionManager 事务管理器有效 数据库隔离级别很多数据库管理系统定义了不同的“事务隔离等级”来控制锁的程度。隔离级别越高，锁的开销越大，同时也会增加死锁发生的几率，需要根据实际业务场景详细分析，设置合理的隔离级别。下表列出了数据库的隔离级别 隔离级别 级别值 说明 Read-Uncommitted（读未提交） 0 可能导致脏读 Read-Committed（读已提交） 1 避免脏读，允许不可重复度和幻读 Repeatable-Reads（可重复读） 2 避免脏读，不可重复读，允许幻读 Serializable （可串行化） 3 串行化，事务一个一个执行，避免脏读、不可重复度、幻读。执行效率低，谨慎使用 脏读事务A对数据进行了增删改，但是未提交，事务B可以读取到事务A未提交的数据，如果事务A此时回滚了，那么事务B读取到的就是脏数据 不可重复读在事务A中，进行了两次读操作，第一次读和第二次读操作之间，另外一个事务B对数据行了修改，造成事务A两次读取的数据不一致。 幻读事务A使用Where条件对一些数据进行查询，此时事务B插入一条数据，这条数据刚好满足事务A的Where条件，导致事务A两次查询出的结果集不一致。 隔离级别越高，数据完整性和一致性就越能得到保证，但是对并发性能的影响也越大 大多数的数据库默认级别是Read-Commited，如SqlServer、Oracle 有一些数据库的默认级别是Repeatable Read，如Mysql（InnoDB） Spring中的隔离级别Spring的事务是基于数据库的，因此隔离级别和数据库差不多，只是多了个默认级别，如下： Spring隔离级别 解释 ISOLATION_DEFAULT PlatfromTransactionManager默认的隔离级别，与选用数据库的默认隔离级别相同 ISOLATION_READ_UNCOMMITTED 最低的隔离级别，同数据库的Read-Uncommitted ISOLATION_READ_COMMITTED 同数据库的Read-Committed ISOLATION_REPEATABLE_READ 同数据库的Repeatable-Reads ISOLATION_SERIALIZABLE 同数据库的Serializable Spring事务嵌套通过上面的数据库事务理论知识，大致了解数据库事务和Spring事务的一些属性和特点，接着分析一些嵌套事务的场景，了解Spring事务的传播机制。 PROPAGATION_REQUIRED（Spring默认）支持当前事务，如果当前没有事务，创建一个事务执行 以ServiceA的methodA()调用ServiceB的methodB()为例， 如果methodA()，methodB()的事务级别定义为PROPAGATION_REQUIRED，那么执行该方法的时候，已经有了事务，此时调用ServiceB.methodB()，发现已经运行在methodA()的事务内，就不再起新事务； 如果ServiceB.methodB()被调用时并没有在事务中，就会新建一个事务运行； 此时无论是methodA()异常还是methodB()异常，事务都会被回滚。 PROPAGATION_REQUIRES_NEW新建事务，如果当前已存在事务，则把当前事务挂起。 假设ServiceA.methodA()的事务级别为PROPAGATION_REQUIRED，ServiceB.methodB()的事务级别为PROPAGATION_REQUIRES_NEW，当执行methodB()时，methodA()所在的事务就被挂起了，等methodB()的事务执行完，才能继续执行methodA()的事务。 这种情况下，内层事务（methodB()的事务）如果已提交，外层事务（methodA()的事务）异常不会导致内层事务回滚。反之内层事务抛出的异常，可以由外层事务捕获，至于外层事务要不要回滚，取决于对异常的处理。 PROPAGATION_SUPPORTS支持当前事务，如果当前没有事务，以非事务的方式执行。这个比较好理解，methodA()如果存在事务，methodB()就加入当前事务中，如果没有事务，则methodB()也不开启事务，内层事务完全依赖于外层事务。 PROPAGATION_MANDATORY支持当前事务，如果当前没有事务，抛出异常。也很好理解，方法必须运行在事务下，如果外层事务不存在，抛异常。 PROPAGATION_NOT_SUPPORTED以非事务方式执行，如果当前存在事务，则挂起当前事务。 PROPAGATION_NEVER与PROPAGATION_MANDATORY相反，PROPAGATION_NEVER完全以非事务方式执行，如果存在事务，抛异常。 PROPAGATION_NESTED这个传播属性就相对复杂了，如果methodB()声明为PROPAGATION_NESTED，在执行methodB()时如果回滚，内层事务将回到一个保存点（SavePoint），而外层事务methodA()该如何处理？可以有以下两个方式： 捕获异常，在catch块里增加业务处理； 回滚/提交]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring事务</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>Spring</tag>
        <tag>事务隔离</tag>
        <tag>事务传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Integer踩坑与Integer的实现原理]]></title>
    <url>%2F2018%2F04%2F23%2FInteger%E8%B8%A9%E5%9D%91%E4%B8%8EInteger%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Integer是很常用的包装类，探索它的实现原理，要从一个测试BUG说起。 问题现象前几天一个账务统计的功能重构，提交测试不久马上收到了测试的BUG邮件，统计结果错误，从测试用例看来，覆盖的用例中所有的测试结果都是错的，赶紧翻开代码排查。 简单描述要实现的功能：A和B的序号互换，A和B的序号都是Integer类型，简化后的错误代码如下： 12345678910111213public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Integer seqA = 1; Integer seqB = 2; System.out.println("交换前: seqA=" + seqA + ",seqB=" + seqB); swap(seqA, seqB); System.out.println("交换后: seqA=" + seqA + ",seqB=" + seqB); &#125; private static void swap(Integer seqA, Integer seqB) throws NoSuchFieldException, IllegalAccessException &#123; int tmp = seqA.intValue(); seqA = seqB.intValue(); seqB = tmp; &#125; 错的很明显有木有？ 值传递和引用传递的问题，对于引用类型，赋值运算会改变引用中保存的地址，原来的地址被覆盖了，但是！！！原来的对象并不会被改变。 运行结果可想而知 12交换前: seqA=1,seqB=2交换后: seqA=1,seqB=2 打回重写，15分钟后，开发同学重新提交了代码 1234567private static void swap(Integer seqA, Integer seqB) throws NoSuchFieldException, IllegalAccessException &#123; Field field = Integer.class.getDeclaredField("value"); field.setAccessible(true); int tmp = seqA.intValue(); field.set(seqA, seqB.intValue()); field.set(seqB, tmp); &#125; 嗯，利用反射改变A和B的值，应该可以了，运行试试： 12交换前: seqA=1,seqB=2交换后: seqA=2,seqB=2 ？？？A的值改变了，为何B的值还是旧的？ 探索原因为了解开这个疑惑，先来看一下这段代码： 12345public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Integer a = 1; Integer b = 1; System.out.println("a==b "+(a == b)); &#125; 运行结果会是什么？ 1a==b true 对象之间的比较，使用==比较的是对象地址，equals才是比较对象的值，为什么这个结果是true？ 别急，反编译看看 12345678910111213141516171819202122232425262728293031323334D:\work\workspace\demo\target\classes\integer&gt;javap -c IntegerDemo.classCompiled from "IntegerDemo.java"public class integer.IntegerDemo &#123; public integer.IntegerDemo(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public static void main(java.lang.String[]) throws java.lang.NoSuchFieldException, java.lang.IllegalAccessException; Code: 0: iconst_1 1: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 4: astore_1 5: iconst_1 6: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 9: astore_2 10: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 13: new #4 // class java/lang/StringBuilder 16: dup 17: invokespecial #5 // Method java/lang/StringBuilder."&lt;init&gt;":()V 20: ldc #6 // String a==b 22: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 25: aload_1 26: aload_2 27: if_acmpne 34 30: iconst_1 31: goto 35 34: iconst_0 35: invokevirtual #8 // Method java/lang/StringBuilder.append:(Z)Ljava/lang/StringBuilder; 38: invokevirtual #9 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 41: invokevirtual #10 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 44: return&#125; main方法的第1行和第6行分别对应Integer a = 1;Integer b = 1;调用的是Integer.valueOf方法。看看这个方法做了什么 123456public static Integer valueOf(int i) &#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; 这里用到了Cache，看看IntegerCache是如何定义的，用来做什么的 1234567891011121314151617181920212223242526private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty("java.lang.Integer.IntegerCache.high"); if (integerCacheHighPropValue != null) &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++);//-128-127的包装类作为cache数组的值 &#125; private IntegerCache() &#123;&#125; &#125; Integer初始化时，对-128~127区间的数字做了缓存处理，这些数字对应的包装类作为cache数组的值。在Integer.valueOf方法中，先判断传入的值是否在此区间内，如果是，通过cache的下标取出对应的value，否则new Integer()。 验证一下 123456789101112131415public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Integer a1 = 127; Integer a2 = 127; Integer b1 = 128; Integer b2 = 128; Integer c1 = -128; Integer c2 = -128; Integer d1 = -129; Integer d2 = -129; System.out.println(" a1==a2 "+(a1 == a2)); System.out.println(" b1==b2 "+(b1 == b2)); System.out.println(" c1==c2 "+(c1 == c2)); System.out.println(" d1==d2 "+(d1 == d2)); &#125; 结果 1234a1==a2 trueb1==b2 falsec1==c2 trued1==d2 false 现在我们回到A和B互换的问题 利用反射赋值，看看field.set方法 12345678910public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException&#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; checkAccess(Reflection.getCallerClass(), clazz, obj, modifiers); &#125; &#125; getFieldAccessor(obj).set(obj, value);&#125; 第二个参数接收的是Object，在互换的代码中，seqB.intValue()进行了一次自动装箱，相当于 1field.set(seqA, Integer.valueOf(seqB.intValue())); 此时seqA = 2，cache缓存中的值会如何变化？通过下面的代码来验证 123456789private static void swap(Integer seqA, Integer seqB) throws NoSuchFieldException, IllegalAccessException &#123; Field field = Integer.class.getDeclaredField("value"); field.setAccessible(true); int tmp = seqA.intValue(); field.set(seqA, seqB.intValue()); System.out.println("int tmp = " + tmp); System.out.println("Integer.valueOf(tmp) = " + Integer.valueOf(tmp)); field.set(seqB, tmp); &#125; 运行结果 1234交换前: seqA=1,seqB=2int tmp = 1Integer.valueOf(tmp) = 2交换后: seqA=2,seqB=2 tmp = 1，但是 Integer.valueOf(tmp) = 2 ？？？Debug i=1，cache对应的下标为129，因为先前做了反射赋值，缓存Integer的值被改成了2，最终返回2。 问题找到了，那么如果要用反射的方式，如何修改代码呢？因为是在反射赋值之后，调用valueOf方法读取cache获取了错误结果，那么在反射赋值前创建Integer临时对象，再赋值给seqB就可以了。修改后的代码： 1234567private static void swap(Integer seqA, Integer seqB) throws NoSuchFieldException, IllegalAccessException &#123; Field field = Integer.class.getDeclaredField("value"); field.setAccessible(true); Integer tmp = new Integer(seqA.intValue()); field.set(seqA, seqB.intValue()); field.set(seqB, tmp); &#125; 运行结果： 12交换前: seqA=1,seqB=2交换后: seqA=2,seqB=1 总结 Integer的实现中设计了缓存，缓存的区间下限是-128，默认上限为127（下限不可更改，上限可以通过JDK的参数-XX:AutoBoxCacheMax修改），缓存了对应数值的Integer对象 在上下限区间内，从缓存中获取值，在这个区间内，同一个数返回的对象相同。 反射赋值会改变对应缓存对象的值]]></content>
      <categories>
        <category>JDK源码</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>Integer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC和内存分配策略]]></title>
    <url>%2F2018%2F04%2F13%2FGC%E6%94%B6%E9%9B%86%E5%99%A8%E5%92%8C%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[收集算法是内存回收的理论依据，垃圾收集器就是内存回收的具体实现。Java虚拟机的规范对垃圾收集器应该如何实现并没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器差别很大。下图是JDK1.6Update22所提供的7种垃圾收集器（G1收集器在此版本属于实验版本，直到JDK1.7 Update14，Hotspot才正式提供了商用的G1收集器） 现代的商用JVM，采用的都是分代收集算法，老年代和新生代的垃圾收集器配合使用，共同完成堆和方法区的GC。上图两个存在连线的收集器，表示可以搭配使用，接下来逐个介绍垃圾收集器。 新生代收集器Serial先看看官方文档怎么介绍它 The discussion to this point has been about the serial collector. The Java HotSpot VM includes three different types of collectors, each with different performance characteristics. The serial collector uses a single thread to perform all garbage collection work, which makes it relatively efficient because there is no communication overhead between threads. It is best-suited to single processor machines, because it cannot take advantage of multiprocessor hardware, although it can be useful on multiprocessors for applications with small data sets (up to approximately 100 MB). The serial collector is selected by default on certain hardware and operating system configurations, or can be explicitly enabled with the option -XX:+UseSerialGC. Serial收集器是最早的收集器，在JDK1.3.1之前，作为新生代虚拟机唯一的选择，它是一个单线程的收集器。即使有多个CPU的处理器，它也只会使用其中一个和单个线程去完成垃圾收集工作，更糟糕的是在它进行垃圾收集时，必须暂停其他所有工作线程（也就是Stop The World），直到GC结束，以下是Serial / Serial Old一起工作的示意图。 Stop The World是在后台进行的，对用户不可见，因此停顿对用户体验的影响是非常大的。虽然GC的停顿在所难免，但是Hotspot的工程师们一直在致力于优化并开发出效率更高，停顿时间更短的垃圾收集器。Serial虽然古老、过时，但是与其他收集器的单线程比较，它是相对高效的，并且仍然是Client模式下新生代的默认收集器。对于单CPU环境而言，Serial没有线程切换的开销，“专心”的做垃圾回收工作自然效率很高。在GUI应用中，分配给JVM的内存一般不会很大（通常不会超过100MB），短暂的停顿是可以接受的。 ParNewParNew收集器其实就是Serial收集器的多线程版本，他的实现复用了Serial相当多的代码，因此除了使用多条线程收集垃圾外，Serial可用的参数（比如：-XX:SurvivorRatio、-XX:PretenureSizeThreshold、-XX:handlePromotionFailure等）。ParNew的工作示意图： ParNew是Server模式下虚拟机中首选的新生代收集器，除了多线程回收这个特点外，还有一个与性能无关的原因是，除了Serial，只有它能够与CMS配合工作，如果在老年代启用-XX:+UseConcMarkSweepGC参数开启CMS，则新生代默认使用ParNew收集器，当然，也可以使用-XX:+UseParNewGC参数来强制开启。ParNew在单核CPU下由于存在线程切换的开销，GC性能并没有比Serial好，即使在双核CPU通过超线程技术都不能肯定的保证性能优于Serial收集器。当然，随着CPU的数量增加，在GC时的资源利用是有利的，可以通过-XX:ParallelGCThreads参数来限制GC的线程数。 Parallel ScavengeParallel Scavenge（以下简称PS）也是一个新生代的收集器，基于复制算法、并行的多线程收集器，它与ParNew的差异在哪？ PS的关注点与其他垃圾收集器不同，CMS等收集器关注的是尽可能的缩短Stop The World的时间，而PS更关心是否达到可控的吞吐量（Throughput）。 什么是吞吐量？ CPU运行用户代码的时间占CPU总消耗时间的比值，即 吞吐量 = 运行用户代码时间 / （运行用户代码时间 + GC耗时） 虚拟机总共耗时100min，GC耗时1min，吞吐量为99% 停顿时间越短，适合需要与用户频繁交互的程序。吞吐量高则可以有效的利用CPU时间，尽快完成程序的运算任务，适合后台程序，以下几个参数可以影响PS收集器的吞吐量： -XX:MaxGCPauseMillis 控制最大GC停顿时间，没有默认值，单位是毫秒。垃圾回收时尽量保证时间不超过设定值。但是，这个值不是越小越好，过小的设定值反而适得其反。例如原本10秒回收一次，停顿100毫秒，调小后变成5秒回收一次，停顿70毫秒。停顿时间确实缩短了，但是频率增加了，吞吐量也下来了。GC停顿时间缩短是以牺牲吞吐量和新生代的空间为代价的。 -XX:GCTimeRatio 回收时间占总时间的比率，相当于是吞吐量的倒数。默认为99，就是允许最大GC耗时占1%。 -XX:+UseAdaptiveSizePolicy 当这个参数打开后，新生代的大小（-Xmn）、Eden与Survivor的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等参数就不需要设置了，虚拟机会根据当前系统的运行状况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间和最大的吞吐量。这种方式称为GC自适应调节策略（GC Ergonomics）。 -Xmx 最大堆大小 如果对JVM的手工优化不太在行，可以开启以上参数，让PS配合自适应调节策略，是很不错的选择。满足以上参数设置存在先后顺序，优先级是： 最大停顿时间 最大吞吐量 满足以上两点，堆的最小大小 老年代收集器Serial OldSerial Old（以下简称SO）是Serial收集器的老年代版本，它采用的是标记-整理算法。在Client模式下作为默认的老年代收集器使用，在Server模式下，它有以下用途： 与Parallel Scavenge搭配使用 [^注]: Parallel Scavenge在老年代中有PS MarkSweep收集器来进行老年代收集，并没有直接使用Serial Old。但是它的实现与Serial Old很接近，官方资料中都用Serial Old替代PS MarkSweep 作为CMS失败的备选收集器 它的工作示意图如下： Serial Old和Serial没有太大区别，也是一个单线程的收集器。 Parallel OldParallel Old（以下简称PO）是PS收集器的老年代版本，使用多线程和标记-整理算法。这个收集器是从JDK1.6之后开始提供的，在此之前，新生代PS一直处于尴尬的位置。如果新生代选择了PS收集器，老年代除了SO收集器外别无选择。但是SO是单线程的，在Server模式下性能不好，即使配合PS收集器也不一定能够获得吞吐量的最大化效果，甚至不如ParNew + CMS的组合给力。 直到PO的出现，”吞吐量优先“终于有了比较名副其实的组合。在吞吐量优先的场景下，可以优先使用PS+PO组合，他们的工作示意图如下： CMSCMS收集器是一种以获取最短停顿时间为目标的并发收集器。主要应用在Server端，重视服务的响应速度，减少停顿时间。首先要了解一下两个概念：并发与并行 并发（Concurrent）：用户线程与GC线程同时执行（不一定并行，可能交替执行），用户程序继续执行，而GC线程在另一个CPU上运行。 并行（Parallel）：多条GC线程同时工作，但此时用户线程仍然处于等待状态。 从以上概念来看，ParNew和Parallel Scavenge都属于并行收集器，GC线程运行时，用户线程必须停止。而CMS的GC线程可以与用户线程同时运行在不同的CPU上。它的运行过程相对于前面几个垃圾收集器更复杂一些，整个过程分为以下4步： 初始标记（CMS initial mark）(STW) 仅仅只是标记以下GC Roots能直接关联到的对象，速度很快 并发标记（CMS concurrent mark） 就是进行GC Roots Tracing的过程，此时用户线程也在运行，会产生对象可达性状态变化 重新标记（CMS remark）(STW) 对并发标记过程中可达性状态变化的对象再次标记，这个停顿时间比初始标记略长 并发清除（CMS concurrent sweep） 初始标记、重新标记都要Stop The World。 CMS工作示意图如下： 使用-XX:+UseConcMarkSweepGC参数在老年代启用CMS收集器。 CMS的优点表现在：并发收集、低停顿，官方文档也称之为并发低停顿收集器（Concurrent Low Pause Collector），但是CMS也存在以下三个明显的缺点： 对CPU资源非常敏感。 在GC运行时，虽然用户线程不会停止，但是会因为占用了一部分CPU资源而导致响应变慢，总吞吐量降低。CMS默认的回收线程数=(N+3)/4，也就是当CPU在4个以上时，并发回收的GC线程占用不超过25%的CPU资源。但是CPU不足4个时，CMS对程序的影响就变得很大，如果CPU负载本来就比较大时，还分出一半的运算能力去执行GC线程，就会导致用户线程执行速度突降50%。此时虚拟机为了应对这种情况，提供了一种”增量模式“（Incremental Mode）。 Incremental Mode Note that the incremental mode is being deprecated in Java SE 8 and may be removed in a future major release. The CMS collector can be used in a mode in which the concurrent phases are done incrementally. Recall that during a concurrent phase the garbage collector thread is using one or more processors. The incremental mode is meant to lessen the effect of long concurrent phases by periodically stopping the concurrent phase to yield back the processor to the application. This mode, referred to here as i-cms, divides the work done concurrently by the collector into small chunks of time that are scheduled between young generation collections. This feature is useful when applications that need the low pause times provided by the CMS collector are run on machines with small numbers of processors (for example, 1 or 2). 所谓”增量模式“，就是在并发标记、并发清理时让GC线程和用户线程抢占CPU资源，交替运行，减少GC线程独占CPU的时间，延长整个GC过程，对用户线程的影响尽可能的减小。但是在JDK8中，这个模式已经不建议使用了。 浮动垃圾碎片（Floating Garbage）无法处理。由于GC线程运行时，用户线程并没有停止，此时还会有新的垃圾产生，就好比垃圾车一边在清理路上的落叶，而此时一阵风刮过，落叶又落在了已经清理过的区域上。这一部分的垃圾出现在标记过程后面，无法在本次GC种处理，只能等待下一次GC。因此CMS不能像其他收集器一样，等老年代填满了之后才进行GC，必须预留一定的空间给用户线程，可以调整参数-XX:CMSInitiatingOccupancyFraction的值来设置百分比，默认情况下设定值为68%。如果CMS运行期预留的内存无法满足新生代内存晋升的需求，就会出现一次Concurrent Mode Failure，此时就会启动预备方案，采用Serial Old对老年代进行回收，这样性能就更差了，设定合理的参数，预留老年代内存百分比就非常重要了。 内存碎片 CMS是基于标记-清除算法的收集器，这意味着每次GC之后都会产生大量的不连续内存空间，内存碎片过多，在分配大对象时就找到不足够的连续空间进行分配，不得不提前触发一次Full GC。为了解决这个问题，CMS提供了-XX:+UseCMSCompactAtFullCollection参数，在Full GC之后触发一次内存碎片整理。内存碎片的问题解决了，但是停顿时间变长了，从这个角度出发，并不是每次GC后都要进行内存碎片整理，因此另一个参数-XX:CMSFullGCsBeforeCompaction可以设置多少次不压缩的Full GC后，进行一次内存碎片整理。 G1收集器G1（Garbage-First）是一款面向服务端应用的收集器，HotSpot开发团队希望它将来能替换CMS收集器，与其他收集器对比，G1有以下特点： 内存划分 G1也采用分代收集的方式，但是它的内存划分与其他收集器不一样，从逻辑上虽然保留了新生代、老年代、方法区的概念，但不再是物理隔离，而是划分为多个大小固定的独立区域（Region）。 G1可以独立管理整个堆，根据这些Region的垃圾堆积程度，在后台维护一个优先列表，每次根据允许收集的时间，优先回收垃圾最多的区域（这就是Garbage First名字的由来）。正因为区域划分和优先级的区域回收，保证了G1收集器在有限时间内可以获得最高的收集效率。 无内存碎片 G1是基于标记-整理算法实现的收集器，它不会产生内存碎片，造成大对象无法分配内存的情况。 可预测的停顿 这是G1相对于其他收集器的最大优势，它可以非常精确的控制停顿，即指定M毫秒内，停顿时间不得超过N毫秒 G1运行时过程分为以下几步： 初始标记（Initial Marking）（STW） 仅仅只是标记一下GC Roots 能直接关联到的对象，并且修改TAMS（Nest Top Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可以的Region中创建对象，此阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking） 从GC Root 开始对堆中对象进行可达性分析，找到存活对象，此阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）（STW） 为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行 筛选回收（Live Data Counting and Evacuation） 首先对各个Region中的回收价值和成本进行排序，根据用户所期望的GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 几种收集器的比较 收集器 方式 内存区域 算法 特点 适用场景 Serial 串行 新生代 标记-复制 单线程 单核Client模式 ParNew 并行 新生代 标记-复制 多线程、可与CMS搭配 多核Server模式 Parallel Scavenge 并行 新生代 标记-复制 多线程、吞吐量优先 后台运算、交互少 Serial Old 串行 老年代 标记-整理 单线程、CMS的后备方案 单核Client Parallel Old 并行 老年代 标记-整理 多线程、吞吐量优先 后台运算、交互少 CMS 并发 老年代 标记-清除 多线程、并发收集 B/S服务端 G1 并发 全部 标记-整理、标记-复制 多线程、并发、停顿可预测 B/S服务端、替换CMS 内存分配Java的自动内存管理策略最终可以归结为自动化的解决两个问题： 如何给对象分配内存 如何回收分配给对象的内存 前面已经讨论了如何回收分配给对象的内存，现在一起研究如何给对象分配内存。所有的对象，都是在堆上分配内存（有些经过JIT编译后被拆分成变量类型并直接在栈上分配），分配内存主要有以下几种方式： 在新生代的Eden区分配 如果启动本地线程分配缓冲，优先在TLAB上分配 也可能直接分配在老年代种（大对象直接进入老年代） 这些分配方式取决于对象的大小、JVM的参数设置和选用哪一种垃圾收集器组合。 首先了解一下GC的几种方式 Minor GC 发生在新生代的GC，因为Java对象大多朝生夕死，所以Minor GC非常频繁。 Major GC 老年代GC，清理老年代区域的内存，可能伴随着Minor GC。 Full GC 清理整个堆空间—包括年轻代和老年代。 优先分配Eden区大多对象，都在新生代Eden区中分配，当Eden区内存不足时，将发起一次Minor GC。 以下代码为例，设置堆大小20MB，不可扩展，其中新生代10MB，老年代10MB，新生代中Eden取与Survivor的比例为8:1。尝试分配3个2MB的对象和1个3MB的对象，开启-XX:+PrintGCDetails观察输出结果。 123456789101112131415/** * vm:-verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:SurivivorRatio=8 */public class TestAlllocation &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] a1,a2,a3,a4; a1 = new byte[2 * _1MB]; a2 = new byte[2 * _1MB]; a3 = new byte[2 * _1MB]; a4 = new byte[3 * _1MB]; &#125;&#125; 输出结果： 1234567891011[GC [PSYoungGen: 7801K-&gt;608K(9216K)] 7801K-&gt;6752K(19456K), 0.0279404 secs] [Times: user=0.00 sys=0.00, real=0.04 secs] [Full GC [PSYoungGen: 608K-&gt;0K(9216K)] [ParOldGen: 6144K-&gt;6687K(10240K)] 6752K-&gt;6687K(19456K) [PSPermGen: 2197K-&gt;2196K(16384K)], 0.0544770 secs] [Times: user=0.02 sys=0.00, real=0.05 secs] Heap PSYoungGen total 9216K, used 3237K [0x0a2c0000, 0x0acc0000, 0x0acc0000) eden space 8192K, 39% used [0x0a2c0000,0x0a5e9720,0x0aac0000) from space 1024K, 0% used [0x0aac0000,0x0aac0000,0x0abc0000) to space 1024K, 0% used [0x0abc0000,0x0abc0000,0x0acc0000) ParOldGen total 10240K, used 6687K [0x098c0000, 0x0a2c0000, 0x0a2c0000) object space 10240K, 65% used [0x098c0000,0x09f47c50,0x0a2c0000) PSPermGen total 16384K, used 2201K [0x058c0000, 0x068c0000, 0x098c0000) object space 16384K, 13% used [0x058c0000,0x05ae6458,0x068c0000) 从上面可以看出，新生代的可用内存为9216K（Eden区内存8192K+1个Survivor区域内存1024K），a1、a2、a3的内存都分配在Eden区上，当分配a4时，剩余内存不足3MB，因此进行一次Minor GC：7801K-&gt;608K。GC之后内存总占用并没有减少，因为a1、a2、a3都还存活着，他们晋升进入了老年代，老年代占用6687K，此时Eden区有足够的空间分配a4，整个新生代占用3237K内存全部分配在Eden上。以上过程足以说明，对象分配内存时，优先分配在Eden区，Eden区内存不足时，发起一次Minor GC，存活的对象晋升，非存活的对象内存被回收。 大对象直接进入老年代大对象是指需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串及数组，比如上面代码中的byte[]数组。频繁出现大对象容易导致内存还有不少空间时提前触发GC以获取连续的内存空间来分配他们，在实际工作中，更要避免这种大量的”短命“大对象。 -XX:PretenureSizeThreshold参数可以设置阈值，如果对象大于这个阈值可以直接进入老年代进行内存分配，这么做的目的是为了避免Eden区和两个Survivor之前频繁发生内存拷贝。还是以上面的代码为例，我们将阈值设置为3MB，使得a4直接进入老年代分配内存。注意：此时要将收集器改为ParNew。 -XX:PretenureSizeThreshold参数只对Serial和ParNew有效，对Parallel Scavenge不起作用，Parallel Scavenge一般也不需要设置。如果遇到必须使用该参数的场景，可以考虑ParNew + CMS的收集器组合。 1234567891011121314151617/** * vm:-verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:SurivivorRatio=8 * -XX:PretenureSizeThreshold=3145728 * -XX:UseParNewGC */public class TestAlllocation &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] a1,a2,a3,a4; a1 = new byte[2 * _1MB]; a2 = new byte[2 * _1MB]; a3 = new byte[2 * _1MB]; a4 = new byte[3 * _1MB]; &#125;&#125; 运行结果如下： 12345678910Heap par new generation total 9216K, used 7791K [0x06200000, 0x06c00000, 0x06c00000) eden space 8192K, 95% used [0x06200000, 0x0699bd88, 0x06a00000) from space 1024K, 0% used [0x06a00000, 0x06a00000, 0x06b00000) to space 1024K, 0% used [0x06b00000, 0x06b00000, 0x06c00000) tenured generation total 10240K, used 3072K [0x06c00000, 0x07600000, 0x07600000) the space 10240K, 30% used [0x06c00000, 0x06f00010, 0x06f00200, 0x07600000) compacting perm gen total 16384K, used 2013K [0x07600000, 0x08600000, 0x0b600000) the space 16384K, 12% used [0x07600000, 0x077f74a0, 0x077f7600, 0x08600000)No shared spaces configured. GC收集器设置成ParNew，此时并没有发生Minor GC，老年代内存占用3027K，说明a4直接进入老年代分配了内存空间。 长期存活对象将进入老年代JVM既然采用了分代收集的思想来管理内存，那么回收时就必须识别哪些对象存活于新生代、哪些对象存活于老年代。为了达到此目的，JVM为每个对象都定义了一个对象年龄（Age）计数器。如果对象在Eden区经过一次Minor GC后依然存活，并且Survivor的内存空间足以分配给它，该对象就会被移到Survivor空间中，此时对象年龄为1。对象在Survivor中每经历一次Minor GC，年龄就+1，当它达到一定的年龄时（默认age=15），就会晋升到老年代。这个阈值可以由-XX:MaxTenuringThreshold来设置，-XX:+PrintTenuringDistribution输出对象的年龄。 以下代码为例，将进入老年代的阈值设置为1，观察内存空间的变化。 12345678910111213141516171819/** * vm:-verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:SurivivorRatio=8 * -XX:+UseSerialGC * -XX:MaxTenuringThreshold=1 * -XX:+PrintTenuringDistribution */public class TestAlllocation &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] a1, a2, a3, a4; a1 = new byte[_1MB / 4]; a2 = new byte[4 * _1MB]; a3 = new byte[4 * _1MB]; a3 = null; a3 = new byte[4 * _1MB]; &#125;&#125; 输出结果： 1234567891011121314151617[GC[DefNewDesired survivor size 524288 bytes, new threshold 1 (max 1)- age 1: 790560 bytes, 790560 total: 6009K-&gt;772K(9216K), 0.0093674 secs] 6009K-&gt;4868K(19456K), 0.0094384 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] [GC[DefNewDesired survivor size 524288 bytes, new threshold 1 (max 1)- age 1: 312 bytes, 312 total: 5038K-&gt;0K(9216K), 0.0041378 secs] 9134K-&gt;4865K(19456K), 0.0041810 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap def new generation total 9216K, used 4235K [0x06200000, 0x06c00000, 0x06c00000) eden space 8192K, 51% used [0x06200000, 0x06622d80, 0x06a00000) from space 1024K, 0% used [0x06a00000, 0x06a00138, 0x06b00000) to space 1024K, 0% used [0x06b00000, 0x06b00000, 0x06c00000) tenured generation total 10240K, used 4865K [0x06c00000, 0x07600000, 0x07600000) the space 10240K, 47% used [0x06c00000, 0x070c0660, 0x070c0800, 0x07600000) compacting perm gen total 16384K, used 2166K [0x07600000, 0x08600000, 0x0b600000) the space 16384K, 13% used [0x07600000, 0x0781d860, 0x0781da00, 0x08600000) 第一次GC发生时，新生代6009K-&gt;772K，总内存占用4868K，a1留在Survivor区，而Survivor空间不足以分配给a2，a2直接进入老年代，第二次GC发生时，新生代5054K-&gt;0K，a1从Survivor进入老年代，总内存几乎不变。 动态对象年龄判定为了更好地适应不同程序的内存情况，JVM并非总是要求对象年龄必须达到阈值才能晋升，如果在Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接晋升，无需达到阈值。 123456789101112131415161718192021/** * vm:-verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:SurivivorRatio=8 * -XX:+UseSerialGC * -XX:MaxTenuringThreshold=15 * -XX:+PrintTenuringDistribution */public class TestAlllocation &#123; private static final int _1MB = 1024 * 1024; public static void main(String[] args) &#123; byte[] a1, a2, a3,a4; a1 = new byte[_1MB / 4]; a2 = new byte[_1MB / 4]; a3 = new byte[4 * _1MB]; a4 = new byte[4 * _1MB]; a4 = null; a4 = new byte[4 * _1MB]; &#125;&#125; 输出结果： 1234567891011121314151617[GC[DefNewDesired survivor size 524288 bytes, new threshold 1 (max 15)- age 1: 1040736 bytes, 1040736 total: 6091K-&gt;1016K(9216K), 0.0080819 secs] 6091K-&gt;5112K(19456K), 0.0081490 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] [GC[DefNewDesired survivor size 524288 bytes, new threshold 15 (max 15)- age 1: 224 bytes, 224 total: 5280K-&gt;0K(9216K), 0.0035703 secs] 9376K-&gt;5110K(19456K), 0.0036212 secs] [Times: user=0.00 sys=0.02, real=0.00 secs] Heap def new generation total 9216K, used 4234K [0x05a00000, 0x06400000, 0x06400000) eden space 8192K, 51% used [0x05a00000, 0x05e22780, 0x06200000) from space 1024K, 0% used [0x06200000, 0x062000e0, 0x06300000) to space 1024K, 0% used [0x06300000, 0x06300000, 0x06400000) tenured generation total 10240K, used 5110K [0x06400000, 0x06e00000, 0x06e00000) the space 10240K, 49% used [0x06400000, 0x068fd820, 0x068fda00, 0x06e00000) compacting perm gen total 16384K, used 2003K [0x06e00000, 0x07e00000, 0x0ae00000) the space 16384K, 12% used [0x06e00000, 0x06ff4c48, 0x06ff4e00, 0x07e00000) 从结果分析，新生代的Survivor区内存占用变成0，而老年代的内存占用49%，说明a1、a2都直接晋升到了老年代，因为他们的内存占用512K，已经达到了Survivor的一半。 空间分配担保在发生Minor GC时，JVM会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间大小，如果大于，改为直接进行一次Full GC，如果小于，则查看HandlePromotionFailure设置是否允许担保失败，如果允许，那么只会进行Minor GC，如果不允许，则也会改为进行一次Full GC。 因为新生代使用复制算法，为了提高内存利用率，通常只使用其中一个Survivor空间来作为复制交换，如果出现所有对象在Minor GC之后都存活的极端情况，就需要老年代进行空间分配担保，让Survivor无法容纳的对象直接进入老年代。老年代要进行这样的担保，必须知道自身还有多少空间，能否容纳这些晋升的对象，所以只好取之前每一次GC后晋升到老年代对象容量的平均大小值作为经验值，与老年代的剩余空间进行比较，决定是否触发Full GC来让老年代腾出更多空间。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>垃圾收集器</tag>
        <tag>内存分配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC算法]]></title>
    <url>%2F2018%2F04%2F09%2FGC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[垃圾收集（Garbage Collection，下文简称GC）是JVM中非常重要的一个功能，JVM成熟的内存分配和回收机制使Java开发者无需像C++开发者那样进行手动分配和回收，更专注于具体功能上以提升开发效率。当然GC机制并非Java的产物，在Java之前，其他语言（1960 年诞生于MIT的Lisp是第一门真正使用内存动态分配和GC技术的语言）。理解GC，有助于当我们的应用发生内存溢出、内存泄漏问题时定位问题和解决问题。 上文的Java运行时数据区提到，程序计数器、虚拟机栈、本地方法栈三个区域都是线程私有的，栈帧中分配多少内存基本上在类结构确定时便是已知的，随着线程的结束，内存也跟着释放回收，不需要GC介入。而Java堆和方法区则不同，他们是线程共享的，对象创建、内存分配、内存回收都是动态进行的，也是GC所关注的内存区域。 关于GC，需要思考以下三件事： 那些内存需要回收？ 何时回收？ 如何回收？ 对象存活判定Java堆存放着几乎所有对象的实例，垃圾收集器在对堆进行回收前，首先要做的是确定这些对象中那些对象是“存活”的，那些对象已经“死去”（不可再使用）。 引用计数算法如今的JVM中已经不再通过引用计数法判定对象是否可以回收。引用计数算法是这么实现的：对象添加一个引用计数器，每当有一个地方引用它是，计数器+1；当引用失效时，计数器-1；任何时刻计数器为0的对象就是不可能再被使用。 从实现上看，引用计数器实现简单、判定效率高，它之所以被JVM弃用，因为有个严重的问题，很难解决循环引用。如以下例子： 12345678910111213141516171819202122232425public class ReferenceCountingGC &#123; public Object instance = null; private static final int _1MB = 1024 * 1024; private byte[] bigSize = new byte[2 * _1MB]; public static void testGC()&#123; ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; System.gc(); &#125; public static void main(String[] args) &#123; testGC(); &#125;&#125; 对象objA、objB互相赋值到instance属性上，除此之外，再无任何引用，实际上objA、objB都无法再被访问，但因为互相引用，导致引用计数器都不等于0，GC无法回收他们的内存，运行结果： 12[GC [PSYoungGen: 6138K-&gt;600K(19456K)] 6138K-&gt;600K(62976K), 0.0016832 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC [PSYoungGen: 600K-&gt;0K(19456K)] [ParOldGen: 0K-&gt;503K(43520K)] 600K-&gt;503K(62976K) [PSPermGen: 1991K-&gt;1990K(16384K)], 0.0175707 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] 但是从结果来看600K-&gt;0K，他们确实被回收，并没有因为互相引用而不回收，这也证明了JVM不采用引用计数算法来判定对象是否存活。 可达性分析算法在主流的JVM中，都是通过可达性分析算法（Reachability Analysis）来判定对象是否存活。该算法的基本思想是：通过一系列称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何的引用链时，即对象是不可达的。 上图中，对象object5、object6、object7虽然互有关联，但是它们到GC Roots是不可达的，所以会被判定为可回收对象。准确的说，GC Roots不是一组对象，而通常是一组特别管理的指向引用类型对象的指针，它们不会作为对象出现在上图，因此对象也不会引用到这些外部“指针”，避免了循环引用。因此得出，只有引用类型变量才被认为是GC Roots，值类型的变量永远不被认为是GC Roots。 Java中可以作为GC Roots的对象有一下几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中引用的对象 选择这些作为GC Roots的依据是什么？ 首先GC Roots的对象必须是存活的引用类型对象，而Java堆是GC的主要管理区域，虚拟机栈、方法区和本地方法栈不受GC管理，因此这些区域内引用的对象作为GC Roots，不会被GC回收。 几种引用关系不论是引用计数算法还是可达性分析算法，都和对象的引用有关系，在JDK1.2以前，Java的引用定义很传统：如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。在这种定义下，一个对象只可能有两种状态：被引用和不被引用。而实际上，还希望存在这一类状态的对象：内存足够时，可以存活在内存中，如果进行垃圾回收后还是内存不足，这些对象也可以被回收。因此JDK1.2之后，Java对引用的概念进行了扩充，将引用分为一下四种： 强引用（Strong Reference） 1Object obj = new Object(); 这类引用只要还存在，就不会被GC回收； 软引用（Soft Reference） 软引用的对象，进行GC时不会立即被回收，而是把这些对象列入回收范围进行第二次回收。如果第二次GC之后还是内存不足，则抛出内存溢出异常，可以通过SoftReference类来实现软引用 弱引用（Weak Reference） 弱引用的对象和软引用类似，但是它的强度比软引用更弱一下，弱引用对象只能活到第二次GC之前，当GC工作时，无论内存是否足够，都会回收对象，可以通过WeakReference类来实现弱引用。 虚引用（Phantom Reference） 它是四种引用中最弱的一种引用关系，虚引用不会对对象的存活产生任何影响，也无法通过虚引用来取得一个对象实例，唯一的目的就是能在这个对象被收集器回收时收到一个系统通知。可以通过PhantomReference来实现虚引用。 二次标记与拯救即使在可达性分析算法中不可达的对象，也并不是一定会被GC回收，首先它会被第一次标记和筛选，筛选的条件是此对象是否有必要执行finalize()方法。如果对象没有覆盖finalize()方法，或者对象的finalize()已经被JVM调用过，这两种情况都视为“没有必要执行”。如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放在一个叫做F-Queue的队列中，并在Finalizer线程中去执行它（JVM会触发finalize()方法，但不会等它执行结束，之所以这样，是因为finalize()一旦执行缓慢或者死循环，都会使F-Queue中的其他对象处于长时间等待中，甚至使GC崩溃）。finalize()是对象避免被回收的最后机会，稍后GC将对F-Queue中的对象进行第二次标记。如果对象要拯救自己避免被回收，只要在finalize()方法中重新建立与GC Roots引用链上任一对象的引用关系即可，比如把自己赋值给某个变量或对象的成员变量，GC第二次标记是会将它移除待回收的集合，以下是对象自我拯救的例子： 1234567891011121314151617181920212223242526272829303132333435363738394041public class FinalizeEscapeGC &#123; /** * 成员变量，拯救自己的钩子 */ public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() &#123; System.out.println("yes,i am still alive :)"); &#125; @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println("finalize method executed"); //把“自己”赋值给成员变量，重新让GC Roots可达 FinalizeEscapeGC.SAVE_HOOK = this; &#125; public static void main(String[] args) throws InterruptedException &#123; SAVE_HOOK = new FinalizeEscapeGC(); SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125;else&#123; System.out.println("1st no,i am dead :("); &#125; //下面的代码与上面相同，但是拯救失败了 SAVE_HOOK = null; System.gc(); Thread.sleep(500); if (SAVE_HOOK != null) &#123; SAVE_HOOK.isAlive(); &#125;else&#123; System.out.println("2nd no,i am dead :("); &#125; &#125;&#125; 123finalize method executedyes,i am still alive :)2nd no,i am dead :( 第一次GC时，执行了finalize()方法，将this赋值给成员变量，成功拯救了自己，但是finalize()只会执行一次，第二次再尝试拯救自己时，失败了 回收方法区很多人认为方法区不受GC管理，Java规范中确实规定不要求方法区实现GC，而且在方法区中进行垃圾收集的“性价比”一般比较低：在Java堆，尤其是新生代，一次GC可以回收70%~95%的空间，而永久代的GC收集效率远低于此。 永久代的GC主要回收两部分内从：废除常量和无用的类。 废弃常量 以常量池回收为例，假如字符串”abc”已经进入了常量池中，但是当前系统没有任何一个String对象是叫做”abc”的，也就是说没有任何String对象引用常量池的”abc”常量，也没有任何地方引用了这个字面量，如果此时发生GC，且必要的情况下，”abc”就会被清理出常量池。其他类、方法、字段的符号引用也类似。 无用的类 与废弃常量相比，判定一个类是否是”无用的类“，条件相对苛刻，必须同时满足以下三个条件才能算是”无用的类“ 该类的所有实例都已经被回收，Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 如果满足以上三个条件，JVM可以对该类进行回收，这里说的仅仅是“可以”回收，而不是和对象一样，必须回收。HotSpot提供了-Xnoclassgc参数进行控制。 JDK1.8开始，JVM用MateSpace作为方法区的实现，MateSpace的内存管理由元空间虚拟机来完成，在MateSpace中，类和其元数据的生命周期和其对应的类加载器是相同的，只要类加载器存活，其加载的类的元数据也是存活的，不会被GC回收。 GC算法本节介绍几种GC算法，具体的实现在后续的文章中分析。 标记-清除算法最基础的收集算法是“标记-清除”（Mark-Sweep）算法，顾名思义，该算法分为“标记”和“清除”两个阶段：首先标记出所有要回收的对象，在标记完成后统一回收所有被标记的对象，标记的过程在上一节已经介绍过。标记-清除算法的执行过程如下图： 标记-清除算法是最基础的算法，后续的GC算法都是基于它的思路并对其不足进行改进而来的。它的不足之处主要体现在： 空间碎片 标记清除后会产生大量不连续的内存空间，碎片太多会导致以后程序在运行过程中分配大对象时，无法找到足够的连续内存而不得不触发另一次GC。 效率不高 因为存在内存碎片，查找下一个可用空闲内存不是一个简单操作，变得更耗时。 复制算法为了解决标记-清除算法的效率问题，复制（Copying）算法出现了，它的算法思想是：将可用内存分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，就将存活的对象复制到另一块上，然后再把已使用过的内存空间一次清理掉。这样每次都对整个半区进行内存回收，内存分配时也不用考虑内存碎片等复杂情况，只要移动栈顶指针，按顺序分配内存即可，实现简单，运行高效。复制算法的执行过程如下图： 复制算法的代价是可用内存为原来的一半，有点太高了。现在的商业虚拟机都采用复制算法来回收新生代。新生代的GC又叫做Minor GC，新生代中的对象98%是“朝生夕死”的，所以Minor GC很频繁，需要很高的回收速度避免性能瓶颈，同时新生代大量的对象生命周期短，也不需要按照1:1来划分内存空间。 Minor GC过程事实上，新生代将内存划分为三个区域，Eden、From Survivor（S0）和To Survivor（S1），三个区域的比例分为为8:1:1。当Minor GC发生时： Eden + S0 的存活对象复制到S1上，清理Eden + S0的空间； S0和S1互换标签，原来的S1变成了S0，S0变成S1，第一次GC结束； Eden + S1的存活对象赋值到S0，清理Eden+S1的空间； S0和S1互换标签，第二次GC结束； Minor GC结束的时候，Eden是空的，S0、S1有一个是空的，而另一个保存着存活对象。所以每次新生代中可用内存为整个新生代容量的90%，只有10%的空间会被浪费。 分配担保上文提到98%的对象“朝生夕死”只是一般场景下的数据，无法保证每次GC都只有不多于10%的对象存活，当Survivor空间不足时，就要依赖老年代内存进行分配担保（Handle Promotion）。如果另一块Survivor空间没有足够空间存放上一次Minor GC之后存活的对象时，这些对象将直接通过分配担保机制进入老年代。 标记整理算法复制收集算法在对象存活率较高时就要进行较多的复制操作，效率很低，极端情况下，所有的对象都存活，如果不想浪费50%的空间，就需要额外的空间进行分配担保，在老年代一般不能直接选用复制算法。 老年代的特点是对象存活率高，因此提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但接下来不是对可回收对象进行清理，而是让活着的对象都向一端移动，然后清理掉边界以外的内存，如下图所示： 分代收集算法当前商业JVM的垃圾回收都采用“分代收集”（Generational Collection）算法，分代收集并不是什么新思想，只是根据对象存活周期的不同将内存划分为几块。通常划分为新生代、老年代。这样就可以根据各个年代的特点采用最适当的收集算法。 新生代特点：对象存活时间短，每次GC都有大量的对象被回收，只有少量存活，选用复制算法，只需付出少量存活对象的复制成本完成内存回收。 老年代特点：对象存活率高、没有额外空间进行分配担保，必须使用“标记-清理”或者“标记-整理”算法来进行回收。 HotSpot的算法实现前文介绍了对象存活判定算法和垃圾收集算法，而在HotSpot虚拟机上实现这些算法，必须对算法的执行效率有严格的考量，才能保证虚拟机高效运行。 枚举根节点从可达性分析中从GC Roots节点找引用链这个操作为例，可作为GC Roots的节点主要在全局性的引用（例如常量或类静态属性）与执行上下文（例如栈帧中的局部变量表）中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，那么必然会消耗很多时间。 GC停顿可达性分析必须工作在一个能确保一致性的快照中进行——这里“一致性”的意思是指在整个分析期间，系统就像停顿在一个时间点上，不能出现分析过程中对象引用关系还在不断变化的情况，否则分析结果的准确性就无法得到保证。这是导致GC时必须停顿所有线程的主要原因，Sun将这一停顿称作“Stop The World”。即使在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须停顿的。 OopMap目前主流的JVM都采用准确式GC（即使用准确式内存管理，JVM可以知道内存中某个位置的数据具体是什么类型，另一种GC方式，是保守式GC），所以当执行系统停顿下来后，并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得知哪些地方存放着对象引用。在HotSpot实现中，是使用一组称为OopMap的数据结构来达到此目的。在类加载完成时，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在特定的位置记录下栈和寄存器中的哪些位置是引用的。这样，GC扫描时就可以直接得知这些信息了。 安全点OopMap可以协助HotSpot快速准确的完成GC Roots枚举，但一个很现实的问题：可能导致引用关系变化，或者说OopMap内容变化的指令非常多，如果每条指令都生成OopMap，非常浪费空间，GC成本也会随之升高。 Hotspot确实没有为每条指令生成OopMap，只是在“特定的位置”记录了这些信息，这些位置称为安全点（Safepoint）,线程执行时并非随时停下开始GC，而是达到安全点时才能暂停。关于安全点，就是如何要让线程在GC发生时都跑到最近的安全点上再停顿，通常有两种方案： 抢先式中断（Preemptive Suspension），不需要线程主动配合，在GC发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让线程继续跑到安全点上。现在几乎没有虚拟机采用这种方式来中断线程响应GC。 主动式中断（Voluntary Suspension），当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标识，各个线程执行时主动去轮询这个标识，发现中断标识为真时就轮询挂起，中断标识一般和安全点是重合的。 安全点该如何选择呢？太多，GC过于频繁，太少，GC等待时间太长。一般会在以下几个位置选择安全点： 循环的结束位置； 方法返回前； 调用方法之后； 抛异常的位置； 选择以上位置作为安全点的原因在于，避免线程长时间无法跑到安全点，如果GC时有一个线程未到达安全点，就会导致GC停顿时间延长。 安全区域安全点解决了线程在运行时如何进入GC的问题，但是线程可能处于Sleep或者Blocked状态，此时无法响应JVM的中断请求，跑到安全点附近，JVM也不可能等待线程被重新分配CPU时间，基于以上情况，就需要安全区域（Safe Region）来解决。 安全区域是指在一段代码片段中，引用关系不会发生变化，也就不用更新OopMap表，在这个区域中的任意地方开始GC都是安全的，可以把安全区域看做是被扩展了的安全点。线程执行过程中，如果进入到安全区域内，就会标识自己已经进入了安全区域，JVM要发起GC时，就不会管这个线程了。一旦线程要离开安全区域时，它要检查系统是否已经完成了根节点枚举（或者是整个GC过程），如果已完成，线程继续执行，否则只能等待可以安全离开安全区域的信号为止。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM之入门]]></title>
    <url>%2F2018%2F04%2F06%2FJVM%E4%B9%8B%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[JVM系列博文，是拜读周志明老师的《深入理解Java虚拟机——JVM高级特性与最佳实践》并结合Oracle官网《Java Virtual Machine Specification》和一些网络博客后，以及自己的理解，作为读书笔记性质记录。 引言JVM（Java Virtual Machine）是Java平台的基础，它基于硬件和操作系统之上，能运行Java字节码的虚拟机。JVM拥有完善的架构体系，如寄存器、堆栈和计数器，还有自己的指令系统，它屏蔽了不同操作系统之间的差异性，使Java程序能够跨平台运行，实现Write Once,Run Anywhere。比较流行的JVM实现为sun公司开发的Hotspot（现已经被Oracle收购）、Oracle的JRockit和IBM的J9。本文和后续文章都基于JDK1.8的Hotspot虚拟机。 这是查看JDK版本，或者验证Java环境变量是否生效的常用命令，一直以来，我只关心版本号，也只停留在第一行输出的内容上。随着对java的深入，我开始关注第三行输出的内容：JVM类型和模式。 Java HotSpot 是虚拟机的名称 Client 是虚拟机的类型，与之对应的是Server类型 build 25.65-b01 是版本号 mixed mode 是虚拟机的工作模式 JVM类型Client和Server为什么虚拟机有Client和Server之分？先来看看官方说明 What’s the difference between the -client and -server systems? These two systems are different binaries. They are essentially two different compilers (JITs)interfacing to the same runtime system. The client system is optimal for applications which need fast startup times or small footprints, the server system is optimal for applications where the overall performance is most important. In general the client system is better suited for interactive applications such as GUIs. Some of the other differences include the compilation policy,heap defaults, and inlining policy. Server JVM和Client JVM分别使用不同的编译器，Client（也叫C1编译器）适用于需要快速启动且内存占用较小的应用，比如GUI等交互式应用，而Server（也叫C2编译器）启动慢，启动后性能更强，适合整体执行效率高的应用比如后台程序，除此之外，编译策略、默认堆大小和内部的策略也存在差异。根据《Java Platform, Standard Edition HotSpot Virtual Machine Garbage Collection Tuning Guide》，看看默认时和最大时堆的大小在两者间的区别 Default Heap SizeUnless the initial and maximum heap sizes are specified on the command line, they are calculated based on the amount of memory on the machine. Client JVM Default Initial and Maximum Heap SizesThe default maximum heap size is half of the physical memory up to a physical memory size of 192 megabytes (MB) and otherwise one fourth of the physical memory up to a physical memory size of 1 gigabyte (GB). For example, if your computer has 128 MB of physical memory, then the maximum heap size is 64 MB, and greater than or equal to 1 GB of physical memory results in a maximum heap size of 256 MB. The maximum heap size is not actually used by the JVM unless your program creates enough objects to require it. A much smaller amount, called the initial heap size, is allocated during JVM initialization. This amount is at least 8 MB and otherwise 1/64th of physical memory up to a physical memory size of 1 GB. The maximum amount of space allocated to the young generation is one third of the total heap size. Server JVM Default Initial and Maximum Heap SizesThe default initial and maximum heap sizes work similarly on the server JVM as it does on the client JVM, except that the default values can go higher. On 32-bit JVMs, the default maximum heap size can be up to 1 GB if there is 4 GB or more of physical memory. On 64-bit JVMs, the default maximum heap size can be up to 32 GB if there is 128 GB or more of physical memory. You can always set a higher or lower initial and maximum heap by specifying those values directly; see the next section. Client JVM在物理内存不超过192MB时，默认堆内存大小为物理内存的一半（物理内存=128MB时，堆内存大小为64MB），当物理内存大于1GB时，堆内存大小最大不超过256MB。当然，JVM初始化时并不会根据物理内存直接分配最大堆内存，而是先分配一个初始化值，至少8M或者物理内存的1/64（物理内存超过1GB时），此外，新生代的最大内存占堆大小的1/3. Server JVM的堆内存默认大小比Client更高，在32位JVM上，如果物理内存&gt;=4GB，则默认的最大堆内存可达1GB。在64位JVM上，如果物理内存达到128G，默认堆内存大小最大为32GB，这些默认值和最大值都可以通过启动参数来指定。 除了默认堆内存大小的区别外，Server JVM会对编译后的代码进行优化。 12345678910public class Test &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); for (int i = 100000000; i &gt; 0; i--) &#123; &#125; long end = System.currentTimeMillis(); System.out.println(end - start); &#125;&#125; 循环1亿次，分别在Server和Client模式下执行（32位JDK）。 执行结果来看，Server的执行耗时明显少于Client，相差几倍。以上代码和命令是运行在32位JVM上，如果在64位JVM上运行呢？ Server和Client的执行耗时并没有明显的差别，究其原因，java命令在64位虚拟机上已经忽略了-client参数且不提供Client类型的jvm，也就是说无法通过该命令以client jvm运行字节码（官方文档） Standard Options -client Selects the Java HotSpot Client VM. A 64-bit capable JDK currently ignores this option and instead uses the Java Hotspot Server VM. For default Java VM selection, see the Server-Class Machine Detection page athttp://docs.oracle.com/javase/7/docs/technotes/guides/vm/server-class.html -server Selects the Java HotSpot Server VM. On a 64-bit capable JDK, only the Java Hotspot Server VM is supported so the -`server` option is implicit. For default a Java VM selection, see the Server-Class Machine Detection page athttp://docs.oracle.com/javase/7/docs/technotes/guides/vm/server-class.html 类型切换JVM是如何决定以哪种类型启动的呢？以下内容来自官方文档：Server-Class Machine Detection Server-Class Machine DetectionStarting with J2SE 5.0, when an application starts up, the launcher can attempt to detect whether the application is running on a “server-class” machine and, if so, use the Java HotSpot Server Virtual Machine (server VM) instead of the Java HotSpot Client Virtual Machine (client VM). The aim is to improve performance even if no one configures the VM to reflect the application it’s running. In general, the server VM starts up more slowly than the client VM, but over time runs more quickly. Note:For Java SE 6, the definition of a server-class machine is one with at least 2 CPUs and at least 2GB of physical memory. In Java SE 6, server-class detection occurs if neither -server nor -client is specified when launching the application on an i586 or Sparc 32-bit machine running Solaris or Linux. As the following table shows, the i586 Microsoft Windows platform uses the client VM by default. The remaining Sun-supported platforms use only the server VM. ​ （各平台的默认JVM类型，—表示不提供该类型的jvm） 从Jave SE 5开始，当一个应用启动的时候，加载器会尝试去检测应用是否运行在 “server-class” 的机器上，如果是，则使用Java HotSpot Server Virtual Machine (server VM)而不是 Java HotSpot Client Virtual Machine (client VM)。这样做的目的是提高执行效率，即使没有为应用显式配置VM。特别强调：从Java SE 6开始， server-class机器的定义是至少有2个CPU和至少2GB的物理内存。 有两种方式可以切换jvm类型，第一种是上面提到过的命令（command-line options），第二种就要修改jvm.cfg配置文件。 32位JDK的配置文件目录：%JAVA_HOME%\jre\lib\i386\jvm.cfg 1234567891011# List of JVMs that can be used as an option to java, javac, etc.# Order is important -- first in this list is the default JVM.# NOTE that this both this file and its format are UNSUPPORTED and# WILL GO AWAY in a future release.## You may also select a JVM in an arbitrary location with the# &quot;-XXaltjvm=&lt;jvm_dir&gt;&quot; option, but that too is unsupported# and may not be available in a future release.#-client KNOWN-server KNOWN 64位JDK下的配置文件目录：%JAVA_HOME%\jre\lib\amd64\jvm.cfg 1234567891011# List of JVMs that can be used as an option to java, javac, etc.# Order is important -- first in this list is the default JVM.# NOTE that this both this file and its format are UNSUPPORTED and# WILL GO AWAY in a future release.## You may also select a JVM in an arbitrary location with the# &quot;-XXaltjvm=&lt;jvm_dir&gt;&quot; option, but that too is unsupported# and may not be available in a future release.#-server KNOWN-client IGNORE 32位JDK只要调整配置顺序即可切换，64位JDK默认不提供client模式，即使切换了，也会提示错误 JVM工作模式命令行输入 1java -X 看到以下结果 这是其中两种JVM的工作模式，除此之外还有-Xcomp模式，先看官方如何解释这三种模式 -Xcomp Forces compilation of methods on first invocation. By default, the Client VM (-client) performs 1,000 interpreted method invocations and the Server VM (-server) performs 10,000 interpreted method invocations to gather information for efficient compilation. Specifying the -Xcomp option disables interpreted method invocations to increase compilation performance at the expense of efficiency. You can also change the number of interpreted method invocations before compilation using the -XX:CompileThreshold option. -Xint Runs the application in interpreted-only mode. Compilation to native code is disabled, and all bytecode is executed by the interpreter. The performance benefits offered by the just in time (JIT) compiler are not present in this mode. -Xmixed Executes all bytecode by the interpreter except for hot methods, which are compiled to native code. -Xint：解释模式（interpreted mode），强制JVM以解释方式执行字节码，禁用编译器。 -Xcomp：编译模式（compiled mode），强制JVM在第一次使用时将所有字节码编译成本地代码，性能高于解释模式。 -Xmixed：混合模式（mixed mode），它同时使用编译模式和解释模式，JVM会对字节码进行分析，将频繁调用的部分编译成本地代码，以提高执行效率；很少调用甚至几乎没有调用的方法以解释模式执行，减少编译和优化的成本。 早期的JVM是通过解释器执行代码，当它发现某些方法或者代码块运行频繁时，就会把这些代码标记为“热点代码（Hot Spot Code）”（HotSpot名称也是由此而来）。后来为了提高热点代码的执行效率，在运行时，虚拟机会把这些代码编译成本地平台相关的机器码进行优化，负责这个编译工作的编译器为即时编译器，即JIT（Just In Time Complier）编译器。 还是以1亿次循环为例，比较三种模式的运行效率。 -Xint效率最差，-Xcomp和-Xmixed几乎没有差别。 运行时数据区 JVM在执行Java程序的过程中把它所管理的内存划分为若干个不同的数据区域，如上图。 程序计数器（Program Counter Register）程序计数器时一块较小的内存空间，可以看作是当前线程所执行字节码的行号指示器。在JVM概念模型里，字节码指示器工作时就是通过改变这个计数器值来选取一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都要依赖计数器完成。 JVM的多线程是通过线程切换并分配处理器时间的方式来实现，任何时刻一个处理器都只会执行一条线程中的指令。因此，为了线程恢复后能恢复到正确的执行位置，每条线程都需要有独立的计数器，线程间互不影响，独立存储，这类内存称为“线程私有”的内存。 线程执行的是Java方法，计数器记录的是JVM的字节码指令地址；如果执行的是Native方法，计数器值为空。此内存区域是唯一一个在JVM规范中没有定义OutOfMemoryError情况的区域。 虚拟机栈（Java Virtual Machine Stacks）虚拟机栈也是线程私有的，其生命周期与线程相同，线程结束内存释放，因此虚拟机栈并不存在垃圾回收的问题。虚拟机栈描述的是Java方法执行的内存模型：每个方法执行时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息，方法的调用其实就是出栈和入栈的过程。 虚拟机栈存储哪些数据？原文描述 局部变量（Local Variables）编译时能确定的数据； 8种基本数据类型、对象的引用、实例方法和方法返回地址（returnAddress）； 方法入参、出参和临时变量； 栈操作（Operand Stacks）记录入栈、出栈操作； 以下代码为例描述出入栈过程。 1234567891011public static void main(String[] args) &#123; methodA();&#125;public static void methodA()&#123; methodB();&#125;public static void methodB()&#123; &#125; 虚拟机栈可能出现的两种异常： 如果线程请求的栈深度大于允许的深度，会抛出StackOverflowError； 如果虚拟机栈扩展时内存不足，就会抛出OutOfMemoryError。 本地方法栈（Native Method Stack）与虚拟机栈相似，只不过虚拟机栈执行的是Java方法，本地方法栈执行的是native方法，也可能抛出StackOverflowError、OutOfMemoryError。由于HotSpot虚拟机中并不区分虚拟机栈和本地方法栈，因此-Xoss参数（设置本地方法栈大小）虽然存在，但实际是无效的，栈容量只能由-Xss参数设置，以下是栈溢出的示例代码。 12345678910111213141516171819202122/** * -Xss128k */public class StackOF &#123; private int stackLength = 1; public void stackLeack() &#123; stackLength++; stackLeack(); &#125; public static void main(String[] args) &#123; StackOF oom = new StackOF(); try &#123; oom.stackLeack(); &#125; catch (Throwable e) &#123; System.out.println("stack length:" + oom.stackLength); throw e; &#125; &#125;&#125; 运行结果： 123456stack length:2103Exception in thread &quot;main&quot; java.lang.StackOverflowError at com.cjluo.StackOF.stackLeack(StackOF.java:11) at com.cjluo.StackOF.stackLeack(StackOF.java:12) at com.cjluo.StackOF.stackLeack(StackOF.java:12) ……省略 以上结果说明栈深度超过JVM允许的最大深度，抛出StackOverflowError，那么要如何抛出OutOfMemoryError？可以通过不断创建新线程，把内存耗尽，但是这种方式产生的内存溢出与栈空间是否足够大并没有任何关系。 堆（Heap）堆是所有线程共享的区域，它在启动时创建，此区域的唯一目的就是存放对象的实例，几乎所有的对象实例都在这个区域分配内存。为什么说“几乎”？因为随着JIT编译器的发展和逃逸技术分析成熟，栈上分配、标量替换优化技术会产生一些微妙的变化，所有对象都分配在堆上也就不完全绝对了。一个JVM实例只能存在一个堆内存，它的大小可以调节，并且在规定的范围内可以自动伸缩。 堆上对象分配的内存，会被回收，负责这个功能的叫做垃圾回收器（garbage collector），堆是垃圾回收器的主要区域，因此也叫GC堆（Garbage Collector Heap），从GC回收的角度看，堆在逻辑上可以划分为以下区域：新生代、老年代和永久区（JDK1.8开始已经没有永久区，取而代之的是MateSpace）。再具体一点，新生代又划分为Eden区、Form Survivor区、To Survivor区。 从内存分配的角度看，线程共享的Java堆可以划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB） 因为堆的大小在一定范围内是可扩展的，如果堆中没有足够的内存可以分配，将抛出OutOfMemoryError异常。以下是代码示例 1234567891011121314151617/*** -verbose:gc -Xms20M -Xmx20M -Xmn10M -XX:+PrintGCDetails -XX:SurvivorRatio=8**/public class HeapOOM &#123; static class OOMObject&#123; int _1M = 1024 * 1024; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;&gt;(); while (true)&#123; list.add(new OOMObject()); &#125; &#125;&#125; 12345678910111213[GC [PSYoungGen: 7909K-&gt;1016K(9216K)] 7909K-&gt;5294K(19456K), 0.0324974 secs] [Times: user=0.03 sys=0.02, real=0.03 secs] [GC-- [PSYoungGen: 9208K-&gt;9208K(9216K)] 13486K-&gt;19444K(19456K), 0.0283762 secs] [Times: user=0.09 sys=0.00, real=0.03 secs] [Full GC [PSYoungGen: 9208K-&gt;0K(9216K)] [ParOldGen: 10236K-&gt;10071K(10240K)] 19444K-&gt;10071K(19456K) [PSPermGen: 2121K-&gt;2120K(16384K)], 0.6551317 secs] [Times: user=0.41 sys=0.00, real=0.65 secs] [Full GC [PSYoungGen: 7821K-&gt;8132K(9216K)] [ParOldGen: 10071K-&gt;8209K(10240K)] 17893K-&gt;16342K(19456K) [PSPermGen: 2171K-&gt;2171K(16384K)], 0.2723715 secs] [Times: user=0.41 sys=0.00, real=0.27 secs] [Full GC [PSYoungGen: 8132K-&gt;8122K(9216K)] [ParOldGen: 8209K-&gt;8204K(10240K)] 16342K-&gt;16327K(19456K) [PSPermGen: 2171K-&gt;2171K(16384K)], 0.2054207 secs] [Times: user=0.27 sys=0.00, real=0.21 secs] Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:2245) at java.util.Arrays.copyOf(Arrays.java:2219) at java.util.ArrayList.grow(ArrayList.java:242) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208) at java.util.ArrayList.add(ArrayList.java:440) at com.cjluo.HeapOOM.main(HeapOOM.java:19) 方法区（Method Area）方法区与堆一样，都是各线程共享的内存区域，它存储JVM加载的元数据，例如Class、Interface、常量、静态变量、方法和类的构造函数等。即该区域存储的包含一些JVM运行的必须信息，尽管JVM虚拟机规定方法区在逻辑上属于堆的一部分，但实际上JVM启动后装载的数据不会被GC回收，直到JVM退出或关闭才会回收，他也有个别名叫做非堆（No-Heap）。如果出现java.lang.OutOfMemoryError:PermGen space，说明是JVM的永久代Perm内存设置不够，常出现于以下几种情况： 应用加载很多第三方jar； Tomcat部署了太多的应用； 大量通过反射生成的类不断被加载导致内存不足； 大量的JSP动态编译。 在HotSpot虚拟机上，方法区又称作永久代，GC分代收集扩展至方法区，因此GC可以和堆一样管理方法区的内存，但是对于其他虚拟机的实现，是不存在永久代的概念（这也符合虚拟机规范，因为并没有具体定义如何实现方法区，也没有要求一定要实现内存回收）。因此JDK7之后的HotSpot中，原本在方法区的字符串常量池被移除，直到JDK8的发布，方法区的实现用MateSpace取代了永久代。以下是几个JDK版本方法区的区别。 JDK6及之前，方法区以永久代实现，常量池在方法区中； JDK7，方法区以永久代实现，但常量池已经移到堆中； JDK8及之后，方法区以MateSpace实现，常量池在MateSpace中 运行时常量池（Run-Time Constant Pool）运行时常量池属于方法区的一部分，Class文件除了有类的版本、字段、方法、接口等描述信息外，还有一项常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存储。 Java并没有规定常量必须在编译期产生，运行期间也可能将新的常量放入池中，例如String的intern()方法。运行时常量池也受方法区内存的限制，也有可能抛出OutOfMemoryError。 HotSpot中的对象对象的创建有四种方式创建对象： new关键字，创建对象最常用的方式； newInstance()方法，两个地方可以调用newInstance()方法，分别是Class类和Constructor类； clone，常说的浅拷贝方式； 反序列化，常说的深拷贝方式。 以new关键字创建对象为例，对象是如何被创建的。 类加载检查jvm在执行一条new指令时，首先去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过，如果没有，就必须先执行类加载过程（后续的文章详述）。 分配内存类加载检查通过后，对象所需的内存大小可以确定（如何确定见内存布局）可以开始分配内存，分配内存即从堆中划分出一块大小确定的空间，此时又分为两种方式： 堆内存空间绝对规整，已使用内存/空闲内存各占一边，中间以指针指示器作为分界，分配时将指针往空闲内存移动对象大小相等的距离，这种方式叫做指针碰撞（Bump The Pointer）； 堆内存空间不规整，已使用内存和空先内存相互交错，分布不均，指针碰撞无法进行，jvm只能维护一个列表，记录哪些是空闲内存，在分配的时候将大小足够的空闲内存分配给对象实例，同时更新列表记录，这种方式叫做空闲列表（Free List） 哪种方式分配内存取决于内存是否规整，内存是否规整又取决于采用哪种GC算法，带有压缩整理功能的GC回收算法，因此使用Serial、ParNew等带Compact过程的收集器时，分配内存采用指针碰撞的方式，而基于标记-清除算法的CMS收集器，通常采用空闲列表方式。 除了可用空间大小外，还有一个需要考虑的问题就是线程安全，并发情况下，有可能出现对象A的内存尚未分配完成，对象B在原指针的位置也进行了内存分配，此问题该如何解决？ 对分配内存的动作进行同步，jvm会采用CAS加上失败重试的机制保证内存分配的原子性； 按照线程将分配内存的动作在不同的空间之中进行，每个线程在堆中先划分一小块内存，即TLAB，哪个线程要分配内存，就在哪个线程的TLAB上进行分配，只有TLAB不足并且需要分配新的TLAB时，才需要同步锁定。是否使用TLAB，可以通过-XX:+/-UseTLAB参数来设置。 初始化分配内存结束，JVM需要对已分配内存空间的实例进行初始化零值（不含对象头），如果分配时使用的是TLAB，则初始化的工作可以提前到TLAB分配时进行。这一步操作保证了对象的实例字段在Java代码中不需要赋初始值就可以直接使用，程序能访问到这些字段的数据类型对应的零值。 设置对象头接着，JVM要对对象进行必要的设置，例如对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等。这些信息都存储在对象头（Object Header）之中。根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对象头有不同的设置方式，具体会在内存布局中详述。 执行init方法以上工作都完成后，JVM的视角看来，已经创建了新的对象，但从Java程序的视角来看，对象必须执行init方法，将为零值的字段进行初始化。一般来说（由字节码中是否跟随invokespecial指令所决定），执行new指令之后会接着执行init方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存布局HotSpot虚拟机中，对象在内存中存储的布局可以分为三块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 Header 自身运行时的数据（Mark Word），32位/64位虚拟机对应的长度分别为32bit/64bit。 ​ 哈希码（hashCode） ​ GC分代年龄 ​ 锁状态标志 ​ 线程持有锁 ​ 偏向线程ID ​ 偏向时间戳 类型指针 数组长度（只有数组对象才有） Instance Data 相同宽度的数据分配到一起（long/double） Padding 非必然存在，起着占位符的作用，JVM要求对象的大小必须是8字节的整数倍，当实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位 句柄访问 reference中存储的是稳定的句柄地址，对象被移动时只会改变句柄中实例数据指针，不影响reference 直接指针访问 对比句柄访问，直接指针访问方式最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问非常频繁，因此这类开销积少成多后也是一项非常大的耗时。HotSpot虚拟机采用的就是这种方式来访问对象，Java堆中会存放类元数据的地址，reference存储的就是对象的地址；]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper客户端-java api简单使用]]></title>
    <url>%2F2018%2F03%2F23%2Fzookeeper%E5%AE%A2%E6%88%B7%E7%AB%AF-java-api%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[参照zookeeper的官方文档，简单学习如何通过java api操作zookeeper。 引入依赖包12345&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.8&lt;/version&gt;&lt;/dependency&gt; 客户端实例创建123456789101112131415161718192021222324252627282930public class ZookeeperInstance &#123; //zk集群的IP:prot，用逗号分隔 private static final String CONNECT_STRING = "192.168.96.129:2181,192.168.96.130:2181," + "192.168.96.131:2181,192.168.96.132:2181"; //计数器 private static CountDownLatch latch = new CountDownLatch(1); public static ZooKeeper getInstance() &#123; ZooKeeper zooKeeper = null; try &#123; //传入集群ip:port配置，超时时间，监听事件 zooKeeper = new ZooKeeper(CONNECT_STRING, 5000, watchedEvent -&gt; &#123; //连接成功时的事件状态为SyncConnected if (watchedEvent.getState() == Watcher.Event.KeeperState.SyncConnected) &#123; System.out.println("zk状态:" + watchedEvent.getState()); latch.countDown(); &#125; &#125;); //等待客户端链接成功 latch.await(); System.out.println("zk启动成功"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return zooKeeper; &#125;&#125; 客户端的几种连接状态： 123456Disconnected(0),//未连接SyncConnected(3),//已连接AuthFailed(4),//认证失败ConnectedReadOnly(5),//只读连接SaslAuthenticated(6),//已认证Expired(-112);//已过期 Znode的增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class Operator &#123; private ZooKeeper zooKeeper; private OperatorListener listener = new OperatorListener(); private List&lt;ACL&gt; acls; private Stat stat = new Stat(); public Operator() &#123; this.zooKeeper = ZookeeperInstance.getInstance(); //权限初始化 this.acls = new ArrayList&lt;&gt;(); ACL acl = new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone")); acls.add(acl); &#125; /** * 创建节点 * @param path 节点路径 * @param value 节点值 * @throws KeeperException * @throws InterruptedException */ private void createZnode(String path,String value) throws KeeperException, InterruptedException &#123; zooKeeper.create(path, value.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); TimeUnit.SECONDS.sleep(1); &#125; /** * 删除节点 * @param path 节点路径 * @throws KeeperException * @throws InterruptedException */ private void deleteZnode(String path) throws KeeperException, InterruptedException &#123; zooKeeper.delete(path, -1); &#125; /** * 更新节点 * @param path 节点路径 * @param value 节点值 * @throws KeeperException * @throws InterruptedException */ private void updateZnode(String path, String value) throws KeeperException, InterruptedException &#123; zooKeeper.setData(path, value.getBytes(), -1); &#125; /** * 获取节点 * @param path 节点路径 * @throws KeeperException * @throws InterruptedException */ private void getNodeData(String path) throws KeeperException, InterruptedException &#123; byte[] bytes = zooKeeper.getData(path, listener, stat); System.out.printf("节点:%s,数据:%s\n", path, new String(bytes)); &#125; /** * 节点是否存在 * @param path 节点路径 * @throws KeeperException * @throws InterruptedException */ private void exists(String path) throws KeeperException, InterruptedException &#123; Stat s = zooKeeper.exists(path, listener); System.out.println(null == s ? "节点" + path + "不存在\n" : "节点" + path + "存在\n"); &#125; /** * 获取子节点 * @param path 节点路径 * @throws KeeperException * @throws InterruptedException */ private void getChild(String path) throws KeeperException, InterruptedException &#123; List&lt;String&gt; childNodeList = zooKeeper.getChildren(path,listener); System.out.println("子节点列表："+ Arrays.toString(childNodeList.toArray())); &#125; public static void main(String[] args) throws KeeperException, InterruptedException &#123; Operator operator = new Operator(); String path = "/myNode"; String child = path +"/moNode-1"; //新建节点 operator.exists(path); operator.createZnode(path,"123"); operator.getNodeData(path); //新建子节点 operator.getChild(path); operator.createZnode(child,"123"); operator.getNodeData(path); operator.updateZnode(path, "456"); operator.getNodeData(path); operator.deleteZnode(child); operator.deleteZnode(path); operator.exists(path); &#125;&#125; 创建节点时，节点模式4种可选 1234PERSISTENT, //持久节点PERSISTENT_SEQUENTIAL,//持久有序节点EPHEMERAL,//临时节点EPHEMERAL_SEQUENTIAL;//临时有序节点 Znode的监听12345678910111213141516171819202122232425262728293031323334353637/** * 监听类，继承Watcher，zk api提供getData(),exists()和getChildren() * 三个方法对Znode进行监听 * */ class OperatorListener implements Watcher &#123; @Override public void process(WatchedEvent watchedEvent) &#123; switch (watchedEvent.getType()) &#123; case None: System.out.println("连接成功"); break; case NodeCreated: System.out.println("创建节点:"+watchedEvent.getPath()); break; case NodeDeleted: System.out.println("删除节点:"+watchedEvent.getPath()); break; case NodeDataChanged: try &#123; System.out.println("修改节点:"+watchedEvent.getPath()); getNodeData(watchedEvent.getPath()); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; break; case NodeChildrenChanged: System.out.println("创建子节点:"+watchedEvent.getPath()); break; default: break; &#125; &#125; &#125; 事件的几种状态 12345None,//连接成功的初始状态NodeCreated,//节点创建时触发NodeDeleted,//节点删除时触发NodeDataChanged,//节点数据更新时触发NodeChildrenChanged;//子节点变化时触发]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>zkjavaApi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper客户端命令详解]]></title>
    <url>%2F2018%2F03%2F19%2Fzookeeper%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本篇分析zookeeper客户端常用指令 启动客户端1sh zkCli.sh [-server ip:port] #启动客户端 默认连接本机的zookeeper，如果指定server，尝试连接指定server的zookeeper connect1connect host:port #连接zk服务端，与close命令配合使用可以连接或者断开zk服务端 help1help #查看有哪些命令 以上是client支持的命令，注意每个命令符之间一定要有空格，两个常见的命令符做个统一说明： path 表示节点的绝对路径（因为zookeeper不支持相对路径） watch 表示是否对节点进行监听（节点的变更将发送客户端通知） ls1ls path [watch] ##查看path节点下的一级子节点 ls21ls2 path [watch] ##查看path节点下的一级子节点，并返回当前节点的stat信息 create1create [-s] [-e] path data acl ##创建子节点 -s 表示是否为有序节点 -e 表示是否为临时节点 path 节点的绝对路径 data 节点的数据 acl 访问权限 如果父节点不存在，无法创建，因此节点必须逐级创建。 默认情况下（不带-s 和-e参数）创建的是持久节点 get1get path [watch] ##获取对应节点的数据和stat信息 set1set path data [version] ##设置存储的数据 [version] 为节点数据的版本，每次数据变更，dataVersion都会递增，version参数类似于乐观锁，指定修改对应version的节点数据，如果version不匹配，不做任何操作，不带version表示忽略版本号。 delete1delete path [version] #删除节点 和set命令一样，delete也可以删除指定version的节点，不带version则表示忽略版本号直接删除 rmr1rmr path #递归删除 如果当前节点下存在子节点，直接删除当前节点会提示节点非空（存在子节点） 此时就要先删除子节点，才能删除当前节点，rmr命令允许递归删除当前节点。 stat1stat path [watch] #获取节点stat信息 跟get的差异在于只获取stat的信息 setquota1setquota -n|-b val path ##设置配额属性 -n 表示该节点下子节点的个数count限制（包括当前节点自己）； -b 表示该节点存储的数据字节大小bytes限制； listquota1listquota path #查看配额属性 之前用setquota命令设置了最大节点count=3，bytes=-1表示不限制大小（count=-1也表示不限制），stat行count=1表示当前节点数为1，bytes=3表示当前字节大小为3 delquota1delquota [-n|-b] path #删除配合属性 -n 只删除count配额 -b 只删除bytes配额 不带-n | -b表示删除该节点的所有配额属性 printwatches on|off1printwatches on|off #开启/关闭 监听信息打印 查看是否开启打印监听 对一个节点设置监听，然后修改节点数据 setAcl1setAcl path acl #设置节点的访问权限 节点的访问权限必须单独设置，子节点无法继承父节点的访问权限。上一篇有提到zookeeper的ACL权限，授权命令为scheme : id : permission，permission对应Create（c）、Read（r）、Write（w）、Delete（d）、Admin（a） getAcl1getAcl path #获取节点的访问权限 sync1sync path #在集群中强制同步节点信息 history1history #列出执行的命令历史 redo1redo cmdno #再次执行 id = cmdno的命令，需配合 history命令查出历史命令的id addauth1addauth scheme auth #节点认证 close1close #断开与zk服务端的连接 断开后，Client状态由CONNECTED变更为CLOSED quit1quit #退出客户端]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>zookeeper客户端命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper数据模型和特性]]></title>
    <url>%2F2018%2F03%2F17%2Fzookeeper%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本篇主要分析zookeeper（以下简称ZK）的数据模型，包含命名空间、层次结构和节点类型等基本属性。ZK的命名空间与分布式的文件系统高度相似，唯一的区别是ZK的命名空间允许每个节点保存数据，就好比一个既是目录也是数据文件文件系统。 命名空间ZK的命名空间规范是以斜杠分隔，所有的路径都是绝对路径（没有相对路径），任何unicode字符都可以作为节点名称，但根据官方文档的描述，必须遵守以下约束： 12345678910111213141516171819·The null character (\u0000) cannot be part of a path name. (This causes problems with the C binding.)#null不允许作为节点名称·The following characters can&apos;t be used because they don&apos;t display well, or render in confusing ways: \u0001 - \u0019 and \u007F - \u009F.#\u0001 - \u0019和\u007F - \u009F范围内的字符禁止作为节点名称，这些字符对应的就是空方块、空三角等无法正确显示的字符·The following characters are not allowed: \ud800 -uF8FFF, \uFFF0 - uFFFF.#\ud800 -uF8FFF和\uFFF0 - uFFFF范围内的字符也禁止·The &quot;.&quot; character can be used as part of another name, but &quot;.&quot; and &quot;..&quot; cannot alone be used to indicate a node along a path, because ZooKeeper doesn&apos;t use relative paths. The following would be invalid: &quot;/a/b/./c&quot; or &quot;/a/b/../c&quot;.#节点名称中不允许只包含&quot;.&quot;和&quot;..&quot;字符，因为ZK不存在相对路径一说，因此这种写法也是不允许的·The token &quot;zookeeper&quot; is reserved.#&quot;zookeeper&quot;作为保留字，不允许作为节点名称 ZK的视图的树形结构 节点ZNodeZK树中的每个节点都称之为ZNode，每个ZNode都可以写入数据，并且维护着一组stat信息（包含版本信息、权限信息，节点属性等），ZNode有以下几个类型： 持久节点（PERSISTENT）：一旦创建，不会因为客户端session断开而删除； 临时节点（EPHEMERAL）：客户端session断开后，自动删除，临时节点下不能创建子节点； 持久有序节点（PERSISTENT_SEQUENTIAL）：有序的持久节点； 临时有序节点（EPHEMERAL_SEQUENTIAL）：有序的临时节点； 有序节点的组成：名称末尾会加上一个顺序号，格式为%010d，表示长度10，左补0，例如node0000000001、node0000000002，利用这个特点可以实现分布式队列。 ZNode有以下特点： watch监听，客户端可以对ZNode进行监视，ZNode的事务操作（创建、修改、删除、新增子节点等）都会触发watch事件，向客户端发送通知，然后清除当前watch监听。 访问权限，存储在ZNode的数据都是以原子的方式进行读写，读取/写入的访问权限都由一个Access Control List（ACL）来控制是否有操作权限。 数据存储大小，ZK的设计初衷是用来作为协调服务，它虽然可以存储数据，但这并不是它的主要目的，因此他允许保存的数据大小上限是1M，这些数据一般为配置信息、状态信息等相对较小，不会占用太大的存储空间。如果非要存储大量的数据，建议把数据保存在大容量存储系统中（bulk storage system），例如NFS、HDFS，而ZK只保存这些数据的存储路径。 Stat上文提到了ZNode维护了一组stat信息，这些信息由哪些组成？又有什么涵义呢？先来看看它的结构。 czxid：ZNode创建时的zxid（create） mzxid：ZNode上次修改时的zxid（modify） pzxid：ZNode子节点变更（新增、修改、删除）的zxid（parent） ctime：节点创建的时间戳（Unix时间戳） mtime：节点上次修改时间戳（Unix时间戳） version：当前节点的版本，如果修改了当前节点保存的数据，每次更改都会自增 cversion：子节点的版本，子节点的事务提交都会引起该值自增 aversion：节点的ACL版本，修改节点的ACL权限，会引起该值自增 ephemeralOwner：如果节点是一个临时节点，该值就是对应的sessionId，持久节点该值为0 dataLength：数据字节长度 numChildren：子节点个数 这里有几个属性，记录着ZNode每次变化的时间。 zxid（Zookeeper Transaction Id），它是一个unix时间戳，stat中保存了三个zxid，分别是czxid（创建）、mzxid（修改）、pzxid（子节点变更），在ZK中任何一次事务提交，都会将提交时的时间戳保存在对应的属性上。 version，stat中保存了三个version，分别是version（当前节点版本，数据修改会导致自增）、cversion（子节点版本，子节点事务提交会导致自增）、aversion（当前节点ACL版本，ACL权限变更会导致自增） 会话先看ZK官网的这张图，会话的开始和结束 客户端从连接成功到连接断开，中间保持的状态就是一个会话（Session），客户端一旦连上ZK server，server就会创建一个session和唯一的64位sessionId用来和客户端保持通信。从上图来看，客户端连接到服务端有以下几个状态： CONNECTING，连接中 CONNECTED，已连接 AUTH_FAILED，认证失败 SESSION_EXPIRED，会话过期 DISCONNECTED，会话断开 CLOSED，会话关闭 客户端连接服务端时，会带一个超时时间的参数，表示多少时间（毫秒）内连上服务端（这个时间取决于zoo.cfg的tickTime配置，至少是2倍tickTime，最大不超过20倍tickTime） 监听在ZK的节点上，任何读请求都可以设置一个watch监听，它有以下三个特点： 一次性，节点的数据变更会出发监听事件并发送通知到客户端，当再次修改这个节点的数据时，不会再次触发监听事件，除非再次设置监听。 发送客户端通知，监听事件是异步发送的，ZK可以保证发送到客户端的通知是有序的，例如客户端对某个ZNode设置监听事件，在这个请求到达服务端前，其他客户端修改了这个ZNode的数据，那么发送请求的客户端是无法感知的。即使设置了监听事件，其他客户端对数据的修改也无法立即通知到这个客户端，因此ZK只能保持数据的最终一致。 watch数据监听，ZNode允许客户端对当前节点和子节点建立监听 可以通过API的三个方法监听ZNode的状态，getData()和exists()用来设置当前节点，getChildren()用来设置子节点监听，以下几个监听条件会触发事件监听： Create event：调用exists()设置节点创建的监听事件 Delete event：调用exits()、getData()和getChildren()设置节点删除的监听事件 Change event：调用exits()、getData()设置节点数据变更的监听事件 Child event：调用getChildren()设置子节点相关的监听事件 具体的监听调用参见后续的API分析 访问权限ZK使用ACL来控制对节点的访问，ACL与UNIX文件访问很类似，在了解这个之前，先了解一下ZK的授权，ZK授权命令scheme : id : permission，表示认证方式、用户和权限，Scheme有以下几种： world：相当于所有客户端都能访问，只有一个id—anyone，命令方式为world:anyone auth：不需要设置用户，相当于授权给有权限的所有用户 digest：使用用户名:密码方式（username:password）来认证，ZK内部使用SHA1和MD5算法来进行加密 ip：根据IP认证，只允许认证的IP进行操作 ACL支持的权限（Permission）有： CREATE：创建当前节点下子节点的权限 READ：只读权限，包括遍历子节点 WRITE：写数据权限 DELETE：节点删除权限 ADMIN：设置权限的权限 具体如何设置权限参见后续的API分析]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>ACL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识zookeeper]]></title>
    <url>%2F2018%2F03%2F15%2F%E5%88%9D%E8%AF%86zookeeper%2F</url>
    <content type="text"><![CDATA[zookeeper简介背景zookeeper是一个分布式解决方案，那么分布式系统有哪些特点？ 并发，同一个分布式系统的不同节点，访问同一个资源文件； 无序，客户端的请求和服务端响应无序，进程间通信的无序； 以上问题引发的分布式问题： 网络波动 分布式系统的各节点部署在不同的机器，甚至不同的机房里，因此网络的稳定性直接影响分布式系统的稳定性，丢包、延迟等会对系统的可用性造成不利影响，严重的可能造成服务瘫痪。 网络分区 所谓的网络分区，简单理解就是在分布式系统里，节点与节点之间网络延迟增大甚至断开导致无法互相通信，造成数据不同步，数据分散在几个区域之中。网络分区导致的结果就是数据一致性问题，不同节点的数据可能不同。 三态 分布式系统的每次请求和响应，都存在三种状态：成功、失败和超时。三态与网络波动有直接的关系 分布式事务 事务的ACID（原子性、一致性、隔离性、持久性），在分布式系统中对数据进行事务处理具有非常大的挑战，很难保证事务在同一条件下提交/回滚，因此两种理论对分布式事务的实现提供了理论基础。 CAP理论这个定理起源于柏克莱加州大学（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在2000年的分布式计算原则研讨会（Symposium on Principles of Distributed Computing（PODC））上提出的一个猜想。 在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理（以上内容摘自维基百科）。 一致性（Consistency），所有节点都保存一份最新数据的副本； 可用性（Availability），客户端的每次请求都能得到服务端的成功响应，无论数据是否正确或最新； 分区容错性（Partition-tolerance），分布式系统一旦出现了网络分区，就必须在一致性和可用性之间保证二选其一。 通过以上理论，无法同时满足三种特性，那么该如何取舍，取决于实际的场景。 在大型互联网产品中，集群的机器数量多，节点分散，因此网络故障或者某些节点故障也难以避免，为了保证N个9的可用性，也就是保证可用性和分区容错（AP），一般会舍弃一致性（放弃瞬时一致，保证最终一致）。而另一种情况下，如果涉及到非常重要的数据，宁可服务停止也要保证数据一致性，比如资金数据等，这就要放弃高可用（CP）。前一种场景，eBay的架构师另辟蹊径又提出了另一种方案，它基于CAP理论，做了进一步延申，就是BASE理论。 BASE理论基本可用（Basically Available），指在发生节点故障时，在保证核心功能正常运行的前提下，允许损失一部分服务，同时为这些服务提供降级服务。 软状态（Soft State），即允许分布式系统存在中间状态，该中间状态不对系统可用性造成影响，因为网络肯定存在延时的情况，允许节点之间数据同步发生延时就是一种软状态。 最终一致性（Eventual Consistency），与强一致性的理论相对，允许数据在短时间内不一致，但最终数据同步后达到一致的状态。 在上述的背景下，zookeeper由雅虎公司基于google chubby研发，作为一个开源的分布式应用协调服务和分布式数据一致性的解决方案，解决分布式系统中遇到的上述问题。 能做什么数据发布/订阅 负载均衡 命名服务 分布式独享锁 master选举 分布式队列 有哪些特点 顺序一致性 来自客户端的更新请求，最终都会严格按照请求顺序在zookeeper中保持一致。简单来说，就是客户端将NodeA的值更新为a，紧接着将值更新为b，那么其他节点同步的数据，肯定是先看到a，再看到b。 原子性 所有事务请求的最终结果在集群中所有节点上是一致的，要么都成功，要么都失败。 持久性 事务操作（增删改）一旦成功并且对客户端做了相应，那么这个更新就会持久存在且不被撤销。 实时性 在某一段特定的时间内，客户端能立即从服务端读取到最新的数据，并且在此时间段内，节点中任何数据的改变，都将被客户端发现。 zookeeper如何安装环境准备系统环境：centos7 软件环境：JDK6+ 单机模式 下载zookeeper，用的是3.4.10版本 1wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz 解压 1tar -zxvf zookeeper-3.4.10.tar.gz 复制并重命名配置文件 进入zk的conf目录，看到一个zoo_sample.cfg文件，先复制一份并命名为zoo.cfg，因为这个sample文件只是配置的范例，zookeeper启动时默认的配置文件为zoo.cfg。 1cp zoo_sample.cfg zoo.cfg 以下是配置文件的有关参数 12345678910111213141516# 心跳间隔，用来监测客户端与服务端的通信，默认是2000毫秒tickTime=2000# 集群环境下，初始化连接时leader与follower数据同步的最长心跳tickTime限制，默认是10 * 2000也就是20秒initLimit=10# 集群环境下，leader与follower请求、应答的最大时间限制限制，默认是5 * 2000也就是10秒syncLimit=5# 快照/数据保存目录，默认是/tmp/zookeeper，建议改掉它，因为tmp目录在服务重启后会被删除dataDir=/tmp/zookeeper# 客户端连接的默认端口clientPort=2181# 限制连接到当前zk服务的客户端数（以IP区分），默认是关闭的#maxClientCnxns=60# 保留多少份dataDir的快照数据，之前的全都会删除#autopurge.snapRetainCount=3# 清理保存在dataDir的交互日志的时间间隔，小时为单位，设置为0表示不删除，默认不删除#autopurge.purgeInterval=1 启动服务 启动服务前，可以通过help查看zk有哪些命令 1sh zkServer.sh help --help替换成其他非zk的命令字符也是可以查看的 start：启动 start-foreground：以前台进程的方式启动，会占用当前命令窗口，建议在调试的时候使用，方便查看输出的信息 stop：停止 restart：重启 status：查看当前zk服务的模式 upgrade：升级 print-cmd：打印指令 start命令启动后，出现 …STARTED表示启动成功 用status命令查看服务状态 Mode：standalone表示zookeeper正以单机模式运行 客户端链接 输入sh zkCli.sh -server serverIP : port，连接到zk服务，看到welcome…表示连接成功 1sh zkCli.sh -server localhost:2181 集群模式集群环境需先准备三个服务器，ip分别为 192.168.96.129 192.168.96.130 192.168.96.131 分别在三台服务器上安装zookeeper，然后按照以下步骤进行配置 修改配置文件 修改zk_home（zookeeper安装的根目录）/conf/zoo.cfg，在末尾加上以下配置 server.id=serverIP:port:port id表示该机器在这个集群中的唯一标识，取值范围1-255； 2888端口是用来集群中各节点相互通讯； 3888端口则是用来选举leader时的通讯； 将三台服务器都按以下配置 server.1=192.168.96.129:2888:3888server.2=192.168.96.130:2888:3888server.3=192.168.96.131:2888:3888 创建myid文件 在zoo.cfg配置的dataDir目录下，创建myid文件，内容就是第一项中配置的id，例如192.168.96.129服务器的myid文件内容为1，192.168.96.130的myid内容为2，192.168.96.131的myid内容为3 启动zookeeper 启动每个服务器上的zookeeper，启动后分别查看状态 正常启动且集群成功，将出现以下提示 Mode：leader表示该服务器是集群中的leader角色 如果出现以下提示， 可以通过查看bin目录下的zookeeper.out日志查找问题，很有可能是因为防火墙没有关闭导致节点无法互相通信，先关闭防火墙再重试 12systemctl stop firewalld --关闭防火墙systemctl disable firewalld --禁止防火墙开机启动 zookeeper的集群有三种角色：Leader/Follower/Observer Leader：接收所有Follower提交的请求，负责与Follower进行数据同步； Follower：接收客户端的请求并参与选举，同时与Leader进行数据同步； Observer：接收客户端的请求，同时与Leader进行数据同步，不参与选举，zookeeper性能扩展的特殊节点。 在集群中如何配置Observer？ 新增一台服务器192.168.96.132，在zoo.cfg增加以下配置 server.4=192.168.96.132:2888:3888:observer 启动后查看status状态 Mode：observer，表示当前zk服务的角色为observer]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
</search>
